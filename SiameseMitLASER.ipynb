{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SiameseMitLASER",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "_wUDuuzC3knQ",
        "iKRw4tSWb1jW",
        "VBYzPcejcJnC",
        "c3fiA2utZEjH",
        "Dx77VUyk5yFe",
        "FVwr0z3eW_iT",
        "koat_Nz1Op49",
        "rY-eNwlA81qU",
        "ZerJu79ImORc",
        "Yy6Nq9ZnlJPc",
        "zBeET4QrnUzg",
        "lRhp-_F8Btvh",
        "yWsBHLDFv0Gc"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinmay5/NLP-Praktikum/blob/one_shot/SiameseMitLASER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wUDuuzC3knQ",
        "colab_type": "text"
      },
      "source": [
        "# This is an attempt to include the LASER multi lingual embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mngRKDEg3x2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/ceshine/LASER.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_8LAr8c6Wyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf \\\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Akm1QeP4lea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%env LASER=/content/LASER\n",
        "!echo $LASER\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo67QGwc7m7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KxeDCC74lhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd LASER"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjFh5jFf4lmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7ADRwFZ4lp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!bash install_models.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5F_sNpe4lkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!bash install_external_tools.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq9RVwqi8pyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/LASER\n",
        "%cd tools-external/fastBPE/\n",
        "! g++ -std=c++11 -pthread -O3 fastBPE/main.cc -IfastBPE -o fast"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpraUBs38jZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ../../tasks/similarity/\n",
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVjP99krE02v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! apt install libopenblas-base libomp-dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdgEtRK5AN9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install faiss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBPIo9wj9GSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "% cd /content/LASER/tasks/similarity\n",
        "! bash wmt.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sS0FP2kG-cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample feasibility check\n",
        "# % cd ../../source/\n",
        "\n",
        "import os\n",
        "import sys\n",
        "LASER = os.environ['LASER']\n",
        "# now include the extra files in the source\n",
        "sys.path.append(LASER + '/source')\n",
        "sys.path.append(LASER + '/source/lib')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39VFH-l3G-l4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from embed import SentenceEncoder, EncodeLoad, EncodeFile\n",
        "from text_processing import Token, BPEfastApply\n",
        "from indexing import IndexCreate, IndexSearchMultiple, IndexPrintConfusionMatrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PKiL_pcyqOm",
        "colab_type": "text"
      },
      "source": [
        "#Essential Imports before we start the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daymP20xX_7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pandas import Series\n",
        "from google.colab import drive\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from tensorflow import summary\n",
        "from torch.utils.data import BatchSampler\n",
        "from collections import Counter\n",
        "from sklearn.metrics import f1_score,accuracy_score, confusion_matrix, classification_report\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-J64A0YqMvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary for label to index, this has been pre-generated\n",
        "encoding_to_labels = {\n",
        "   0:\t\"Amazon Instant Videos\",\n",
        "   1 :\"Android Apps\",\n",
        "   2 : \"Automotive\",\n",
        "   3 :\"Baby\",\n",
        "   4 :\"Beauty\",\n",
        "   5 :\"CDs and Vinyl\",\n",
        "   6 :\"Cell Phones and Accessories\",\n",
        "   7 :\"Clothing, Shoes, Jewelry\",\n",
        "   8 :\"Digital Music\",\n",
        "   9 :\"Electronics\",\n",
        "  10 :\"Grocery and Gourmet\",\n",
        "  11 :\"Health and Personal Care\",\n",
        "  12 :\"Home and Kitchen\",\n",
        "  13 :\"Kindle\",\n",
        "  14 :\"Movies and TV\",\n",
        "  15 :\"Musical Instruments\",\n",
        "  16 :\"Office Products\",\n",
        "  17 :\"Patio Garden\",\n",
        "  18 :\"Pet Supplies\",\n",
        "  19 :\"Sports, Outdoors\",\n",
        "  20 :\"Tool and Home Improvement\",\n",
        "  21 :\"Toys_and_Games\",\n",
        "  22 :\"Video Games\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-JUCiTYsRWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lebel_from_code(code):\n",
        "  if code not in encoding_to_labels.keys():\n",
        "    raise KeyError(\"Invalid Code\")\n",
        "  return encoding_to_labels[code]\n",
        "\n",
        "def get_all_labels():\n",
        "  return list(encoding_to_labels.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkWncWkbZGyI",
        "colab_type": "text"
      },
      "source": [
        "## Please make your life easy and execute these statements. If any error pops up, reset runtime and restart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GNIm96jp_zhM",
        "colab": {}
      },
      "source": [
        "!pip install -q tb-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0QcrlYeTdBR-",
        "colab": {}
      },
      "source": [
        "# # Delete any old logs.... be smart while using this\n",
        "% rm -rf /content/logs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rOreic85dBSC",
        "colab": {}
      },
      "source": [
        "% mkdir -p '/content/logs/tensorboard/round2/train/'\n",
        "% mkdir -p '/content/logs/tensorboard/round2/val/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfajFKSRoqSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKRw4tSWb1jW",
        "colab_type": "text"
      },
      "source": [
        "## These are the steps to replicate results of the LSTM file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ZY3F5ZbCwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_PATH = '/content/LASER/models/'\n",
        "sentence_encoder = SentenceEncoder(\n",
        "    str(MODEL_PATH + \"bilstm.93langs.2018-12-26.pt\"),\n",
        "    max_sentences=None,\n",
        "    max_tokens=10000,\n",
        "    cpu=False)\n",
        "sentence_encoder.use_cuda=True #just a guess not sure if this would actually help"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBYzPcejcJnC",
        "colab_type": "text"
      },
      "source": [
        "# This is the code section for trying Triplet Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP-YgYTWcQWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TripletDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Train: For each sample (anchor) randomly chooses a positive and negative samples\n",
        "    Test: Creates fixed triplets for testing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, select_column, training_mode=True):\n",
        "         \n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.train = training_mode\n",
        "        self.select_column = select_column\n",
        "        if self.train:\n",
        "            self.train_labels = self.dataset.labels_encoded\n",
        "            # Drop the labels column so that remaining features form part of the training set\n",
        "            self.dataset = self.dataset.drop('labels_encoded', axis=1)\n",
        "            self.train_data = self.dataset\n",
        "            self.labels_set = set(self.train_labels.to_numpy())\n",
        "            self.label_to_indices = {label: np.where(self.train_labels.to_numpy() == label)[0]\n",
        "                                     for label in self.labels_set} # redundent in our case if we decide to use numeric labels based on certain sklearn packages\n",
        "\n",
        "        else:\n",
        "            self.test_labels = self.dataset.labels_encoded\n",
        "            # Drop the labels column so that remaining features form part of the training set\n",
        "            self.dataset = self.dataset.drop('labels_encoded', axis=1)\n",
        "            self.test_data = self.dataset\n",
        "            # generate fixed triplets for testing\n",
        "            self.labels_set = set(self.test_labels.to_numpy())\n",
        "            self.label_to_indices = {label: np.where(self.test_labels.to_numpy() == label)[0]\n",
        "                                     for label in self.labels_set}\n",
        "\n",
        "            random_state = np.random.RandomState(42)\n",
        "            \n",
        "            #print(\"Length of the dataset is {}\".format(len(self.test_data)))\n",
        "            \n",
        "            triplets = []\n",
        "            for i in range(len(self.test_data)):\n",
        "                  triplets.append([i,\n",
        "                           random_state.choice(self.label_to_indices[self.test_labels.iloc[i]]),\n",
        "                           random_state.choice(self.label_to_indices[\n",
        "                                                 np.random.choice(\n",
        "                                                     list(self.labels_set - set([self.test_labels.iloc[i]]))\n",
        "                                                 )\n",
        "                                             ])\n",
        "                         ]) \n",
        "                         \n",
        "            self.test_triplets = triplets\n",
        "           \n",
        "    def __getitem__(self, index):\n",
        "        if self.train:\n",
        "            #print(type(self.train_data))\n",
        "            img1, label1 = self.train_data.iloc[index], self.train_labels.iloc[index]\n",
        "            positive_index = index\n",
        "            while positive_index == index:\n",
        "                positive_index = np.random.choice(self.label_to_indices[label1])\n",
        "            negative_label = np.random.choice(list(self.labels_set - set([label1])))\n",
        "            negative_index = np.random.choice(self.label_to_indices[negative_label])\n",
        "            img2 = self.train_data.iloc[positive_index]\n",
        "            img3 = self.train_data.iloc[negative_index]\n",
        "        else:\n",
        "            img1 = self.test_data.iloc[self.test_triplets[index][0]]\n",
        "            img2 = self.test_data.iloc[self.test_triplets[index][1]]\n",
        "            img3 = self.test_data.iloc[self.test_triplets[index][2]]\n",
        "\n",
        "        img1 = img1[self.select_column]\n",
        "        img2 = img2[self.select_column]\n",
        "        img3 = img3[self.select_column]\n",
        "        \n",
        "        #print(\"Computation of the entities are completed\")\n",
        "\n",
        "        return (img1, img2, img3)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTXpYPem32iM",
        "colab_type": "text"
      },
      "source": [
        "## Load the entire dataset in this section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzVaHR7VcQlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/\n",
        "input_data = pd.read_csv('actual_dataset.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ9OuN6fnEjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImQg2ouIupGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY_gevdXu8ZV",
        "colab_type": "text"
      },
      "source": [
        "### We would use review.text as the input and use it to predict review.primaryCategories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfzKSTw5vMvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data.Product_Category.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbUHtUzLl-T1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = input_data.rename(columns={'Product_Category':'label'})\n",
        "input_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm6KWbuh95tX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LASER = os.environ['LASER']\n",
        "FASTBPE = LASER + '/tools-external/fastBPE/fast'\n",
        "MOSES_BDIR = LASER + '/tools-external/moses-tokenizer/tokenizer/'\n",
        "MOSES_TOKENIZER = MOSES_BDIR + 'tokenizer.perl -q -no-escape -threads 20 -l '\n",
        "MOSES_LC = MOSES_BDIR + 'lowercase.perl'\n",
        "NORM_PUNC = MOSES_BDIR + 'normalize-punctuation.perl -l '\n",
        "DESCAPE = MOSES_BDIR + 'deescape-special-chars.perl'\n",
        "REM_NON_PRINT_CHAR = MOSES_BDIR + 'remove-non-printing-char.perl'\n",
        "MECAB = LASER + '/tools-external/mecab'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9sXKsW1M9Xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from subprocess import check_output\n",
        "def sample_token(string_val, lang='en'):\n",
        "    tok = check_output(\n",
        "                REM_NON_PRINT_CHAR\n",
        "                + '|' + NORM_PUNC + lang\n",
        "                + '|' + DESCAPE\n",
        "                + '|' + MOSES_TOKENIZER + lang\n",
        "                + ('| python3 -m jieba -d ' if lang == 'zh' else '')\n",
        "                + ('|' + MECAB + '/bin/mecab -O wakati -b 50000 ' if lang == 'ja' else ''),\n",
        "                input=string_val,\n",
        "                encoding='UTF-8',\n",
        "                shell=True)\n",
        "    return tok.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT4TrHkYvT0Q",
        "colab_type": "text"
      },
      "source": [
        "### Clean the data as that might cause problems later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faPNpMFclPpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = input_data.reviewText.isnull()==True\n",
        "samp = index[index==True]\n",
        "input_data = input_data.drop(samp.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KID-p2ZpvZP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data.reviewText.isnull().sum() #Value should be always zero"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmGwWzJZNkwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Do the above mentioned operation on all the entries in the column. So,\n",
        "  \n",
        "\"\"\"\n",
        "input_data['tokenized_review'] = input_data.reviewText.apply(sample_token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPBpVwpGROPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bpe_codes = './LASER/models/93langs.fcodes'\n",
        "bpe_vocab = bpe_codes.replace('fcodes', 'fvocab')\n",
        "\n",
        "\n",
        "def apply_bpe(sentence):\n",
        "  sentence = sentence.replace('\"','\\\\\"')\n",
        "  bpe = check_output(\n",
        "      'echo -n \"'+sentence +'\" | '+ FASTBPE + ' applybpe_stream '+  bpe_codes +' '+bpe_vocab,\n",
        "      encoding='UTF-8',\n",
        "      shell=True)\n",
        "  return bpe.strip()\n",
        "\n",
        "\n",
        "#input_data['tokenized_review_bpe'] = input_data.reviewText.apply(bpe_sentences)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEgQx2HgZml4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_sentence = '\"love the magnet easel ... \" slow motion me \"great for moving to different areas ... wish it had some sort of non skid pad on bottom though ...'\n",
        "apply_bpe(tok_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHVv21bCcIXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data['tokenized_review_bpe'] = input_data.tokenized_review.apply(apply_bpe)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoizC9eVsr7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Let's encode the values in here so that we can use them as label encoder later\n",
        "\"\"\"\n",
        "# import labelencoder\n",
        "# instantiate labelencoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "input_data['labels_encoded'] = le.fit_transform(input_data['label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB-DB0WB2_Rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data.to_csv('./preprocessed_output.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3fiA2utZEjH",
        "colab_type": "text"
      },
      "source": [
        "# Reading preprocessed output uptil BPE and then converting it into sentence embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stWEpEY7U_mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Trying to apply the sentence encoding so that we can store the entire thing\n",
        "%cd /content/\n",
        "input_data = pd.read_csv('./preprocessed_output.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMmedeXLU_kR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directly copy pasted from stackoverflow \n",
        "# https://stackoverflow.com/questions/35092720/verbosity-pandas-apply"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0jawvqYKJFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Because of the nature in which LASER tends to perform encoding we have to do this extra tweek\n",
        "def to_list(element):\n",
        "  return [element]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijP0eityf6mG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data['tokenized_review_bpe_list'] = input_data.tokenized_review_bpe.apply(to_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ohTxzwRU_hK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def progress_coroutine(print_on = 10):\n",
        "    print (\"Starting progress monitor\")\n",
        "\n",
        "    iterations = 0\n",
        "    while True:\n",
        "        yield\n",
        "        iterations += 1\n",
        "        if (iterations % print_on == 0):\n",
        "            print (\"{} iterations done\".format(iterations))\n",
        "\n",
        "def percentage_coroutine(to_process, print_on_percent = 0.10):\n",
        "    print (\"Starting progress percentage monitor\")\n",
        "\n",
        "    processed = 0\n",
        "    count = 0\n",
        "    print_count = to_process*print_on_percent\n",
        "    while True:\n",
        "        yield\n",
        "        processed += 1\n",
        "        count += 1\n",
        "        if (count >= print_count):\n",
        "            count = 0\n",
        "            pct = (float(processed)/float(to_process))*100\n",
        "\n",
        "            print (\"{}% finished\".format(pct))\n",
        "\n",
        "def trace_progress(func, progress = None):\n",
        "    def callf(*args, **kwargs):\n",
        "        if (progress is not None):\n",
        "            progress.send(None)\n",
        "\n",
        "        return func(*args, **kwargs)\n",
        "\n",
        "    return callf\n",
        "\n",
        "def my_func(i):\n",
        "    return i ** 2\n",
        "\n",
        "data_series = input_data.tokenized_review_bpe_list\n",
        "#co1 = progress_coroutine()\n",
        "co2 = percentage_coroutine(len(data_series))\n",
        "input_data['sentence_embedding'] = data_series.apply(trace_progress(sentence_encoder.encode_sentences, progress = co2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Vw0OaCgf6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Please let this be here for debugging purpose. The output shape should be (1,1024) ideally\n",
        "\n",
        "\"\"\"\n",
        "# string_input = 'I like the item pri@@ cing . M@@ y grand@@ dau@@ ghter wanted to mark on it but I wanted it just for the let@@ ters .'\n",
        "# sentence = (to_list(string_input))\n",
        "# print(sentence_encoder.encode_sentences(sentence).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb_uW2UKhfs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The value should be True\n",
        "len(input_data.sentence_embedding.iloc[0]) == 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpfv_95F2MqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = input_data[['sentence_embedding','labels_encoded']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FM5LGWEU_ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data.to_pickle('./preprocessed_sentence_encoded.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx77VUyk5yFe",
        "colab_type": "text"
      },
      "source": [
        "## If you already have the preprocessed data, use it here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw0-djNMYxxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the file from gdrive which is faster than downloading it\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QyyEHMw52rT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/\n",
        "input_data = pd.read_csv('/gdrive/My Drive/d_training_concatenated_230k.csv', index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytjqgiLurasK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBxmvXGoqi27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data.head()\n",
        "#input_data = input_data.drop('Unnamed:0', axis=1)\n",
        "df = input_data\n",
        "df = df[pd.notnull(df['tokenized_review_bpe'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC-JdgYkryls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.tokenized_review_bpe.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KryRitpr8I2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd3drVZISrtq",
        "colab_type": "text"
      },
      "source": [
        "##  Same set of preprocessing equations that we used for the actual index file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uDG6vLzdS5ho",
        "colab": {}
      },
      "source": [
        "# Because of the nature in which LASER tends to perform encoding we have to do this extra tweek\n",
        "def to_list(element):\n",
        "  return [element]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3UmjrdiMS5h-",
        "colab": {}
      },
      "source": [
        "input_data['tokenized_review_bpe_list'] = input_data.tokenized_review_bpe.apply(to_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YAN8XLWwS5iC",
        "colab": {}
      },
      "source": [
        "def progress_coroutine(print_on = 10):\n",
        "    print (\"Starting progress monitor\")\n",
        "\n",
        "    iterations = 0\n",
        "    while True:\n",
        "        yield\n",
        "        iterations += 1\n",
        "        if (iterations % print_on == 0):\n",
        "            print (\"{} iterations done\".format(iterations))\n",
        "\n",
        "def percentage_coroutine(to_process, print_on_percent = 0.10):\n",
        "    print (\"Starting progress percentage monitor\")\n",
        "\n",
        "    processed = 0\n",
        "    count = 0\n",
        "    print_count = to_process*print_on_percent\n",
        "    while True:\n",
        "        yield\n",
        "        processed += 1\n",
        "        count += 1\n",
        "        if (count >= print_count):\n",
        "            count = 0\n",
        "            pct = (float(processed)/float(to_process))*100\n",
        "\n",
        "            print (\"{}% finished\".format(pct))\n",
        "\n",
        "def trace_progress(func, progress = None):\n",
        "    def callf(*args, **kwargs):\n",
        "        if (progress is not None):\n",
        "            progress.send(None)\n",
        "\n",
        "        return func(*args, **kwargs)\n",
        "\n",
        "    return callf\n",
        "\n",
        "def my_func(i):\n",
        "    return i ** 2\n",
        "\n",
        "data_series = input_data.tokenized_review_bpe_list\n",
        "co1 = progress_coroutine()\n",
        "co2 = percentage_coroutine(len(data_series))\n",
        "input_data['sentence_embedding'] = data_series.apply(trace_progress(sentence_encoder.encode_sentences, progress = co1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FW6_Ssr3S5iM",
        "colab": {}
      },
      "source": [
        "# The value should be True\n",
        "len(input_data.sentence_embedding.iloc[0]) == 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeLXKI4z2CYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop irrelevant columns now\n",
        "input_data = input_data[['sentence_embedding','labels_encoded']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PqkNX211S5iQ",
        "colab": {}
      },
      "source": [
        "data_series.to_pickle('/gdrive/My Drive/d_training_concatenated_183k_embedded.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVwr0z3eW_iT",
        "colab_type": "text"
      },
      "source": [
        "# If you have sentence embedded dataframe present, use those here directly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGp-uBC9Pwhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the file from gdrive which is faster than downloading it\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BIOsjmBSqRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input_data.to_csv('/gdrive/My Drive/d_training_concatenated_183k_encoded.pkl')\n",
        "input_data = pd.read_pickle('/gdrive/My Drive/d_training_final.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqEnzjojSqPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(input_data.sentence_embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-ARJLniSqMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Leaving this line here because used it to generate the csv file.\n",
        "  Should be refactored in the next iteration\n",
        "\"\"\"\n",
        "# temp_input_data = input_data[:183943]\n",
        "# len(temp_input_data)\n",
        "# temp_input_data.to_csv('/gdrive/My Drive/d_training_concatenated_183k.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obSsZW6dBqrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Let's encode the values in here so that we can use them as label encoder later\n",
        "\"\"\"\n",
        "# import labelencoder\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# # instantiate labelencoder object\n",
        "# le = LabelEncoder()\n",
        "\n",
        "# input_data['labels_encoded'] = le.fit_transform(input_data['label'])\n",
        "\n",
        "\"\"\"\n",
        "  We are leaving this part here again for completeness since we have a preprocessed dataframe which \n",
        "  has most of the things in place\n",
        "\"\"\"\n",
        "\n",
        "input_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85LV2EV4BqmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The value should be True\n",
        "#len(input_data.sentence_embedding.iloc[0]) == 1 \n",
        "input_data.sentence_embedding.iloc[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwGXxRMJPJUY",
        "colab_type": "text"
      },
      "source": [
        "##Quickly check if both train and validate dataloaders work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jUp4DY8cUl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikgRo75cfgU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 1000\n",
        "siamese_train_dataset = TripletDataset(training_mode=True,dataset=input_data,select_column='sentence_embedding')\n",
        "loader = DataLoader(siamese_train_dataset, batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddasqFvxh5QG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for index, (img1,img2,img3) in enumerate(loader):\n",
        "  print (img1.shape)\n",
        "  print(img2.shape)\n",
        "  print(img3.shape)\n",
        "  break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwJuQJ3XVrfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "siamese_val_dataset = TripletDataset(training_mode=False,dataset=input_data,select_column='sentence_embedding')\n",
        "val_loader = DataLoader(siamese_val_dataset, batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlaBuJNcVrpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for index, (img1,img2,img3) in enumerate(val_loader):\n",
        "  print (img1.shape)\n",
        "  print(img2.shape)\n",
        "  print(img3.shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R7HfIwQN-JO",
        "colab_type": "text"
      },
      "source": [
        "## Let us drop all the un-necessary columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHOe1eyjN9eE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We used this here: https://stackoverflow.com/questions/45846189/how-to-delete-all-columns-in-dataframe-except-certain-ones\n",
        "input_data = input_data[['sentence_embedding','labels_encoded']]\n",
        "input_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWPIplWsz8iV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us split the data into train and test sections. This we can use later for our purpose\n",
        "# interestingly, missing out on shuffling would lead to unexpected results\n",
        "# so we need to take care of that as well\n",
        "# This command shuffles the dataset completely\n",
        "input_data = input_data.sample(frac=1).reset_index(drop=True)\n",
        "# now we split it\n",
        "train_split = 0.8\n",
        "last_train_index = int(len(input_data) * train_split)\n",
        "data_train = input_data.iloc[:last_train_index]\n",
        "data_test = input_data.iloc[last_train_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWLAenAwz8qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train.labels_encoded.iloc[-1] == data_test.labels_encoded.iloc[0]  #Indicating that our splits of the data are correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koat_Nz1Op49",
        "colab_type": "text"
      },
      "source": [
        "# We first define the simple Triplet Function in here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSJRaTBiOpSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplet loss\n",
        "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative, size_average=True):\n",
        "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
        "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
        "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
        "        return losses.mean() if size_average else losses.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY-eNwlA81qU",
        "colab_type": "text"
      },
      "source": [
        "# Final Train Procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRnu-2oaQqkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "siamese_train_dataset = TripletDataset(training_mode=True,dataset=data_train, select_column='sentence_embedding')\n",
        "siamese_test_dataset = TripletDataset(training_mode=False,dataset=data_test, select_column='sentence_embedding')\n",
        "\n",
        "\n",
        "# Putting in the batch sampler here. Again, we need two batch samplers, one for each type\n",
        "\n",
        "train_loader = DataLoader(siamese_train_dataset,  batch_size=40, shuffle=True)\n",
        "val_loader = DataLoader(siamese_test_dataset,  batch_size=40, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdgDrJGr6i8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "margin = 1.\n",
        "model = TripletNetwork(encoder=sentence_encoder)\n",
        "model.cuda()\n",
        "#loss_fn = OnlineTripletLoss(margin, RandomNegativeTripletSelector(margin))\n",
        "# Fist start with the offline variant\n",
        "loss_fn = TripletLoss(margin)\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
        "n_epochs = 20\n",
        "log_interval = 50\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8M6qOE4Ixxaa",
        "colab": {}
      },
      "source": [
        "from tensorboardcolab import TensorBoardColab\n",
        "tb = TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZerJu79ImORc",
        "colab_type": "text"
      },
      "source": [
        "## This is the Colab Integration as it will be useul for our task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JodQfo6mNt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install latest Tensorflow build\n",
        "!pip install -q tf-nightly-2.0-preview\n",
        "from tensorflow import summary\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k86QKp30v0_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete any old logs.... be smart while using this\n",
        "% rm -rf /content/logs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53uZxjiDnDah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "% mkdir -p '/content/logs/tensorboard/train/'\n",
        "% mkdir -p '/content/logs/tensorboard/test/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j01oTX8dnDfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "current_time = str(datetime.datetime.now().timestamp())\n",
        "train_log_dir = '/content/logs/tensorboard/train/' + current_time\n",
        "test_log_dir = '/content/logs/tensorboard/test/' + current_time\n",
        "train_summary_writer = summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = summary.create_file_writer(test_log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR6Xw2Mz22Ia",
        "colab_type": "text"
      },
      "source": [
        "## Actual training code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K-lYpXS_8UV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_epoch(val_loader, model, loss_fn, cuda):\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        val_loss = 0\n",
        "        for batch_idx, (img1,img2,img3) in enumerate(val_loader):\n",
        "            \n",
        "            # First let us reshape the tensors\n",
        "            batch_size = img1.shape[0]\n",
        "            img1 = img1.reshape(batch_size,-1)\n",
        "            img2 = img2.reshape(batch_size,-1)\n",
        "            img3 = img3.reshape(batch_size,-1)\n",
        "            # Now we can optimize the remaining terms\n",
        "            \n",
        "            outputs = model(img1,img2,img3) \n",
        "            loss_inputs = outputs\n",
        "\n",
        "            loss_outputs = loss_fn(*loss_inputs)\n",
        "            \n",
        "            val_loss += loss_outputs.item()\n",
        "            \n",
        "    return val_loss/(batch_idx + 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTxnqAvN_5Ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval):\n",
        "    \n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (img1,img2,img3) in enumerate(train_loader):\n",
        "        # First let us reshape the tensors\n",
        "        batch_size = img1.shape[0]\n",
        "        img1 = img1.reshape(batch_size,-1)\n",
        "        img2 = img2.reshape(batch_size,-1)\n",
        "        img3 = img3.reshape(batch_size,-1)\n",
        "        # Now we can optimize the remaining terms\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(img1,img2,img3) \n",
        "        loss_inputs = outputs\n",
        "        \n",
        "        loss_outputs = loss_fn(*outputs) #We assume that target value is not known and hence is None\n",
        "        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
        "        losses.append(loss.item())\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                batch_idx * len(img1), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), np.mean(losses))\n",
        "            \n",
        "            print(message)\n",
        "            losses = []\n",
        "\n",
        "    total_loss /= (batch_idx + 1)\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMPNdk6_RztF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval,\n",
        "        start_epoch=0):\n",
        "    \"\"\"\n",
        "    Loaders, model, loss function and metrics should work together for a given task,\n",
        "    i.e. The model should be able to process data output of loaders,\n",
        "    loss function should process target output of loaders and outputs from the model\n",
        "\n",
        "    Examples: Classification: batch loader, classification model, NLL loss, accuracy metric\n",
        "    Siamese network: Siamese loader, siamese model, contrastive loss\n",
        "    Online triplet learning: batch loader, embedding model, online triplet loss\n",
        "    \"\"\"\n",
        "    for epoch in range(0, start_epoch):\n",
        "        scheduler.step()\n",
        "\n",
        "    for epoch in range(start_epoch, n_epochs):\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Train stage\n",
        "        train_loss = train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval)\n",
        "        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)\n",
        "#         with train_summary_writer.as_default():\n",
        "#            summary.scalar('loss', train_loss, step=epoch)\n",
        "        tb.save_value('Loss', 'train_loss', epoch, train_loss)\n",
        "        print(message)\n",
        "        val_loss = test_epoch(val_loader, model, loss_fn, cuda)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        message += '\\nEpoch: {}/{}. Validation set: Average loss: {:.4f}'.format(epoch + 1, n_epochs,\n",
        "                                                                                     val_loss)\n",
        "        \n",
        "        torch.save(model.state_dict(), './{}-model.pth'.format(epoch))\n",
        "        tb.save_value('Loss', 'val_loss', epoch, val_loss)\n",
        "#         with test_summary_writer.as_default():\n",
        "#            summary.scalar('loss', val_loss, step=epoch)        \n",
        "\n",
        "#        print(message)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE7xETdQLNfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = True\n",
        "n_epochs = 200\n",
        "fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda,log_interval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df5OaEQT02wa",
        "colab_type": "text"
      },
      "source": [
        "# The Code here is to try and implement the online Batch Sampling Selection Procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERTjB0zztxnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7A0tgL80ye6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/\n",
        "input_data = pd.read_pickle('/content/drive/My Drive/d_training_final.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5NdmsYeyY4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(input_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-mtMvx0b25bh",
        "colab": {}
      },
      "source": [
        "# Let us split the data into train and test sections. This we can use later for our purpose\n",
        "# interestingly, missing out on shuffling would lead to unexpected results\n",
        "# so we need to take care of that as well\n",
        "# This command shuffles the dataset completely\n",
        "input_data = input_data.sample(frac=1).reset_index(drop=True)\n",
        "# now we split it\n",
        "train_split = 0.8\n",
        "last_train_index = int(len(input_data) * train_split)\n",
        "data_train = input_data.iloc[:last_train_index]\n",
        "data_test = input_data.iloc[last_train_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "21-ENHTN25b1",
        "colab": {}
      },
      "source": [
        "data_train.labels_encoded.iloc[-1] != data_test.labels_encoded.iloc[0]  #Indicating that our splits of the data are correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcikqQjD092u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BalancedBatchSampler(BatchSampler):\n",
        "    \"\"\"\n",
        "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
        "    Returns batches of size n_classes * n_samples\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels, n_classes, n_samples):\n",
        "        self.labels = labels\n",
        "        self.labels_set = list(set(self.labels.to_numpy()))\n",
        "        self.label_to_indices = {label: np.where(self.labels.to_numpy() == label)[0]\n",
        "                                 for label in self.labels_set}\n",
        "        for l in self.labels_set:\n",
        "            np.random.shuffle(self.label_to_indices[l])\n",
        "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
        "        self.count = 0\n",
        "        self.n_classes = n_classes\n",
        "        self.n_samples = n_samples\n",
        "        self.n_dataset = len(self.labels)\n",
        "        self.batch_size = self.n_samples * self.n_classes\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        while self.count + self.batch_size < self.n_dataset:\n",
        "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
        "            indices = []\n",
        "            for class_ in classes:\n",
        "                indices.extend(self.label_to_indices[class_][\n",
        "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
        "                                                                         class_] + self.n_samples])\n",
        "                self.used_label_indices_count[class_] += self.n_samples\n",
        "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
        "                    np.random.shuffle(self.label_to_indices[class_])\n",
        "                    self.used_label_indices_count[class_] = 0\n",
        "            yield indices\n",
        "            self.count += self.n_classes * self.n_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_dataset // self.batch_size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJKJcWNgHArF",
        "colab_type": "text"
      },
      "source": [
        "# This is the definition of the Online Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-xIsmzV7Tfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OnlineTripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Online Triplets loss\n",
        "    Takes a batch of embeddings and corresponding labels.\n",
        "    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n",
        "    triplets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin, triplet_selector):\n",
        "        super(OnlineTripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.triplet_selector = triplet_selector\n",
        "\n",
        "    def forward(self, embeddings, target):\n",
        "\n",
        "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
        "\n",
        "        if embeddings.is_cuda:\n",
        "            triplets = triplets.cuda()\n",
        "\n",
        "        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n",
        "        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n",
        "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
        "\n",
        "        return losses.mean(), len(triplets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2lVJimGIx1O",
        "colab_type": "text"
      },
      "source": [
        "# This is a variation via the Triplet Loader because I could not find any other way out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48TpoQRCJLD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OnlineTripletDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Train: For each sample (anchor) randomly chooses a positive and negative samples\n",
        "    Test: Creates fixed triplets for testing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, select_column, training_mode=True):\n",
        "         \n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.train = training_mode\n",
        "        self.select_column = select_column\n",
        "        self.labels = self.dataset.labels_encoded\n",
        "        if self.train:\n",
        "            self.train_labels = self.dataset.labels_encoded\n",
        "            # Drop the labels column so that remaining features form part of the training set\n",
        "            self.dataset = self.dataset.drop('labels_encoded', axis=1)\n",
        "            self.train_data = self.dataset\n",
        "            self.labels_set = set(self.train_labels.to_numpy())\n",
        "            self.label_to_indices = {label: np.where(self.train_labels.to_numpy() == label)[0]\n",
        "                                     for label in self.labels_set} # redundent in our case if we decide to use numeric labels based on certain sklearn packages\n",
        "\n",
        "        else:\n",
        "            self.test_labels = self.dataset.labels_encoded\n",
        "            # Drop the labels column so that remaining features form part of the training set\n",
        "            self.dataset = self.dataset.drop('labels_encoded', axis=1)\n",
        "            self.test_data = self.dataset\n",
        "            # generate fixed triplets for testing\n",
        "            self.labels_set = set(self.test_labels.to_numpy())\n",
        "            self.label_to_indices = {label: np.where(self.test_labels.to_numpy() == label)[0]\n",
        "                                     for label in self.labels_set}\n",
        "\n",
        "            random_state = np.random.RandomState(42)\n",
        "            \n",
        "            #print(\"Length of the dataset is {}\".format(len(self.test_data)))\n",
        "            \n",
        "            triplets = []\n",
        "            for i in range(len(self.test_data)):\n",
        "                  triplets.append([i,\n",
        "                           random_state.choice(self.label_to_indices[self.test_labels.iloc[i]]),\n",
        "                           random_state.choice(self.label_to_indices[\n",
        "                                                 np.random.choice(\n",
        "                                                     list(self.labels_set - set([self.test_labels.iloc[i]]))\n",
        "                                                 )\n",
        "                                             ])\n",
        "                         ]) \n",
        "                         \n",
        "            self.test_triplets = triplets\n",
        "           \n",
        "    def __getitem__(self, index):\n",
        "        if self.train:\n",
        "            #print(type(self.train_data))\n",
        "            sent1, label1 = self.train_data.iloc[index], self.train_labels.iloc[index]\n",
        "            positive_index = index\n",
        "            while positive_index == index:\n",
        "                positive_index = np.random.choice(self.label_to_indices[label1])\n",
        "            negative_label = np.random.choice(list(self.labels_set - set([label1])))\n",
        "            negative_index = np.random.choice(self.label_to_indices[negative_label])\n",
        "            sent2 = self.train_data.iloc[positive_index]\n",
        "            sent3 = self.train_data.iloc[negative_index]\n",
        "            # Also include the labels under consideration\n",
        "            label2 = self.labels.iloc[positive_index]\n",
        "            label3 = self.labels.iloc[negative_index]\n",
        "        else:\n",
        "            sent1 = self.test_data.iloc[self.test_triplets[index][0]]\n",
        "            sent2 = self.test_data.iloc[self.test_triplets[index][1]]\n",
        "            sent3 = self.test_data.iloc[self.test_triplets[index][2]]\n",
        "            label1 = self.labels.iloc[self.test_triplets[index][0]]\n",
        "            label2 = self.labels.iloc[self.test_triplets[index][1]]\n",
        "            label3 = self.labels.iloc[self.test_triplets[index][2]]\n",
        "        # Filter to select only the required column rather than all\n",
        "        sent1 = sent1[self.select_column]\n",
        "        sent2 = sent2[self.select_column]\n",
        "        sent3 = sent3[self.select_column]\n",
        "        \n",
        "\n",
        "        return ((sent1,label1), (sent2,label2), (sent3,label3))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P99BAFNhZBwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "  This cell should be basically `True` indicating that the two datasets have all the\n",
        "  labels ie shuffling done earlier was meaninigful\n",
        "\"\"\"\n",
        "set(data_train.labels_encoded) == set(data_test.labels_encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZOD7HvJ3zKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "siamese_train_dataset = OnlineTripletDataset(training_mode=True,dataset=data_train, select_column='sentence_embedding')\n",
        "siamese_test_dataset = OnlineTripletDataset(training_mode=False,dataset=data_test, select_column='sentence_embedding')\n",
        "\n",
        "batch_size = 5\n",
        "# Putting in the batch sampler here. Again, we need two batch samplers, one for each type\n",
        "\n",
        "# We'll create mini batches by sampling labels that will be present in the mini batch and number of examples from each datatype\n",
        "train_batch_sampler = BalancedBatchSampler(data_train.labels_encoded, n_classes=23, n_samples=batch_size) # Number of classes in the current dataset is 23\n",
        "val_batch_sampler = BalancedBatchSampler(data_test.labels_encoded, n_classes=23, n_samples=batch_size) # Number of classes in the current dataset is 23\n",
        "\n",
        "\n",
        "online_train_loader = DataLoader(siamese_train_dataset, batch_sampler=train_batch_sampler)\n",
        "online_val_loader = DataLoader(siamese_test_dataset, batch_sampler=val_batch_sampler)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrDgswhZ4HNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        " Quick sanity check for the batch samplers\n",
        " The outpur shall be 23 times batch size ie we have elements from each of the categories\n",
        "\"\"\"\n",
        "\n",
        "for index, (img1,img2,img3) in enumerate(online_train_loader):\n",
        "  print (img1[0].shape)\n",
        "  print (img2[0].shape)\n",
        "  print (img3[0].shape)\n",
        "  break\n",
        "\n",
        "print(\"Let us test the scenario for validation dataloader\")\n",
        "  \n",
        "for index, (img1,img2,img3) in enumerate(online_val_loader):\n",
        "  print (img1[0].shape)\n",
        "  print (img2[0].shape)\n",
        "  print (img3[0].shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR61vY4tM-SW",
        "colab_type": "text"
      },
      "source": [
        "# Modified Siamese Network to be called Triplet Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbJFhGMaM-A3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TripletNetwork(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(TripletNetwork, self).__init__()\n",
        "\n",
        "        self.input_dim=1024\n",
        "        self.fc = nn.Sequential(nn.Linear(1024, 512),\n",
        "                                  nn.BatchNorm1d(num_features=512),\n",
        "                                  nn.ReLU(),\n",
        "                                  #nn.Dropout(),\n",
        "                                  nn.Linear(512,256),\n",
        "                                  nn.BatchNorm1d(num_features=256),\n",
        "                                  nn.ReLU(),\n",
        "                                  #nn.Dropout(p=0.6),\n",
        "                                  nn.Linear(256,64)\n",
        "                               )\n",
        "       \n",
        "    def forward(self, x1, x2, x3):\n",
        "       \n",
        "        \"\"\"\n",
        "          We updated the network now and are directly getting the sentence embeddings\n",
        "        \"\"\"\n",
        "        # These are the embeddings and what we want to achieve is to make\n",
        "        # these embeddings which should be in the same region denoting their language to\n",
        "        # fall a bit away based on the task at hand\n",
        "        output1 = self.fc(x1.cuda())\n",
        "        output2 = self.fc(x2.cuda())\n",
        "        output3 = self.fc(x3.cuda())\n",
        "        \n",
        "        return (output1, output2, output3)\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.encoder.encode_sentences(sentences = x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlDNl66-7Ueo",
        "colab_type": "text"
      },
      "source": [
        "# This is the Triplet Selection code that is being used in this place"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzTqQdqS7T2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pdist(vectors):\n",
        "  \"\"\"\n",
        "    This is the basic definition of the distance matrix from a vector\n",
        "  \"\"\"\n",
        "  distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n",
        "      dim=1).view(-1, 1)\n",
        "  return distance_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Odz1NjZ4LN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "class FunctionNegativeTripletSelector():\n",
        "    \"\"\"\n",
        "    For each positive pair, takes the hardest negative sample (with the greatest triplet loss value) to create a triplet\n",
        "    Margin should match the margin used in triplet loss.\n",
        "    negative_selection_fn should take array of loss_values for a given anchor-positive pair and all negative samples\n",
        "    and return a negative index for that pair\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin, negative_selection_fn, cpu=False):\n",
        "        super(FunctionNegativeTripletSelector, self).__init__()\n",
        "        self.cpu = cpu\n",
        "        self.margin = margin\n",
        "        self.negative_selection_fn = negative_selection_fn\n",
        "\n",
        "    def get_triplets(self, embeddings, labels):\n",
        "        if self.cpu:\n",
        "            embeddings = embeddings.cpu()\n",
        "        distance_matrix = pdist(embeddings)\n",
        "        distance_matrix = distance_matrix.cpu()\n",
        "        \n",
        "        labels = labels.cpu().data.numpy()\n",
        "        triplets = []\n",
        "        for label in set(labels):\n",
        "            label_mask = (labels == label)\n",
        "            label_indices = np.where(label_mask)[0]\n",
        "            if len(label_indices) < 2:\n",
        "                continue\n",
        "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
        "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
        "            anchor_positives = np.array(anchor_positives)\n",
        "\n",
        "            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n",
        "            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n",
        "                loss_values = ap_distance - distance_matrix[torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n",
        "                loss_values = loss_values.data.cpu().numpy()\n",
        "                hard_negative = self.negative_selection_fn(loss_values)\n",
        "                if hard_negative is not None:\n",
        "                    hard_negative = negative_indices[hard_negative]\n",
        "                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n",
        "\n",
        "        if len(triplets) == 0:\n",
        "            triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n",
        "\n",
        "        triplets = np.array(triplets)\n",
        "\n",
        "        return torch.LongTensor(triplets)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX7FRICw7HYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hardest_negative(loss_values):\n",
        "    hard_negative = np.argmax(loss_values)\n",
        "    return hard_negative if loss_values[hard_negative] > 0 else None\n",
        "\n",
        "\n",
        "def random_hard_negative(loss_values):\n",
        "    hard_negatives = np.where(loss_values > 0)[0]\n",
        "    return np.random.choice(hard_negatives) if len(hard_negatives) > 0 else None\n",
        "\n",
        "\n",
        "def semihard_negative(loss_values, margin):\n",
        "    semihard_negatives = np.where(np.logical_and(loss_values < margin, loss_values > 0))[0]\n",
        "    return np.random.choice(semihard_negatives) if len(semihard_negatives) > 0 else None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB-fLNLG6i_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def HardestNegativeTripletSelector(margin, cpu=False):\n",
        "  return FunctionNegativeTripletSelector(margin=margin,negative_selection_fn=hardest_negative, cpu=cpu)\n",
        "\n",
        "\n",
        "def RandomNegativeTripletSelector(margin, cpu=False): \n",
        "  return FunctionNegativeTripletSelector(margin=margin, negative_selection_fn=random_hard_negative, cpu=cpu)\n",
        "\n",
        "\n",
        "def SemihardNegativeTripletSelector(margin, cpu=False): \n",
        "  return FunctionNegativeTripletSelector(margin=margin, negative_selection_fn=lambda x: semihard_negative(x, margin), cpu=cpu)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSMiDjZWSbGe",
        "colab_type": "text"
      },
      "source": [
        "# This train code is for the Online Triplet selection network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXI9oTnwV3vs",
        "colab_type": "text"
      },
      "source": [
        "## Creating new event objects for Tensorboard. Visualisation on previous dashboard possible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccXh8-EOQ_vU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "margin = 2.\n",
        "model = TripletNetwork()\n",
        "model.cuda()\n",
        "loss_fn = OnlineTripletLoss(margin,triplet_selector=SemihardNegativeTripletSelector(margin=margin))\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
        "log_interval = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEpFqPbhjqpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_writer = SummaryWriter('/content/logs/tensorboard/round2/train/')\n",
        "val_writer = SummaryWriter('/content/logs/tensorboard/round2/val/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFoyQ0PNSajZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval):\n",
        "    \n",
        "    model.train()\n",
        "    losses = []\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, ((sent1,label1),(sent2,label2),(sent3,label3)) in enumerate(train_loader):\n",
        "        labels = []\n",
        "        batch_size = sent1.shape[0]\n",
        "        sent1 = sent1.reshape(batch_size,-1)\n",
        "        sent2 = sent2.reshape(batch_size,-1)\n",
        "        sent3 = sent3.reshape(batch_size,-1)\n",
        "        labels.extend(label1)\n",
        "        labels.extend(label2)\n",
        "        labels.extend(label3)\n",
        "        #print(labels)\n",
        "        labels = torch.Tensor(labels).cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sent1,sent2,sent3) \n",
        "        loss_inputs = outputs\n",
        "        \n",
        "        # We have three outputs, index 0 shall be for anchor, 1 is positive and 2 negative\n",
        "        # please double check\n",
        "        \n",
        "        loss_inputs = torch.cat((outputs[0],outputs[1], outputs[2]), 0)\n",
        "        \n",
        "        #print()\n",
        "        \n",
        "        loss_outputs = loss_fn(loss_inputs, labels) #We assume that target value is not known and hence is None\n",
        "        \n",
        "        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
        "        losses.append(loss.item())\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                batch_idx * len(img1), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), np.mean(losses))\n",
        "            \n",
        "            print(message)\n",
        "            losses = []\n",
        "\n",
        "    total_loss /= (batch_idx + 1)\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlO5Ja_PXyyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_epoch(val_loader, model, loss_fn, cuda):\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        val_loss = 0\n",
        "        for batch_idx, ((sent1,label1),(sent2,label2),(sent3,label3)) in enumerate(val_loader):\n",
        "            \n",
        "            labels = []\n",
        "            batch_size = sent1.shape[0]\n",
        "            sent1 = sent1.reshape(batch_size,-1)\n",
        "            sent2 = sent2.reshape(batch_size,-1)\n",
        "            sent3 = sent3.reshape(batch_size,-1)\n",
        "            labels.extend(label1)\n",
        "            labels.extend(label2)\n",
        "            labels.extend(label3)\n",
        "        \n",
        "            labels = torch.Tensor(labels).cuda()\n",
        "          \n",
        "            outputs = model(sent1,sent2,sent3) \n",
        "\n",
        "            \n",
        "            loss_inputs = torch.cat((outputs[0],outputs[1], outputs[2]), 0)\n",
        "            \n",
        "\n",
        "            loss_outputs = loss_fn(loss_inputs, labels) #We assume that target value is not known and hence is None\n",
        "            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            \n",
        "    return val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BX4rsUm8053",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval,\n",
        "        start_epoch=0):\n",
        "    \"\"\"\n",
        "    Loaders, model, loss function and metrics should work together for a given task,\n",
        "    i.e. The model should be able to process data output of loaders,\n",
        "    loss function should process target output of loaders and outputs from the model\n",
        "\n",
        "    Examples: Classification: batch loader, classification model, NLL loss, accuracy metric\n",
        "    Siamese network: Siamese loader, siamese model, contrastive loss\n",
        "    Online triplet learning: batch loader, embedding model, online triplet loss\n",
        "    \"\"\"\n",
        "    for epoch in range(0, start_epoch):\n",
        "        scheduler.step()\n",
        "\n",
        "    for epoch in range(start_epoch, n_epochs):\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Train stage\n",
        "        train_loss = train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval)\n",
        "        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)\n",
        "        \n",
        "        train_writer.add_scalar('loss', train_loss,epoch)\n",
        "\n",
        "        print(message)\n",
        "        val_loss = test_epoch(val_loader, model, loss_fn, cuda)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        message += '\\nEpoch: {}/{}. Validation set: Average loss: {:.4f}'.format(epoch + 1, n_epochs,\n",
        "                                                                                     val_loss)\n",
        "        print(message)\n",
        "        torch.save(model.state_dict(), './{}-model.pth'.format(epoch))\n",
        "        torch.save(optimizer.state_dict(),'./{}-optim.pth'.format(epoch) ) #we need to save both for Adam\n",
        "\n",
        "        val_writer.add_scalar('loss', val_loss,epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S01rUUY8802M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = True\n",
        "n_epochs = 25\n",
        "fit(online_train_loader, online_val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda,log_interval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz5spps0aAKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lm8u8R4O21M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir /content/logs/tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqIjnEpERaOG",
        "colab_type": "text"
      },
      "source": [
        "## Experimental Section of applying Softmax on the embedding space to see the output. This is bit of a change from the normal operation of our Triplet Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVI_Mn79OkcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassificationNet(nn.Module):\n",
        "  \n",
        "\n",
        "  def __init__(self, embedding_network, num_classes):\n",
        "    super(ClassificationNet, self).__init__()\n",
        "    self.embedding_network = embedding_network #We would like to freeze these embeddings\n",
        "    self.num_classes = num_classes\n",
        "    self.nonlinear = nn.ReLU()\n",
        "    self.fc1 = nn.Linear(64, 64)\n",
        "    #self.fc2 = nn.Linear(64, 32)\n",
        "    self.fc3 = nn.Linear(64, self.num_classes)\n",
        "    # Freeze the weights of this network\n",
        "    \"\"\" This attempt is to first allow classification network to train\"\"\"\n",
        "    for param in self.embedding_network.fc.parameters():\n",
        "           param.requires_grad = False\n",
        "    \n",
        "  \n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.embedding_network.fc(x)\n",
        "    x = self.fc1(self.nonlinear(x))\n",
        "    #x = self.fc2(self.nonlinear(x))\n",
        "    x = self.fc3(self.nonlinear(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgCvK3yUUVlw",
        "colab_type": "text"
      },
      "source": [
        "### Dummy dataset generation for training the FC and softmax layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX8B_3GoT9c7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to define a specific dataloader in this case, which shall be frankly, quite simple and dummy\n",
        "class SoftmaxDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Train: For each sample (anchor) randomly chooses a positive and negative samples\n",
        "    Test: Creates fixed triplets for testing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, select_column):\n",
        "         \n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.train_labels = self.dataset.labels_encoded\n",
        "            # Drop the labels column so that remaining features form part of the training set\n",
        "        self.dataset = self.dataset.drop('labels_encoded', axis=1)\n",
        "        self.train_data = self.dataset\n",
        "        self.select_column = select_column\n",
        "       \n",
        "    def __getitem__(self, index):\n",
        "        #print(type(self.train_data))\n",
        "        selected_frame, label = self.train_data.iloc[index], self.train_labels.iloc[index]\n",
        "        str_data = selected_frame[self.select_column]\n",
        "\n",
        "        return str_data, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy6Nq9ZnlJPc",
        "colab_type": "text"
      },
      "source": [
        "## overfit on a smaller dataset but only for initial evaluation. Ignore later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irnA5T-T_TVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "miniature_data_train = data_train[:10000]\n",
        "miniature_data_test = data_test[:500]\n",
        "# Change the next cell with the correct values\n",
        "batch_size = 10\n",
        "num_epochs=30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V7jVh1KkoEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax_train_dataset = SoftmaxDataset(dataset=miniature_data_train,select_column='sentence_embedding')\n",
        "softmax_test_dataset = SoftmaxDataset(dataset=miniature_data_test,select_column='sentence_embedding')\n",
        "# Now the dataloaders need to be defined as well\n",
        "classification_train_loader = DataLoader(softmax_train_dataset, batch_size=batch_size,shuffle=True)\n",
        "classification_test_loader = DataLoader(softmax_test_dataset, batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT4YawdSkoA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(data_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBeET4QrnUzg",
        "colab_type": "text"
      },
      "source": [
        "##Actual data loader for the model that we plan to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC0nL-sYSOcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Global declarations\n",
        "num_classes=23\n",
        "num_epochs = 40\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MjSwcL7J0zZm",
        "colab": {}
      },
      "source": [
        "# Load a pre-existing model \n",
        "PATH = '/content/7-model.pth'\n",
        "model = TripletNetwork()\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzXYFUDYRZtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 23\n",
        "net = ClassificationNet(embedding_network=model, num_classes=num_classes)\n",
        "net.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4kYkHpsRaoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptV1nJq2YM4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax_train_dataset = SoftmaxDataset(dataset=data_train,select_column='sentence_embedding')\n",
        "softmax_test_dataset = SoftmaxDataset(dataset=data_test,select_column='sentence_embedding')\n",
        "# Now the dataloaders need to be defined as well\n",
        "classification_train_loader = DataLoader(softmax_train_dataset, batch_size=batch_size,shuffle=True)\n",
        "classification_test_loader = DataLoader(softmax_test_dataset, batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WfPWuYg3GkKg",
        "colab": {}
      },
      "source": [
        "train_writer = SummaryWriter('/content/logs/tensorboard/train/')\n",
        "val_writer = SummaryWriter('/content/logs/tensorboard/val/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXkjJP4dQ0m1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next we need to train this network in order to learn the weights of the fc layer and cross entropy weights\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(classification_train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        #reshape the input to align\n",
        "        batch_size = inputs.shape[0]\n",
        "        inputs = inputs.reshape(batch_size,-1)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Write the scalars\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).cpu().sum().item()\n",
        "    print(\"loss is {}\".format(running_loss/total))   \n",
        "    print('Accuracy of the network on the train samples: {} '.format((100 * correct / total)))\n",
        "    train_writer.add_scalar('Loss', running_loss/total,epoch)\n",
        "    train_writer.add_scalar('Accuracy', 100*correct/total,epoch)\n",
        "    # Now this is the section for testing the model in the same epoch\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_val = 0\n",
        "    with torch.no_grad():\n",
        "        for data in classification_test_loader:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.cuda()\n",
        "            batch_size = inputs.shape[0]\n",
        "            inputs = inputs.reshape(batch_size,-1)\n",
        "            # Next the evaluation\n",
        "            outputs = net(inputs)\n",
        "            labels = labels.cuda()\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss_val = loss_val + loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).cpu().sum().item()\n",
        "            \n",
        "    print('Accuracy of the network on the test samples:{}'.format(\n",
        "        (100 * correct / total)))\n",
        "    print(\"loss validation set is {}\".format(loss_val/total)) \n",
        "    torch.save(net.state_dict(), '/content/{}-classifier.pth'.format(epoch))\n",
        "    val_writer.add_scalar('Loss', loss_val/total,epoch)\n",
        "    val_writer.add_scalar('Accuracy', 100*correct/total,epoch)\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rct7EmUMGzHI",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir /content/logs/tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCnOhz2Yg3T6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First load the best model based on the classification scenario above\n",
        "net = ClassificationNet(embedding_network=model, num_classes=num_classes)\n",
        "PATH = '/content/15-classifier.pth'\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "net.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsGspogEg3dW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Still not ready. Please refer to the KNN part for now\n",
        "all_labels = []\n",
        "all_predictions=[]\n",
        "with torch.no_grad():\n",
        "        for data in classification_test_loader:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.cuda()\n",
        "            batch_size = inputs.shape[0]\n",
        "            inputs = inputs.reshape(batch_size,-1)\n",
        "            # Next the evaluation\n",
        "            outputs = net(inputs)\n",
        "            labels = labels.cuda()\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss_val = loss_val + loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            for idx in range(batch_size):\n",
        "                all_predictions.append(predicted[idx].item())\n",
        "                all_labels.append(labels[idx].item())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7IOcTdmFInnH",
        "colab": {}
      },
      "source": [
        "labels = list(set(data_train.labels_encoded))\n",
        "cm = confusion_matrix(all_labels, all_predictions, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JClpBpSJK-sD",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(cm)\n",
        "plt.title('Confusion matrix of the classifier')\n",
        "fig.colorbar(cax)\n",
        "ax.set_xticklabels([''] + get_all_labels())\n",
        "ax.set_yticklabels([''] + get_all_labels())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "83WzcFdcK-sJ",
        "colab": {}
      },
      "source": [
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=False, ax = ax, fmt='g', cmap='Greens'); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels([''] + get_all_labels());\n",
        "ax.yaxis.set_ticklabels([''] + get_all_labels());"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sTu_7QLyK-sN",
        "colab": {}
      },
      "source": [
        "print(classification_report(all_labels, all_predictions,labels=labels, target_names=get_all_labels()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AgLXPXLlwUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "all_labels_binary = label_binarize(all_labels, classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22])\n",
        "all_predictions_binary = label_binarize(all_predictions, classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22])\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(all_labels_binary[:, i], all_predictions_binary[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot of a ROC curve for a specific class\n",
        "labels = get_all_labels()\n",
        "for i in range(num_classes):\n",
        "    plt.figure()\n",
        "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic for {}'.format(get_lebel_from_code(i)))\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUtQOIrhlm7D",
        "colab_type": "text"
      },
      "source": [
        "### Putting all ROC values in a single curve for clarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9K6pYWuFffjy",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "all_labels_binary = label_binarize(all_labels, classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22])\n",
        "all_predictions_binary = label_binarize(all_predictions, classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22])\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(all_labels_binary[:, i], all_predictions_binary[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot of a ROC curve for a specific class\n",
        "labels = get_all_labels()\n",
        "for i in range(num_classes):\n",
        "    plt.plot(fpr[i], tpr[i],\n",
        "             label='ROC curve of {0} (area = {1:0.2f})'\n",
        "             ''.format(get_lebel_from_code(i), roc_auc[i]))\n",
        "  \n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "#plt.title('Receiver operating characteristic for {}'.format(get_lebel_from_code(i)))\n",
        "plt.legend(loc=(1.04,0))\n",
        "plt.savefig('roc.png',dpi=400,bbox_inches='tight')\n",
        "plt.show()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jElqeitX7QCQ",
        "colab_type": "text"
      },
      "source": [
        "# Finally here we try and implement the 1-Nearest classification process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy2klJSDDVUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a pre-existing model \n",
        "PATH = '/content/11-model-GOOD.pth'\n",
        "model = TripletNetwork()\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y4ChZ6L85qA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First of all, our model shall be the one we trained for embeddings in the \n",
        "# 60 dimensional space using Triplet loss. So,\n",
        "embedding_model = model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y07EHij5G7TR",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "softmax_train_dataset = SoftmaxDataset(dataset=data_train,select_column='sentence_embedding')\n",
        "softmax_test_dataset = SoftmaxDataset(dataset=data_test,select_column='sentence_embedding')\n",
        "# Now the dataloaders need to be defined as well\n",
        "classification_train_loader = DataLoader(softmax_train_dataset, batch_size=batch_size,shuffle=True)\n",
        "classification_test_loader = DataLoader(softmax_test_dataset, batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxkf2Kad8iTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us save the embedding of all elements in the train_data as well as their index\n",
        "# this can be used later for our purpose\n",
        "embedding_space = []\n",
        "for i, data in enumerate(classification_train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda()\n",
        "        batch_size = inputs.shape[0]\n",
        "        inputs = inputs.reshape(batch_size,-1)\n",
        "        labels = labels.cuda()\n",
        "        embeddings = embedding_model.fc(inputs)\n",
        "        for index,tensor in enumerate(embeddings):\n",
        "           embedding_space.append((labels[index],tensor))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vrATUP_LQ_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist = [space for index,space in embedding_space[:-1]]\n",
        "result = torch.stack(dist, dim=0)\n",
        "result.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP1KZCSc6-yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#select the norm\n",
        "def select_norm(vector_x, power=2):\n",
        "  return torch.norm(vector_x,p=power,dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4yPqZrBIPtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_indices_of_most_common_neighbours(neighbours_list, candidate):\n",
        "  indices = []\n",
        "  for index,val in enumerate(neighbours_list):\n",
        "      if val == candidate:\n",
        "          indices.append(index)\n",
        "  return indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9k06iaKSBHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_prob(dist_values, indices):\n",
        "  inverse_distance = 1/dist_values\n",
        "  return inverse_distance[indices].sum().item()/inverse_distance.sum().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN3wtiUuLRHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k_nearest_neighbour(distance_embeddings, vector, k=3):  \n",
        "    updated_result = select_norm(vector_x=(distance_embeddings - vector), power=2)\n",
        "    dist_values, indices = torch.topk(updated_result,k=k ,largest=False)\n",
        "    candidate_indexes = []\n",
        "\n",
        "    for index in indices:\n",
        "      candidate_indexes.append((embedding_space[index][0]).item())  \n",
        "    # Earlier we were just returning the most common value\n",
        "    # Now we also want to return the associated\n",
        "    # probabilities \n",
        "    most_common_neighbour = Counter(candidate_indexes).most_common(1)[0][0] #1 indicates most frequent element\n",
        "    # convert the list into numpy array for ease \n",
        "    candidate_indexes_numpy = np.asarray(candidate_indexes)\n",
        "    indices = get_indices_of_most_common_neighbours(neighbours_list=candidate_indexes_numpy, candidate= most_common_neighbour)\n",
        "    prob_value = generate_prob(dist_values, indices)\n",
        "    # getting probability based on distance sum\n",
        "    \n",
        "    return most_common_neighbour, prob_value\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMlN9hFQ_pdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "all_labels = []\n",
        "prob_values = []\n",
        "all_predictions=[]\n",
        "for data in classification_test_loader:\n",
        "    inputs, labels = data\n",
        "    inputs = inputs.cuda()\n",
        "    batch_size = inputs.shape[0]\n",
        "    inputs = inputs.reshape(batch_size,-1)\n",
        "    # Next the evaluation\n",
        "    \n",
        "    output = embedding_model.fc(inputs)\n",
        "    for idx,query in enumerate(output):\n",
        "        predicted, prob = k_nearest_neighbour(result, query, 5)\n",
        "        correct += (predicted == labels[idx])\n",
        "        prob_values.append(prob)\n",
        "        all_labels.append(labels[idx].item())\n",
        "        all_predictions.append(predicted)\n",
        "    total += labels.size(0)\n",
        "    correct = correct.cpu().sum()\n",
        "print('Accuracy of the network on the test samples: %d %%' % (\n",
        "        100 * correct /total))\n",
        "print(prob_values[:20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6dTNg9X_C5P",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating the Micro and Macro F1 scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpUVFVx9_B_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1_score(all_labels, all_predictions, average='micro') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Qw9O4BBu1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1_score(all_labels, all_predictions, average='macro') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_tN-b-SBLPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = list(set(data_train.labels_encoded))\n",
        "cm = confusion_matrix(all_labels, all_predictions, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYrGlrsnDJiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(cm)\n",
        "plt.title('Confusion matrix of the classifier')\n",
        "fig.colorbar(cax)\n",
        "ax.set_xticklabels([''] + get_all_labels())\n",
        "ax.set_yticklabels([''] + get_all_labels())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2Qt2zWlEhnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=False, ax = ax, fmt='g', cmap='Greens'); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels([''] + get_all_labels());\n",
        "ax.yaxis.set_ticklabels([''] + get_all_labels());"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZvX13V9A4bZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy_score(all_labels, all_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SbT8LFAGBM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(all_labels, all_predictions,labels=labels, target_names=get_all_labels()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTIuwz6lw15K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_prob_score_labels (all_labels):\n",
        "    new_arr = np.zeros((len(all_labels),23))\n",
        "    for row,index in enumerate(all_labels):\n",
        "        new_arr[row][index] = prob_values[row]\n",
        "    return new_arr\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EIffCoLzwuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_labels_binary = label_binarize(all_labels, classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22])\n",
        "all_predictions_score = generate_prob_score_labels(all_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpZYo8E9y6Nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first = all_labels_binary != 0\n",
        "second = all_predictions_score != 0\n",
        "(first != second).sum() #Should be a zero"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOomtYd7OQDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_predictions_score[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsLGvFTA8pVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(all_labels_binary[:, i], all_predictions_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot of a ROC curve for a specific class\n",
        "labels = get_all_labels()\n",
        "for i in range(num_classes):\n",
        "    plt.figure()\n",
        "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic for {}'.format(get_lebel_from_code(i)))\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KXN1cr3l8fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "#all_labels_binary = label_binarize(all_labels, classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22])\n",
        "#all_predictions_binary = label_binarize(all_predictions, classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22])\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(all_labels_binary[:, i], all_predictions_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot of a ROC curve for a specific class\n",
        "labels = get_all_labels()\n",
        "for i in range(num_classes):\n",
        "    plt.plot(fpr[i], tpr[i],\n",
        "             label='ROC curve of {0} (area = {1:0.2f})'\n",
        "             ''.format(get_lebel_from_code(i), roc_auc[i]))\n",
        "  \n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "#plt.title('Receiver operating characteristic for {}'.format(get_lebel_from_code(i)))\n",
        "plt.legend(loc=(1.04,0))\n",
        "plt.savefig('rocNN.png',dpi=400,bbox_inches='tight')\n",
        "plt.show()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsWM7ebZVKEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRhp-_F8Btvh",
        "colab_type": "text"
      },
      "source": [
        "#tSNE implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcSB73uOBtVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result.shape\n",
        "input_tsne = result.cpu().detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VglvddPbBtQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from sklearn.manifold import TSNE\n",
        "time_start = time.time()\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "tsne_results = tsne.fit_transform(input_tsne)\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcxUvEidSqgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_all = [label.cpu().numpy() for label,_ in embedding_space]\n",
        "label_all[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg2w3qovRWL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subset_size = 10000\n",
        "tsne_results_subset = tsne_results[:subset_size]\n",
        "label_all_subset = label_all[:subset_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5_ddTiaJzc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = []\n",
        "tokens = []\n",
        "\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "for value in tsne_results_subset:\n",
        "    x.append(value[0])\n",
        "    y.append(value[1])\n",
        "\n",
        "plt.figure(figsize=(16, 16)) \n",
        "for i in range(len(x)):\n",
        "    plt.scatter(x[i],y[i])\n",
        "    plt.annotate(label_all_subset[i],\n",
        "                 xy=(x[i], y[i]),\n",
        "                 xytext=(5, 2),\n",
        "                 textcoords='offset points',\n",
        "                 ha='right',\n",
        "                 va='bottom')\n",
        "plt.savefig(\"tsne.png\", dpi = 500)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSk5SELzzXn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWsBHLDFv0Gc",
        "colab_type": "text"
      },
      "source": [
        "# Include here the section for using training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Vo2FWUWwnbv",
        "colab": {}
      },
      "source": [
        "%cd /content/\n",
        "test_data = pd.read_pickle('/content/drive/My Drive/dtest_preprocessed_sentence_encoded.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5XVwbe1mwnb2",
        "colab": {}
      },
      "source": [
        "len(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVwn-HkK55rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u3Ma3g058vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data.labels_encoded.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "frEPFus32aYk",
        "colab": {}
      },
      "source": [
        "# Let us split the data into train and test sections. This we can use later for our purpose\n",
        "# interestingly, missing out on shuffling would lead to unexpected results\n",
        "# so we need to take care of that as well\n",
        "# This command shuffles the dataset completely\n",
        "test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
        "# now we split it\n",
        "train_split = 0.8\n",
        "last_train_index = int(len(test_data) * train_split)\n",
        "data_one_shot_train = test_data.iloc[:last_train_index]\n",
        "data_one_shot_test = test_data.iloc[last_train_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBg39WJL2aYs",
        "colab": {}
      },
      "source": [
        "data_one_shot_train.labels_encoded.iloc[-1] != data_one_shot_test.labels_encoded.iloc[0]  #Indicating that our splits of the data are correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ykFCG_mlxVnE",
        "colab": {}
      },
      "source": [
        "# We need to define a specific dataloader in this case, which shall be frankly, quite simple and dummy\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Train: For each sample (anchor) randomly chooses a positive and negative samples\n",
        "    Test: Creates fixed triplets for testing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, select_column):\n",
        "         \n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.train_labels = self.dataset.labels_encoded\n",
        "            # Drop the labels column so that remaining features form part of the training set\n",
        "        self.dataset = self.dataset.drop('labels_encoded', axis=1)\n",
        "        self.train_data = self.dataset\n",
        "        self.select_column = select_column\n",
        "       \n",
        "    def __getitem__(self, index):\n",
        "        #print(type(self.train_data))\n",
        "        selected_frame, label = self.train_data.iloc[index], self.train_labels.iloc[index]\n",
        "        str_data = selected_frame[self.select_column]\n",
        "\n",
        "        return str_data, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1bcxIkQnx40i",
        "colab": {}
      },
      "source": [
        "# Global declarations\n",
        "num_classes=30\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MudBi6zkzUwF",
        "colab_type": "text"
      },
      "source": [
        "## Load the best TripletNetwork Model in this step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H0e5-Rt-zkSt",
        "colab": {}
      },
      "source": [
        "# Load a pre-existing model \n",
        "PATH = '/content/11-model-GOOD.pth'\n",
        "model = TripletNetwork()\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YzIwK9oNzkS8",
        "colab": {}
      },
      "source": [
        "# First of all, our model shall be the one we trained for embeddings in the \n",
        "# 60 dimensional space using Triplet loss. So,\n",
        "embedding_model = model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VquKgFqvzkTA",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "one_shot_train_dataset = SoftmaxDataset(dataset=data_one_shot_train,select_column='sentence_embedding')\n",
        "one_shot_test_dataset = SoftmaxDataset(dataset=data_one_shot_test,select_column='sentence_embedding')\n",
        "# Now the dataloaders need to be defined as well\n",
        "one_shot_train_loader = DataLoader(one_shot_train_dataset, batch_size=batch_size,shuffle=True)\n",
        "one_shot_test_loader = DataLoader(one_shot_test_dataset, batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KChbo0NR0xlH",
        "colab_type": "text"
      },
      "source": [
        "##  Generating some set of embeddings before getting train examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6rop9jfqzkTE",
        "colab": {}
      },
      "source": [
        "# Let us save the embedding of all elements in the train_data as well as their index\n",
        "# this can be used later for our purpose\n",
        "embedding_space = []\n",
        "for i, data in enumerate(one_shot_train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda()\n",
        "        batch_size = inputs.shape[0]\n",
        "        inputs = inputs.reshape(batch_size,-1)\n",
        "        labels = labels.cuda()\n",
        "        embeddings = embedding_model.fc(inputs)\n",
        "        for index,tensor in enumerate(embeddings):\n",
        "           embedding_space.append((labels[index],tensor))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ppZ_kSqnzkTK",
        "colab": {}
      },
      "source": [
        "dist = [space for index,space in embedding_space[:-1]]\n",
        "result_one_shot = torch.stack(dist, dim=0)\n",
        "result_one_shot.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GlmGHMbl15lJ",
        "colab": {}
      },
      "source": [
        "#select the norm\n",
        "def select_norm(vector_x, power=2):\n",
        "  return torch.norm(vector_x,p=power,dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYujp8fJzpt3",
        "colab_type": "text"
      },
      "source": [
        "## KNN Code Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SpFLKZlxyGEG",
        "colab": {}
      },
      "source": [
        "def k_nearest_neighbour(distance_embeddings, vector, k=3):  \n",
        "    updated_result = select_norm(vector_x=(distance_embeddings - vector), power=2)\n",
        "    values, indices = torch.topk(updated_result,k=k ,largest=False)\n",
        "    candidate_indexes = []\n",
        "    for index in indices:\n",
        "      candidate_indexes.append((embedding_space[index][0]).item())\n",
        "    return Counter(candidate_indexes).most_common(1)[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvVTH2QdzKiD",
        "colab_type": "text"
      },
      "source": [
        "## The embeddings that were generated by our best model needs to be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RvfnqzTiyGEK",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "all_labels = []\n",
        "all_predictions=[]\n",
        "for data in one_shot_test_loader:\n",
        "    inputs, labels = data\n",
        "    inputs = inputs.cuda()\n",
        "    batch_size = inputs.shape[0]\n",
        "    inputs = inputs.reshape(batch_size,-1)\n",
        "    # Next the evaluation\n",
        "    \n",
        "    output = embedding_model.fc(inputs)\n",
        "    for idx,query in enumerate(output):\n",
        "        predicted = k_nearest_neighbour(result_one_shot, query, 5)\n",
        "        all_predictions.append(predicted)\n",
        "        correct += (predicted == labels[idx])\n",
        "        all_labels.append(labels[idx].item())\n",
        "    total += labels.size(0)\n",
        "    correct = correct.cpu().sum()\n",
        "print('Accuracy of the network on the test samples: %d %%' % (\n",
        "        100 * correct /total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaGWMXTH7KSy",
        "colab_type": "text"
      },
      "source": [
        "# Finally the tSNE for the samples that we see now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hZAeFxE27cLN",
        "colab": {}
      },
      "source": [
        "result_one_shot.shape\n",
        "input_tsne = result_one_shot.cpu().detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gXaOrMxk7cLT",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from sklearn.manifold import TSNE\n",
        "time_start = time.time()\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "tsne_results = tsne.fit_transform(input_tsne)\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZuuWGVlu7cLW",
        "colab": {}
      },
      "source": [
        "label_all = [label.cpu().numpy() for label,_ in embedding_space]\n",
        "label_all[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8iBHfPTU7cLY",
        "colab": {}
      },
      "source": [
        "subset_size = 10000\n",
        "tsne_results_subset = tsne_results[:subset_size]\n",
        "label_all_subset = label_all[:subset_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d1H6vFvY7cLe",
        "colab": {}
      },
      "source": [
        "labels = []\n",
        "tokens = []\n",
        "\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "for value in tsne_results_subset:\n",
        "    x.append(value[0])\n",
        "    y.append(value[1])\n",
        "\n",
        "plt.figure(figsize=(16, 16)) \n",
        "for i in range(len(x)):\n",
        "    plt.scatter(x[i],y[i])\n",
        "    plt.annotate(label_all_subset[i],\n",
        "                 xy=(x[i], y[i]),\n",
        "                 xytext=(5, 2),\n",
        "                 textcoords='offset points',\n",
        "                 ha='right',\n",
        "                 va='bottom')\n",
        "plt.savefig(\"tsne-test.png\", dpi = 500)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "410uB0XI7H6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}