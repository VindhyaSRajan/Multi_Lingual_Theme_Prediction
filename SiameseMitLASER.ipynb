{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SiameseMitLASER",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "p9R6hGfqVJh_",
        "j3FwhbOyhwHA"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinmay5/NLP-Praktikum/blob/master/SiameseMitLASER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sScywYm8EJhe",
        "colab_type": "code",
        "outputId": "90429e47-7bae-41dc-f3f2-df0ed4dc93cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqSjJNruoa8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUcKCcufznUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')\n",
        "# %cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bENmrfgZLusD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install torch==1.0.1 -f https://download.pytorch.org/whl/cu100/stable # CUDA 10.0 build"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFppkR1CAEg4",
        "colab_type": "code",
        "outputId": "a5db8c1e-b1ab-4019-da44-ee3a1aa510db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip3 install torchvision\n",
        "!pip install nltk\n",
        "!pip install tqdm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.1.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtDA2b2VYMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#coding=utf-8\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "stops1 = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "def clean_sent(sent):\n",
        "    sent = sent.lower()\n",
        "    sent = re.sub(u'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]',' ',sent)\n",
        "    sent = re.sub('¡',' ',sent)\n",
        "    sent = re.sub('¿',' ',sent)\n",
        "    sent = re.sub('Á','á',sent)\n",
        "    sent = re.sub('Ó','ó',sent)\n",
        "    sent = re.sub('Ú','ú',sent)\n",
        "    sent = re.sub('É','é',sent)\n",
        "    sent = re.sub('Í','í',sent)\n",
        "    return sent\n",
        "  \n",
        "def cleanSpanish(df):\n",
        "    df['spanish1'] = df.spanish1.map(lambda x: ' '.join([ word for word in\n",
        "                                                         nltk.word_tokenize(clean_sent(x))]))\n",
        "    df['spanish2'] = df.spanish2.map(lambda x: ' '.join([ word for word in\n",
        "                                                         nltk.word_tokenize(clean_sent(x))]))\n",
        "    \n",
        "def removeSpanishStopWords(df, stop):\n",
        "\tdf['spanish1'] = df.spanish1.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x)\n",
        "                                                         if word not in stop]))\n",
        "\tdf['spanish2'] = df.spanish2.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x)\n",
        "                                                         if word not in stop]))\n",
        "\n",
        "\n",
        "def data_preprocessing():\n",
        "\n",
        "    # Training data\n",
        "    import os\n",
        "    os.chdir(\"/content/\")\n",
        "    !ls\n",
        "\n",
        "    df_train_en_sp = pd.read_csv('./cikm_english_train_20180516.txt', sep='\t', header=None,\n",
        "                                 error_bad_lines=False)\n",
        "    df_train_sp_en = pd.read_csv('./cikm_spanish_train_20180516.txt', sep='\t', header=None,\n",
        "                                 error_bad_lines=False)\n",
        "    df_train_en_sp.columns = ['english1', 'spanish1', 'english2', 'spanish2', 'result']\n",
        "    df_train_sp_en.columns = ['spanish1', 'english1', 'spanish2', 'english2', 'result']\n",
        "    train1 = pd.DataFrame(pd.concat([df_train_en_sp['spanish1'], df_train_sp_en['spanish1']], axis=0))\n",
        "    train2 = pd.DataFrame(pd.concat([df_train_en_sp['spanish2'], df_train_sp_en['spanish2']], axis=0))\n",
        "    train_data = pd.concat([train1, train2], axis=1).reset_index()\n",
        "    train_data = train_data.drop(['index'], axis=1)\n",
        "    result = pd.DataFrame(pd.concat([df_train_en_sp['result'], df_train_sp_en['result']], axis=0)).reset_index()\n",
        "    result = result.drop(['index'], axis=1)\n",
        "    # pd.get_dummies(result['result']).head()\n",
        "    train_data['result'] = result\n",
        "\n",
        "    # Evaluation data\n",
        "    test_data = pd.read_csv('./cikm_test_a_20180516.txt', sep='\t', header=None, error_bad_lines=False)\n",
        "    test_data.columns = ['spanish1', 'spanish2']\n",
        "\n",
        "\n",
        "    cleanSpanish(train_data)\n",
        "    removeSpanishStopWords(train_data, stops1)\n",
        "    cleanSpanish(test_data)\n",
        "    removeSpanishStopWords(test_data, stops1)\n",
        "\n",
        "    train_data.replace('', np.nan, inplace=True)\n",
        "    dirty_data = train_data[train_data.isnull().any(axis=1)]\n",
        "    print ('dirty sample count:', dirty_data.shape[0])\n",
        "    print ('positive dirty training sample:', len(dirty_data[dirty_data['result'] == 1]))\n",
        "    print ('negative dirty training sample:', len(dirty_data[dirty_data['result'] == 0]))\n",
        "\n",
        "    train_data = train_data.dropna()\n",
        "    test_data.replace('', np.nan, inplace=True)\n",
        "    test_data = test_data.dropna()\n",
        "    print ('Train sample count:', train_data.shape[0], 'Test sample count:', test_data.shape[0])\n",
        "\n",
        "    train_data.columns = ['s1', 's2', 'label']\n",
        "    test_data.columns = ['s1', 's2']\n",
        "\n",
        "    train_data.to_csv(\"cleaned_train.csv\", index=False)\n",
        "    test_data.to_csv(\"cleaned_test.csv\", index=False)\n",
        "\n",
        "def get_embedding(word_dict, embedding_path, embedding_dim=300):\n",
        "    # find existing word embeddings\n",
        "    word_vec = {}\n",
        "    with open(embedding_path) as f:\n",
        "        for line in f:\n",
        "            word, vec = line.split(' ', 1)\n",
        "            if word in word_dict:\n",
        "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
        "    print('Found {0}/{1} words with embedding vectors'.format(\n",
        "        len(word_vec), len(word_dict)))\n",
        "    missing_word_num = len(word_dict) - len(word_vec)\n",
        "    missing_ratio = round(float(missing_word_num) / len(word_dict), 4) * 100\n",
        "    print('Missing Ratio: {}%'.format(missing_ratio))\n",
        "\n",
        "    # handling unknown embeddings\n",
        "    for word in word_dict:\n",
        "        if word not in word_vec:\n",
        "            # If word not in word_vec, create a random embedding for it\n",
        "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "            word_vec[word] = new_embedding\n",
        "    print (\"Filled missing words' embeddings.\")\n",
        "    print (\"Embedding Matrix Size: \", len(word_vec))\n",
        "\n",
        "    return word_vec\n",
        "\n",
        "def save_embed(obj, path):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "    print ('Embedding saved')\n",
        "\n",
        "def load_embed(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkVYLPcL3iTA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWYKXO5gJPf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Some of the variables that are going to be treated as constants and used throughout the code\n",
        "\"\"\"\n",
        "\n",
        "data_preprocess = True # Would preprocess the data and generate the embeddings\n",
        "make_dict = True #  Would generate the embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZEEUAKwX2Uh",
        "colab_type": "code",
        "outputId": "6cdca1fd-0182-444a-f377-77c661c05eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "\"\"\" Data Preprocessing \"\"\"\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "os.chdir(\"/content/\")\n",
        "!ls\n",
        "\n",
        "with open('siamese-config.yaml') as f:\n",
        "  config = yaml.load(f)\n",
        "\n",
        "  \n",
        "if data_preprocess:\n",
        "    print ('Pre-processing Original Data ...')\n",
        "    data_preprocessing()\n",
        "    print ('Data Pre-processing Done!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cikm_english_train_20180516.txt  cleaned_train.csv    wiki.en.vec\n",
            "cikm_spanish_train_20180516.txt  embedding.pkl\t      wiki.es.vec\n",
            "cikm_test_a_20180516.txt\t sample_data\n",
            "cleaned_test.csv\t\t siamese-config.yaml\n",
            "Pre-processing Original Data ...\n",
            "cikm_english_train_20180516.txt  cleaned_train.csv    wiki.en.vec\n",
            "cikm_spanish_train_20180516.txt  embedding.pkl\t      wiki.es.vec\n",
            "cikm_test_a_20180516.txt\t sample_data\n",
            "cleaned_test.csv\t\t siamese-config.yaml\n",
            "dirty sample count: 73\n",
            "positive dirty training sample: 5\n",
            "negative dirty training sample: 68\n",
            "Train sample count: 21327 Test sample count: 4998\n",
            "Data Pre-processing Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C05Cgh29Lx-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.embed_size = config['model']['embed_size']\n",
        "        self.batch_size = config['model']['batch_size']\n",
        "        self.hidden_size = config['model']['encoder']['hidden_size']\n",
        "        self.num_layers = config['model']['encoder']['num_layers']\n",
        "        self.bidir = config['model']['encoder']['bidirectional']\n",
        "        if self.bidir:\n",
        "            self.direction = 2\n",
        "        else: self.direction = 1\n",
        "        self.dropout = config['model']['encoder']['dropout']\n",
        "\n",
        "        self.embedding = config['embedding_matrix']\n",
        "        self.lstm = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size, dropout=self.dropout,\n",
        "                            num_layers=self.num_layers, bidirectional=self.bidir)\n",
        "        self.lstm = self.lstm.cuda()\n",
        "\n",
        "    def initHiddenCell(self):\n",
        "        rand_hidden = Variable(torch.randn(self.direction * self.num_layers, self.batch_size, self.hidden_size))\n",
        "        rand_cell = Variable(torch.randn(self.direction * self.num_layers, self.batch_size, self.hidden_size))\n",
        "        rand_hidden = rand_hidden.cuda()\n",
        "        rand_cell = rand_cell.cuda()\n",
        "        return rand_hidden, rand_cell\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = self.embedding(input).view(1, 1, -1)\n",
        "        output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "        return output, hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egyATa-gLyBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Siamese_lstm(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Siamese_lstm, self).__init__()\n",
        "\n",
        "        self.encoder = LSTMEncoder(config)\n",
        "        self.fc_dim = config['model']['fc_dim']\n",
        "\n",
        "        self.input_dim = 5 * self.encoder.direction * self.encoder.hidden_size\n",
        "        # self.classifier = nn.Sequential(\n",
        "        #     nn.Linear(self.input_dim, self.fc_dim),\n",
        "        #     nn.Linear(self.fc_dim, 2)\n",
        "        # )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, int(self.input_dim/2)),\n",
        "            nn.Linear(int(self.input_dim/2), 2)\n",
        "        ).cuda()\n",
        "\n",
        "    def forward(self, s1, s2):\n",
        "\n",
        "        # init hidden, cell\n",
        "        h1, c1 = self.encoder.initHiddenCell()\n",
        "        h2, c2 = self.encoder.initHiddenCell()\n",
        "\n",
        "        # input one by one\n",
        "\n",
        "        for i in range(len(s1)):\n",
        "\n",
        "            v1, h1, c1 = self.encoder(s1[i], h1, c1)\n",
        "            \n",
        "        for j in range(len(s2)):\n",
        "            v2, h2, c2 = self.encoder(s2[j], h2, c2)\n",
        "            \n",
        "        # utilize these two encoded vectors\n",
        "        features = torch.cat((v1,torch.abs(v1 - v2),v2,v1*v2, (v1+v2)/2), 2)\n",
        "        # features = v1-v2\n",
        "        output = self.classifier(features)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6detYnDOBU9",
        "colab_type": "code",
        "outputId": "9f3f2137-105d-4b32-f8c6-3592111857c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "!cat siamese-config.yaml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "experiment_name: 'siamese-baseline'\n",
            "\n",
            "task: 'train'\n",
            "make_dict: False\n",
            "data_preprocessing: False\n",
            "\n",
            "ckpt_dir: 'ckpt/'\n",
            "\n",
            "training:\n",
            "    num_epochs: 20\n",
            "    learning_rate: 0.01\n",
            "    # options = ['adam', 'adadelta', 'rmsprop']\n",
            "    optimizer: 'sgd'\n",
            "\n",
            "\n",
            "embedding:\n",
            "    full_embedding_path: 'input/wiki.es.vec'\n",
            "    cur_embedding_path: 'input/embedding.pkl'\n",
            "\n",
            "model:\n",
            "    fc_dim: 100\n",
            "    name: 'siamese'\n",
            "    embed_size: 300\n",
            "    batch_size: 1\n",
            "    embedding_freeze: False\n",
            "    encoder:\n",
            "        hidden_size: 150\n",
            "        num_layers: 1\n",
            "        bidirectional: False\n",
            "        dropout: 0.0\n",
            "\n",
            "result:\n",
            "    filename: 'result.txt'\n",
            "    filepath: 'res/'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfZBaI7FO4lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class myDS(Dataset):\n",
        "\n",
        "    def __init__(self, df, all_sents):\n",
        "        # Assign vocabularies.\n",
        "        self.s1 = df['s1'].tolist()\n",
        "        self.s2 = df['s2'].tolist()\n",
        "        self.label = df['label'].tolist()\n",
        "        self.vocab = Vocab(all_sents, sos_token='<sos>', eos_token='<eos>', unk_token='<unk>')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Split sentence into words.\n",
        "        s1_words = self.s1[idx].split()\n",
        "        s2_words = self.s2[idx].split()\n",
        "\n",
        "        # Add <SOS> and <EOS> tokens.\n",
        "        s1_words = [self.vocab.sos_token] + s1_words + [self.vocab.eos_token]\n",
        "        s2_words = [self.vocab.sos_token] + s2_words + [self.vocab.eos_token]\n",
        "\n",
        "        # Lookup word ids in vocabularies.\n",
        "        s1_ids = [self.vocab.word2id(word) for word in s1_words]\n",
        "        s2_ids = [self.vocab.word2id(word) for word in s2_words]\n",
        "        label = self.label[idx]\n",
        "\n",
        "        return s1_ids, s2_ids, label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_dkKjxNO4q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class mytestDS(Dataset):\n",
        "\n",
        "    def __init__(self, df, all_sents):\n",
        "        # Assign vocabularies.\n",
        "        self.s1 = df['s1'].tolist()\n",
        "        self.s2 = df['s2'].tolist()\n",
        "        self.vocab = Vocab(all_sents, sos_token='<sos>', eos_token='<eos>', unk_token='<unk>')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.s1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Split sentence into words.\n",
        "        s1_words = self.s1[idx].split()\n",
        "        s2_words = self.s2[idx].split()\n",
        "\n",
        "        # Add <SOS> and <EOS> tokens.\n",
        "        s1_words = [self.vocab.sos_token] + s1_words + [self.vocab.eos_token]\n",
        "        s2_words = [self.vocab.sos_token] + s2_words + [self.vocab.eos_token]\n",
        "\n",
        "        # Lookup word ids in vocabularies.\n",
        "        s1_ids = [self.vocab.word2id(word) for word in s1_words]\n",
        "        s2_ids = [self.vocab.word2id(word) for word in s2_words]\n",
        "        \n",
        "        return s1_ids, s2_ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBD2uMOtO4od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab(object):\n",
        "    def __init__(self, all_sents, max_size=None, sos_token=None, eos_token=None, unk_token=None):\n",
        "        \"\"\"Initialize the vocabulary.\n",
        "        Args:\n",
        "            iter: An iterable which produces sequences of tokens used to update\n",
        "                the vocabulary.\n",
        "            max_size: (Optional) Maximum number of tokens in the vocabulary.\n",
        "            sos_token: (Optional) Token denoting the start of a sequence.\n",
        "            eos_token: (Optional) Token denoting the end of a sequence.\n",
        "            unk_token: (Optional) Token denoting an unknown element in a\n",
        "                sequence.\n",
        "        \"\"\"\n",
        "        self.max_size = max_size\n",
        "        self.pad_token = '<pad>'\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.unk_token = unk_token\n",
        "\n",
        "        # Add special tokens.\n",
        "        id2word = [self.pad_token]\n",
        "        if sos_token is not None:\n",
        "            id2word.append(self.sos_token)\n",
        "        if eos_token is not None:\n",
        "            id2word.append(self.eos_token)\n",
        "        if unk_token is not None:\n",
        "            id2word.append(self.unk_token)\n",
        "\n",
        "        # Update counter with token counts.\n",
        "        counter = Counter()\n",
        "        for x in all_sents:\n",
        "            counter.update(x.split())\n",
        "\n",
        "        # Extract lookup tables.\n",
        "        if max_size is not None:\n",
        "            counts = counter.most_common(max_size)\n",
        "        else:\n",
        "            counts = counter.items()\n",
        "            counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
        "        words = [x[0] for x in counts]\n",
        "        id2word.extend(words)\n",
        "        word2id = {x: i for i, x in enumerate(id2word)}\n",
        "\n",
        "        self._id2word = id2word\n",
        "        self._word2id = word2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._id2word)\n",
        "\n",
        "    def word2id(self, word):\n",
        "        \"\"\"Map a word in the vocabulary to its unique integer id.\n",
        "        Args:\n",
        "            word: Word to lookup.\n",
        "        Returns:\n",
        "            id: The integer id of the word being looked up.\n",
        "        \"\"\"\n",
        "        if word in self._word2id:\n",
        "            return self._word2id[word]\n",
        "        elif self.unk_token is not None:\n",
        "            return self._word2id[self.unk_token]\n",
        "        else:\n",
        "            raise KeyError('Word \"%s\" not in vocabulary.' % word)\n",
        "\n",
        "    def id2word(self, id):\n",
        "        \"\"\"Map an integer id to its corresponding word in the vocabulary.\n",
        "        Args:\n",
        "            id: Integer id of the word being looked up.\n",
        "        Returns:\n",
        "            word: The corresponding word.\n",
        "        \"\"\"\n",
        "        return self._id2word[id]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgowMsbBJPWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wiv2kjMRENYH",
        "colab_type": "code",
        "outputId": "4ea4a437-cb67-4afe-a131-5bcaec79b461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q09HF_6GX2b_",
        "colab_type": "code",
        "outputId": "eede4ac8-bce4-4c02-8b54-23d80dfc1c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\"\"\" Read Data \"\"\"\n",
        "\n",
        "train_data = pd.read_csv('cleaned_train.csv')\n",
        "test_data = pd.read_csv('cleaned_test.csv')\n",
        "\n",
        "# split dataset\n",
        "msk = np.random.rand(len(train_data)) < 0.8\n",
        "train = train_data[msk]\n",
        "valid = train_data[~msk]\n",
        "all_sents = train_data['s1'].tolist() + train_data['s2'].tolist() + test_data['s1'].tolist() + test_data['s2'].tolist()\n",
        "\n",
        "# dataset\n",
        "trainDS = myDS(train, all_sents)\n",
        "validDS = myDS(valid, all_sents)\n",
        "\n",
        "print ('Data size:',train_data.shape[0], test_data.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size: 21327 4998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8NPOcxrGdBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Get Embedding \"\"\"\n",
        "\n",
        "#full_embed_path = config['embedding']['full_embedding_path']\n",
        "#cur_embed_path = config['embedding']['cur_embedding_path']\n",
        "\n",
        "# TODO: Make sure we read it from the config file in the future\n",
        "full_embed_path = \"wiki.es.vec\"\n",
        "cur_embed_path = \"embedding.pkl\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxSsU6PmVVWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this line if the embeddings need to be loaded\n",
        "\n",
        "\n",
        "#!curl https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec > ./wiki.en.vec\n",
        "#!curl https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.es.vec > ./wiki.es.vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1kwrEk8YBqR",
        "colab_type": "code",
        "outputId": "ab414571-b0ad-4864-a3cd-867d5d843062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists(cur_embed_path) and not make_dict:\n",
        "    embed_dict = load_embed(cur_embed_path)\n",
        "    print ('Loaded existing embedding.')\n",
        "else:\n",
        "    print ('Making embedding...')\n",
        "    embed_dict = get_embedding(trainDS.vocab._id2word, full_embed_path)\n",
        "    save_embed(embed_dict,\"embedding.pkl\")\n",
        "    print ('Saved generated embedding.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Making embedding...\n",
            "Found 5141/5773 words with embedding vectors\n",
            "Missing Ratio: 10.95%\n",
            "Filled missing words' embeddings.\n",
            "Embedding Matrix Size:  5773\n",
            "Embedding saved\n",
            "Saved generated embedding.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0hY462lGO_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(embed_dict)\n",
        "# initialize nn embedding\n",
        "embedding = nn.Embedding(vocab_size, config['model']['embed_size'])\n",
        "embed_list = []\n",
        "for word in trainDS.vocab._id2word:\n",
        "    embed_list.append(embed_dict[word])\n",
        "weight_matrix = np.array(embed_list)\n",
        "# pass weights to nn embedding\n",
        "embedding.weight = nn.Parameter(torch.from_numpy(weight_matrix).type(torch.FloatTensor), requires_grad = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knAMkjneRumZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding\n",
        "config['embedding_matrix'] = embedding\n",
        "config['vocab_size'] = len(embed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoaBdfULRur3",
        "colab_type": "code",
        "outputId": "73ecdd4c-7b65-4e04-e5bf-11e3a1441889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# model\n",
        "siamese = Siamese_lstm(config)\n",
        "siamese.cuda()\n",
        "siamese.encoder.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMEncoder(\n",
              "  (embedding): Embedding(5773, 300)\n",
              "  (lstm): LSTM(300, 150)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQHsY8anYMs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(embed_dict)\n",
        "# initialize nn embedding\n",
        "embedding = nn.Embedding(vocab_size, config['model']['embed_size'])\n",
        "embed_list = []\n",
        "for word in trainDS.vocab._id2word:\n",
        "    embed_list.append(embed_dict[word])\n",
        "weight_matrix = np.array(embed_list)\n",
        "# pass weights to nn embedding\n",
        "embedding.weight = nn.Parameter(torch.from_numpy(weight_matrix).type(torch.FloatTensor), requires_grad = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SphmC-tGZeLP",
        "colab_type": "text"
      },
      "source": [
        "Code for preprocessing the text and other cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRmMu1-GYB1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTXhvxZHXRIL",
        "colab_type": "text"
      },
      "source": [
        "Setting up of the Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y_obpC3XPyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer\n",
        "learning_rate = config['training']['learning_rate']\n",
        "if config['training']['optimizer'] == 'sgd':\n",
        "    optimizer = torch.optim.SGD(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)\n",
        "elif config['training']['optimizer'] == 'adam':\n",
        "    optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)\n",
        "elif config['training']['optimizer'] == 'adadelta':\n",
        "    optimizer = torch.optim.Adadelta(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)\n",
        "elif config['training']['optimizer'] == 'rmsprop':\n",
        "    optimizer = torch.optim.RMSprop(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plIkyXb4XP4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss func\n",
        "loss_weights = Variable(torch.FloatTensor([1, 3]))\n",
        "if torch.cuda.is_available():\n",
        "    loss_weights = loss_weights.cuda()\n",
        "criterion = torch.nn.CrossEntropyLoss(loss_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFOSUSNwXPtH",
        "colab_type": "code",
        "outputId": "f1ac3007-4793-4f52-cb63-671c7889b035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Restore saved model (if one exists).\n",
        "ckpt_path = os.path.join('./', config['experiment_name']+'.pt')\n",
        "\n",
        "if os.path.exists(ckpt_path):\n",
        "    print('Loading checkpoint: %s' % ckpt_path)\n",
        "    ckpt = torch.load(ckpt_path)\n",
        "    epoch = ckpt['epoch']\n",
        "    siamese.load_state_dict(ckpt['siamese'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "else:\n",
        "    epoch = 0\n",
        "    print ('Fresh start!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fresh start!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqeYI35WFi74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# log info\n",
        "train_log_string = '%s :: Epoch %i :: Iter %i / %i :: train loss: %0.4f'\n",
        "valid_log_string = '%s :: Epoch %i :: valid loss: %0.4f\\n'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9R6hGfqVJh_",
        "colab_type": "text"
      },
      "source": [
        "## Training Procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Ua1mQOVI0t",
        "colab_type": "code",
        "outputId": "9e5a4d91-25a5-4a4b-cd6d-b19e98ef8ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2465
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "# save every epoch for visualization\n",
        "train_loss_record = []\n",
        "valid_loss_record = []\n",
        "best_record = 10.0\n",
        "\n",
        "# training\n",
        "print ('Experiment:{}\\n'.format(config['experiment_name']))\n",
        "\n",
        "    \n",
        "while (epoch < config['training']['num_epochs']):\n",
        "\n",
        "    print ('Start Epoch{} Training...'.format(epoch))\n",
        "\n",
        "    # loss\n",
        "    train_loss = []\n",
        "    train_loss_sum = []\n",
        "    # dataloader\n",
        "    train_dataloader = DataLoader(dataset=trainDS, shuffle=True, num_workers=2, batch_size=1)\n",
        "\n",
        "    for idx, data in enumerate(train_dataloader, 0):\n",
        "\n",
        "        # get data\n",
        "        s1, s2, label = data\n",
        "        \n",
        "        # putting the data into cuda\n",
        "        s1 = torch.from_numpy(np.array(s1))\n",
        "        s2 = torch.from_numpy(np.array(s2))\n",
        "        label = torch.from_numpy(np.array(label))\n",
        "        \n",
        "        s1 = s1.cuda()\n",
        "        s2 = s2.cuda()\n",
        "        label = label.cuda()\n",
        "        \n",
        "        # clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        \n",
        "        # input\n",
        "        output = siamese(s1, s2)\n",
        "        output = output.squeeze(0)\n",
        "\n",
        "        # loss backward\n",
        "        loss = criterion(output, Variable(label))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss.data.cpu())\n",
        "        train_loss_sum.append(loss.data.cpu())\n",
        "\n",
        "        # Every once and a while check on the loss\n",
        "        if ((idx + 1) % 5000) == 0:\n",
        "            print(train_log_string % (datetime.now(), epoch, idx + 1, len(train), np.mean(train_loss)))\n",
        "            train_loss = []\n",
        "\n",
        "    # Record at every epoch\n",
        "    print ('Train Loss at epoch{}: {}\\n'.format(epoch, np.mean(train_loss_sum)))\n",
        "    train_loss_record.append(np.mean(train_loss_sum))\n",
        "\n",
        "    # Valid\n",
        "    print ('Epoch{} Validating...'.format(epoch))\n",
        "\n",
        "    # loss\n",
        "    valid_loss = []\n",
        "    # dataloader\n",
        "    valid_dataloader = DataLoader(dataset=validDS, shuffle=True, num_workers=2, batch_size=1)\n",
        "\n",
        "    for idx, data in enumerate(valid_dataloader, 0):\n",
        "        # get data\n",
        "        s1, s2, label = data\n",
        "\n",
        "        # putting the data into cuda\n",
        "        s1 = torch.from_numpy(np.array(s1))\n",
        "        s2 = torch.from_numpy(np.array(s2))\n",
        "        label = torch.from_numpy(np.array(label))\n",
        "        \n",
        "        s1 = s1.cuda()\n",
        "        s2 = s2.cuda()\n",
        "        label = label.cuda()\n",
        "        \n",
        "        # input\n",
        "        output = siamese(s1, s2)\n",
        "        output = output.squeeze(0)\n",
        "\n",
        "        # loss\n",
        "        loss = criterion(output, Variable(label))\n",
        "        valid_loss.append(loss.data.cpu())\n",
        "\n",
        "    print(valid_log_string % (datetime.now(), epoch, np.mean(valid_loss)))\n",
        "    # Record\n",
        "    valid_loss_record.append(np.mean(valid_loss))\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "    # Keep track of best record\n",
        "    if np.mean(valid_loss) < best_record:\n",
        "        best_record = np.mean(valid_loss)\n",
        "        # save the best model\n",
        "        state_dict = {\n",
        "            'epoch': epoch,\n",
        "            'siamese': siamese.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }\n",
        "        torch.save(state_dict, ckpt_path)\n",
        "        print ('Model saved!\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment:siamese-baseline\n",
            "\n",
            "Start Epoch3 Training...\n",
            "2019-05-21 22:27:26.995351 :: Epoch 3 :: Iter 5000 / 17013 :: train loss: 0.3915\n",
            "2019-05-21 22:28:47.501385 :: Epoch 3 :: Iter 10000 / 17013 :: train loss: 0.4048\n",
            "2019-05-21 22:30:11.273321 :: Epoch 3 :: Iter 15000 / 17013 :: train loss: 0.3719\n",
            "Train Loss at epoch3: 0.3892628252506256\n",
            "\n",
            "Epoch3 Validating...\n",
            "2019-05-21 22:31:30.875104 :: Epoch 3 :: valid loss: 0.3873\n",
            "\n",
            "Model saved!\n",
            "\n",
            "Start Epoch4 Training...\n",
            "2019-05-21 22:32:52.124570 :: Epoch 4 :: Iter 5000 / 17013 :: train loss: 0.3404\n",
            "2019-05-21 22:34:13.403940 :: Epoch 4 :: Iter 10000 / 17013 :: train loss: 0.3437\n",
            "2019-05-21 22:35:35.186432 :: Epoch 4 :: Iter 15000 / 17013 :: train loss: 0.3472\n",
            "Train Loss at epoch4: 0.34338438510894775\n",
            "\n",
            "Epoch4 Validating...\n",
            "2019-05-21 22:36:54.765014 :: Epoch 4 :: valid loss: 0.3658\n",
            "\n",
            "Model saved!\n",
            "\n",
            "Start Epoch5 Training...\n",
            "2019-05-21 22:38:15.622470 :: Epoch 5 :: Iter 5000 / 17013 :: train loss: 0.3026\n",
            "2019-05-21 22:39:37.149971 :: Epoch 5 :: Iter 10000 / 17013 :: train loss: 0.3044\n",
            "2019-05-21 22:40:59.543065 :: Epoch 5 :: Iter 15000 / 17013 :: train loss: 0.2995\n",
            "Train Loss at epoch5: 0.30207836627960205\n",
            "\n",
            "Epoch5 Validating...\n",
            "2019-05-21 22:42:19.070706 :: Epoch 5 :: valid loss: 0.3708\n",
            "\n",
            "Start Epoch6 Training...\n",
            "2019-05-21 22:43:39.453905 :: Epoch 6 :: Iter 5000 / 17013 :: train loss: 0.2656\n",
            "2019-05-21 22:45:00.235498 :: Epoch 6 :: Iter 10000 / 17013 :: train loss: 0.2559\n",
            "2019-05-21 22:46:21.893381 :: Epoch 6 :: Iter 15000 / 17013 :: train loss: 0.2617\n",
            "Train Loss at epoch6: 0.26249587535858154\n",
            "\n",
            "Epoch6 Validating...\n",
            "2019-05-21 22:47:40.839431 :: Epoch 6 :: valid loss: 0.3710\n",
            "\n",
            "Start Epoch7 Training...\n",
            "2019-05-21 22:49:01.295399 :: Epoch 7 :: Iter 5000 / 17013 :: train loss: 0.2071\n",
            "2019-05-21 22:50:21.693219 :: Epoch 7 :: Iter 10000 / 17013 :: train loss: 0.2336\n",
            "2019-05-21 22:51:42.306404 :: Epoch 7 :: Iter 15000 / 17013 :: train loss: 0.2409\n",
            "Train Loss at epoch7: 0.22568874061107635\n",
            "\n",
            "Epoch7 Validating...\n",
            "2019-05-21 22:53:00.329146 :: Epoch 7 :: valid loss: 0.4011\n",
            "\n",
            "Start Epoch8 Training...\n",
            "2019-05-21 22:54:19.119699 :: Epoch 8 :: Iter 5000 / 17013 :: train loss: 0.1892\n",
            "2019-05-21 22:55:38.455321 :: Epoch 8 :: Iter 10000 / 17013 :: train loss: 0.1910\n",
            "2019-05-21 22:56:58.760145 :: Epoch 8 :: Iter 15000 / 17013 :: train loss: 0.1998\n",
            "Train Loss at epoch8: 0.1960529237985611\n",
            "\n",
            "Epoch8 Validating...\n",
            "2019-05-21 22:58:15.878047 :: Epoch 8 :: valid loss: 0.3801\n",
            "\n",
            "Start Epoch9 Training...\n",
            "2019-05-21 22:59:34.371517 :: Epoch 9 :: Iter 5000 / 17013 :: train loss: 0.1583\n",
            "2019-05-21 23:00:54.094949 :: Epoch 9 :: Iter 10000 / 17013 :: train loss: 0.1601\n",
            "2019-05-21 23:02:12.508282 :: Epoch 9 :: Iter 15000 / 17013 :: train loss: 0.1763\n",
            "Train Loss at epoch9: 0.16643072664737701\n",
            "\n",
            "Epoch9 Validating...\n",
            "2019-05-21 23:03:28.998171 :: Epoch 9 :: valid loss: 0.4001\n",
            "\n",
            "Start Epoch10 Training...\n",
            "2019-05-21 23:04:48.428865 :: Epoch 10 :: Iter 5000 / 17013 :: train loss: 0.1125\n",
            "2019-05-21 23:06:06.187788 :: Epoch 10 :: Iter 10000 / 17013 :: train loss: 0.1490\n",
            "2019-05-21 23:07:24.878376 :: Epoch 10 :: Iter 15000 / 17013 :: train loss: 0.1442\n",
            "Train Loss at epoch10: 0.13759568333625793\n",
            "\n",
            "Epoch10 Validating...\n",
            "2019-05-21 23:08:41.578418 :: Epoch 10 :: valid loss: 0.4468\n",
            "\n",
            "Start Epoch11 Training...\n",
            "2019-05-21 23:09:59.687554 :: Epoch 11 :: Iter 5000 / 17013 :: train loss: 0.1071\n",
            "2019-05-21 23:11:16.624119 :: Epoch 11 :: Iter 10000 / 17013 :: train loss: 0.1124\n",
            "2019-05-21 23:12:33.356165 :: Epoch 11 :: Iter 15000 / 17013 :: train loss: 0.1321\n",
            "Train Loss at epoch11: 0.1182505190372467\n",
            "\n",
            "Epoch11 Validating...\n",
            "2019-05-21 23:13:49.492473 :: Epoch 11 :: valid loss: 0.4469\n",
            "\n",
            "Start Epoch12 Training...\n",
            "2019-05-21 23:15:08.352013 :: Epoch 12 :: Iter 5000 / 17013 :: train loss: 0.0857\n",
            "2019-05-21 23:16:26.771588 :: Epoch 12 :: Iter 10000 / 17013 :: train loss: 0.1056\n",
            "2019-05-21 23:17:44.872561 :: Epoch 12 :: Iter 15000 / 17013 :: train loss: 0.1006\n",
            "Train Loss at epoch12: 0.10035441815853119\n",
            "\n",
            "Epoch12 Validating...\n",
            "2019-05-21 23:19:01.312102 :: Epoch 12 :: valid loss: 0.5057\n",
            "\n",
            "Start Epoch13 Training...\n",
            "2019-05-21 23:20:19.715872 :: Epoch 13 :: Iter 5000 / 17013 :: train loss: 0.0704\n",
            "2019-05-21 23:21:38.338429 :: Epoch 13 :: Iter 10000 / 17013 :: train loss: 0.0894\n",
            "2019-05-21 23:22:56.419657 :: Epoch 13 :: Iter 15000 / 17013 :: train loss: 0.0763\n",
            "Train Loss at epoch13: 0.083287812769413\n",
            "\n",
            "Epoch13 Validating...\n",
            "2019-05-21 23:24:12.591097 :: Epoch 13 :: valid loss: 0.4954\n",
            "\n",
            "Start Epoch14 Training...\n",
            "2019-05-21 23:25:30.594853 :: Epoch 14 :: Iter 5000 / 17013 :: train loss: 0.0645\n",
            "2019-05-21 23:26:49.055352 :: Epoch 14 :: Iter 10000 / 17013 :: train loss: 0.0737\n",
            "2019-05-21 23:28:06.590857 :: Epoch 14 :: Iter 15000 / 17013 :: train loss: 0.0766\n",
            "Train Loss at epoch14: 0.0723523274064064\n",
            "\n",
            "Epoch14 Validating...\n",
            "2019-05-21 23:29:22.522570 :: Epoch 14 :: valid loss: 0.5376\n",
            "\n",
            "Start Epoch15 Training...\n",
            "2019-05-21 23:30:40.224660 :: Epoch 15 :: Iter 5000 / 17013 :: train loss: 0.0572\n",
            "2019-05-21 23:31:58.923758 :: Epoch 15 :: Iter 10000 / 17013 :: train loss: 0.0604\n",
            "2019-05-21 23:33:17.094919 :: Epoch 15 :: Iter 15000 / 17013 :: train loss: 0.0752\n",
            "Train Loss at epoch15: 0.06663064658641815\n",
            "\n",
            "Epoch15 Validating...\n",
            "2019-05-21 23:34:33.251234 :: Epoch 15 :: valid loss: 0.5427\n",
            "\n",
            "Start Epoch16 Training...\n",
            "2019-05-21 23:35:51.166357 :: Epoch 16 :: Iter 5000 / 17013 :: train loss: 0.0520\n",
            "2019-05-21 23:37:09.817456 :: Epoch 16 :: Iter 10000 / 17013 :: train loss: 0.0598\n",
            "2019-05-21 23:38:27.546974 :: Epoch 16 :: Iter 15000 / 17013 :: train loss: 0.0596\n",
            "Train Loss at epoch16: 0.05654129013419151\n",
            "\n",
            "Epoch16 Validating...\n",
            "2019-05-21 23:39:43.913475 :: Epoch 16 :: valid loss: 0.5851\n",
            "\n",
            "Start Epoch17 Training...\n",
            "2019-05-21 23:41:02.353307 :: Epoch 17 :: Iter 5000 / 17013 :: train loss: 0.0418\n",
            "2019-05-21 23:42:21.366902 :: Epoch 17 :: Iter 10000 / 17013 :: train loss: 0.0466\n",
            "2019-05-21 23:43:38.549071 :: Epoch 17 :: Iter 15000 / 17013 :: train loss: 0.0590\n",
            "Train Loss at epoch17: 0.048996467143297195\n",
            "\n",
            "Epoch17 Validating...\n",
            "2019-05-21 23:44:54.937262 :: Epoch 17 :: valid loss: 0.6124\n",
            "\n",
            "Start Epoch18 Training...\n",
            "2019-05-21 23:46:13.781334 :: Epoch 18 :: Iter 5000 / 17013 :: train loss: 0.0302\n",
            "2019-05-21 23:47:30.893147 :: Epoch 18 :: Iter 10000 / 17013 :: train loss: 0.0475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jACju1PFOBXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1s2hwjaWi_g",
        "colab_type": "text"
      },
      "source": [
        "Include Torch vision in here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0-2kOgMOBdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "\n",
        "plt.plot(train_loss_record)\n",
        "plt.plot(valid_loss_record)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltprn13EOBaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\" Inference \"\"\"\n",
        "# if config['taks'] == 'inference':\n",
        "testDS = mytestDS(test_data, all_sents)\n",
        "# Do not shuffle here\n",
        "test_dataloader = DataLoader(dataset=testDS, num_workers=2, batch_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXq8_NtIWcqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = []\n",
        "for idx, data in enumerate(test_dataloader, 0):\n",
        "\n",
        "    # get data\n",
        "    s1, s2 = data\n",
        "\n",
        "    # input\n",
        "    output = siamese(s1,s2)\n",
        "    output = output.squeeze(0)\n",
        "\n",
        "    # feed output into softmax to get prob prediction\n",
        "    sm = nn.Softmax(dim=1)\n",
        "    res = sm(output.data)[:,1]\n",
        "    result += res.data.tolist()\n",
        "\n",
        "result = pd.DataFrame(result)\n",
        "print 'Inference Done.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcfTeWUVpiK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6BpPi7aWctf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res_path = os.path.join(config['result']['filepath'], config['result']['filename'])\n",
        "result.to_csv(res_path,header=False,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3FwhbOyhwHA",
        "colab_type": "text"
      },
      "source": [
        "##Extra Section that will come in handy later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gphZG0tjZLhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "   \n",
        "    def __init__(self, margin):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.eps = 1e-9\n",
        "\n",
        "    def forward(self, output1, output2, target, size_average=True):\n",
        "        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n",
        "        losses = 0.5 * (target.float() * distances +\n",
        "                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2)) #ReLU function does the same task as selecting max\n",
        "        if size_average:\n",
        "          return losses.mean() \n",
        "        return losses.sum()\n",
        "\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplet loss\n",
        "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative, size_average=True):\n",
        "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
        "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
        "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
        "        if size_average:\n",
        "          return losses.mean()\n",
        "        return losses.sum()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApJFvPn_h8P6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OnlineContrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The pair selector and embeddings are sent\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin, pair_selector):\n",
        "        super(OnlineContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.pair_selector = pair_selector\n",
        "\n",
        "    def forward(self, embeddings, target):\n",
        "        positive_pairs, negative_pairs = self.pair_selector.get_pairs(embeddings, target)\n",
        "        if embeddings.is_cuda:\n",
        "            positive_pairs = positive_pairs.cuda()\n",
        "            negative_pairs = negative_pairs.cuda()\n",
        "        positive_loss = (embeddings[positive_pairs[:, 0]] - embeddings[positive_pairs[:, 1]]).pow(2).sum(1)\n",
        "        negative_loss = F.relu(\n",
        "            self.margin - (embeddings[negative_pairs[:, 0]] - embeddings[negative_pairs[:, 1]]).pow(2).sum(\n",
        "                1).sqrt()).pow(2)\n",
        "        loss = torch.cat([positive_loss, negative_loss], dim=0) #dim 0 is the rows\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class OnlineTripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplet selector and the embeddings are sent\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin, triplet_selector):\n",
        "        super(OnlineTripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.triplet_selector = triplet_selector\n",
        "\n",
        "    def forward(self, embeddings, target):\n",
        "\n",
        "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
        "\n",
        "        if embeddings.is_cuda:\n",
        "            triplets = triplets.cuda()\n",
        "\n",
        "        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n",
        "        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n",
        "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
        "\n",
        "        return losses.mean(), len(triplets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3AupsAslLAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TripletDataSet(Dataset):\n",
        "    \"\"\"\n",
        "    Train: For each sample (anchor) randomly chooses a positive and negative samples\n",
        "    Test: Creates fixed triplets for testing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_df):\n",
        "        self.train_df = train_df\n",
        "        self.arg1 = train_df['s1']\n",
        "        self.arg2 = train_df['s2']\n",
        "\n",
        "        \n",
        "        random_state = np.random.RandomState(29)\n",
        "\n",
        "        triplets = [[i,\n",
        "                     random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
        "                     random_state.choice(self.label_to_indices[\n",
        "                                             np.random.choice(\n",
        "                                                 list(self.labels_set - set([self.test_labels[i].item()]))\n",
        "                                             )\n",
        "                                         ])\n",
        "                     ]\n",
        "                    for i in range(len(self.train_df))]\n",
        "        self.test_triplets = triplets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Split sentence into words.\n",
        "        s1_words = self.arg1[idx].split()\n",
        "        s2_words = self.arg2[idx].split()\n",
        "\n",
        "        # Add <SOS> and <EOS> tokens.\n",
        "        s1_words = [self.vocab.sos_token] + s1_words + [self.vocab.eos_token]\n",
        "        s2_words = [self.vocab.sos_token] + s2_words + [self.vocab.eos_token]\n",
        "\n",
        "        # Lookup word ids in vocabularies.\n",
        "        s1_ids = [self.vocab.word2id(word) for word in s1_words]\n",
        "        s2_ids = [self.vocab.word2id(word) for word in s2_words]\n",
        "        label = self.label[idx]\n",
        "\n",
        "        return s1_ids, s2_ids, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wUDuuzC3knQ",
        "colab_type": "text"
      },
      "source": [
        "# This is an attempt to include the LASER multi lingual embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mngRKDEg3x2h",
        "colab_type": "code",
        "outputId": "3f048f0c-126f-47ce-a79c-613cd14f3e09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/ceshine/LASER.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'LASER'...\n",
            "remote: Enumerating objects: 504, done.\u001b[K\n",
            "remote: Total 504 (delta 0), reused 0 (delta 0), pack-reused 504\n",
            "Receiving objects: 100% (504/504), 2.56 MiB | 4.22 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_8LAr8c6Wyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf \\\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Akm1QeP4lea",
        "colab_type": "code",
        "outputId": "820d226f-87bd-4d11-e5de-1768915bf3ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%env LASER=/content/LASER\n",
        "!echo $LASER\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: LASER=/content/LASER\n",
            "/content/LASER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo67QGwc7m7F",
        "colab_type": "code",
        "outputId": "742157ec-ba9a-49c3-8c86-151ede0bccd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KxeDCC74lhZ",
        "colab_type": "code",
        "outputId": "25ff48b5-20c3-4720-97b5-c5bcf4d73cf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd LASER"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LASER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjFh5jFf4lmg",
        "colab_type": "code",
        "outputId": "5918ca05-f4f5-4c28-b8cd-7c8be2fc4721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LASER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7ADRwFZ4lp8",
        "colab_type": "code",
        "outputId": "f82595f9-aca3-4da3-c1e8-dff1fc682099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!bash install_models.sh"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading networks\n",
            " - creating directory /content/LASER/models\n",
            " - bilstm.eparl21.2018-11-19.pt\n",
            " - eparl21.fcodes\n",
            " - eparl21.fvocab\n",
            " - bilstm.93langs.2018-12-26.pt\n",
            " - 93langs.fcodes\n",
            " - 93langs.fvocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5F_sNpe4lkm",
        "colab_type": "code",
        "outputId": "68b41446-edc3-4fa3-8efa-c696b3375b90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "source": [
        "!bash install_external_tools.sh"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing external tools\n",
            " - creating directory /content/LASER/tools-external/moses-tokenizer/tokenizer\n",
            " - download tokenizer/tokenizer.perl\n",
            " - download tokenizer/detokenizer.perl\n",
            " - download tokenizer/normalize-punctuation.perl\n",
            " - download tokenizer/remove-non-printing-char.perl\n",
            " - download tokenizer/deescape-special-chars.perl\n",
            " - download tokenizer/lowercase.perl\n",
            " - download tokenizer/basic-protected-patterns\n",
            " - creating directory /content/LASER/tools-external/moses-tokenizer/share/nonbreaking_prefixes\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ca\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.cs\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.de\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.el\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.en\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.es\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.fi\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.fr\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ga\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.hu\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.is\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.it\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.lt\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.lv\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.nl\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.pl\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.pt\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ro\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ru\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.sk\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.sl\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.sv\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ta\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.yue\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.zh\n",
            " - download fastBPE software from github\n",
            "--2019-05-30 07:03:05--  https://github.com/glample/fastBPE/archive/master.zip\n",
            "Resolving github.com (github.com)... 140.82.118.4\n",
            "Connecting to github.com (github.com)|140.82.118.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/glample/fastBPE/zip/master [following]\n",
            "--2019-05-30 07:03:05--  https://codeload.github.com/glample/fastBPE/zip/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 192.30.253.120\n",
            "Connecting to codeload.github.com (codeload.github.com)|192.30.253.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [ <=>                ]   8.88K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-05-30 07:03:06 (107 KB/s) - ‘master.zip’ saved [9091]\n",
            "\n",
            "Archive:  master.zip\n",
            "87eeb4dc5dd1542c2346d18b35aad6942bd04c6f\n",
            "   creating: fastBPE-master/\n",
            "  inflating: fastBPE-master/LICENSE  \n",
            "  inflating: fastBPE-master/README.md  \n",
            "   creating: fastBPE-master/fastBPE/\n",
            "  inflating: fastBPE-master/fastBPE/fastBPE.hpp  \n",
            "  inflating: fastBPE-master/fastBPE/fastBPE.pyx  \n",
            "  inflating: fastBPE-master/fastBPE/main.cc  \n",
            "  inflating: fastBPE-master/setup.py  \n",
            " - compiling\n",
            "\u001b[01m\u001b[Kg++:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kfast.cc: No such file or directory\n",
            "\u001b[01m\u001b[Kg++:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kno input files\n",
            "compilation terminated.\n",
            "ERROR: compilation failed, please install manually\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq9RVwqi8pyM",
        "colab_type": "code",
        "outputId": "7c5eaf09-32c7-45f1-bef3-aa2e5c6ea1d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd tools-external/fastBPE/\n",
        "! g++ -std=c++11 -pthread -O3 fastBPE/main.cc -IfastBPE -o fast"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LASER/tools-external/fastBPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpraUBs38jZO",
        "colab_type": "code",
        "outputId": "4b910143-5681-4c2e-e550-83595d2f4b8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%cd ../../tasks/similarity/\n",
        "! ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LASER/tasks/similarity\n",
            "README.md  wmt.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVjP99krE02v",
        "colab_type": "code",
        "outputId": "d809e9eb-b0ec-4550-cfdd-0dacc0c501ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "! apt install libopenblas-base libomp-dev"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopenblas-base is already the newest version (0.2.20+ds-4).\n",
            "libopenblas-base set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libomp-doc\n",
            "The following NEW packages will be installed:\n",
            "  libomp-dev libomp5\n",
            "0 upgraded, 2 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 239 kB of archives.\n",
            "After this operation, 804 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp-dev amd64 5.0.1-1 [5,088 B]\n",
            "Fetched 239 kB in 0s (1,358 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 130911 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Selecting previously unselected package libomp-dev.\n",
            "Preparing to unpack .../libomp-dev_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp-dev (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libomp-dev (5.0.1-1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdgEtRK5AN9g",
        "colab_type": "code",
        "outputId": "058f113a-a137-416c-91f7-85e56f402c8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "! pip install faiss"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting faiss\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/1c/4ae6cb87cf0c09c25561ea48db11e25713b25c580909902a92c090b377c0/faiss-1.5.3-cp36-cp36m-manylinux1_x86_64.whl (4.7MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from faiss) (1.16.3)\n",
            "Installing collected packages: faiss\n",
            "Successfully installed faiss-1.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBPIo9wj9GSg",
        "colab_type": "code",
        "outputId": "67ae3db3-571d-417e-a58c-d3e77ecef159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "% cd ./tasks/similarity\n",
        "! bash wmt.sh"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: './tasks/similarity'\n",
            "/content/LASER/tasks/similarity\n",
            " - Download WMT data\n",
            "LASER: similarity search\n",
            "\n",
            "Processing:\n",
            " - loading encoder /content/LASER/models/bilstm.93langs.2018-12-26.pt\n",
            " - creating directory embed\n",
            " - Tokenizer: newstest2012.cs in language cs  \n",
            " - fast BPE: processing newstest2012.tok.cs\n",
            " - Encoder: newstest2012.bpe.cs to newstest2012.enc.cs\n",
            " - Encoder: 3003 sentences in 5s\n",
            " - embedding: ./embed/newstest2012.enc.cs 3003 examples of dim 1024\n",
            " - creating FAISS index\n",
            " - Tokenizer: newstest2012.de in language de  \n",
            " - fast BPE: processing newstest2012.tok.de\n",
            " - Encoder: newstest2012.bpe.de to newstest2012.enc.de\n",
            " - Encoder: 3003 sentences in 4s\n",
            " - embedding: ./embed/newstest2012.enc.de 3003 examples of dim 1024\n",
            " - creating FAISS index\n",
            " - Tokenizer: newstest2012.en in language en  \n",
            " - fast BPE: processing newstest2012.tok.en\n",
            " - Encoder: newstest2012.bpe.en to newstest2012.enc.en\n",
            " - Encoder: 3003 sentences in 4s\n",
            " - embedding: ./embed/newstest2012.enc.en 3003 examples of dim 1024\n",
            " - creating FAISS index\n",
            " - Tokenizer: newstest2012.es in language es  \n",
            " - fast BPE: processing newstest2012.tok.es\n",
            " - Encoder: newstest2012.bpe.es to newstest2012.enc.es\n",
            " - Encoder: 3003 sentences in 4s\n",
            " - embedding: ./embed/newstest2012.enc.es 3003 examples of dim 1024\n",
            " - creating FAISS index\n",
            " - Tokenizer: newstest2012.fr in language fr  \n",
            " - fast BPE: processing newstest2012.tok.fr\n",
            " - Encoder: newstest2012.bpe.fr to newstest2012.enc.fr\n",
            " - Encoder: 3003 sentences in 4s\n",
            " - embedding: ./embed/newstest2012.enc.fr 3003 examples of dim 1024\n",
            " - creating FAISS index\n",
            "Confusion matrix:\n",
            "langs   cs       de       en       es       fr       avg     \n",
            "cs     0.00%    0.70%    0.90%    0.67%    0.77%    0.76%\n",
            "de     0.83%    0.00%    1.17%    0.93%    1.03%    0.99%\n",
            "en     0.93%    1.27%    0.00%    0.83%    1.07%    1.02%\n",
            "es     0.53%    0.77%    0.97%    0.00%    0.57%    0.71%\n",
            "fr     0.50%    0.90%    1.13%    0.60%    0.00%    0.78%\n",
            "avg    0.70%    0.91%    1.04%    0.76%    0.86%    1.07%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sS0FP2kG-cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample feasibility check\n",
        "# % cd ../../source/\n",
        "\n",
        "import os\n",
        "import sys\n",
        "LASER = os.environ['LASER']\n",
        "# now include the extra files in the source\n",
        "sys.path.append(LASER + '/source')\n",
        "sys.path.append(LASER + '/source/lib')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39VFH-l3G-l4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from embed import SentenceEncoder, EncodeLoad, EncodeFile\n",
        "from text_processing import Token, BPEfastApply\n",
        "from indexing import IndexCreate, IndexSearchMultiple, IndexPrintConfusionMatrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLAqI6hNbCtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tnCKLFUbC2s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c198f42d-f3af-4d5c-b311-e5b2f88516c4"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/LASER/tasks/similarity'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKRw4tSWb1jW",
        "colab_type": "text"
      },
      "source": [
        "## These are the steps to replicate results of the LSTM file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvPCpy5Ucnaa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49248a57-9b98-4568-ef44-a25fc7eac945"
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN4E6iNobC0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "# Training data\n",
        "train_en_sp = pd.read_csv('cikm_english_train_20180516.txt', sep='\t', header=None, error_bad_lines=False)\n",
        "train_sp_en = pd.read_csv('cikm_spanish_train_20180516.txt', sep='\t', header=None, error_bad_lines=False)\n",
        "\n",
        "train_en_sp.columns = ['english1', 'spanish1', 'english2', 'spanish2', 'result']\n",
        "train_sp_en.columns = ['spanish3', 'english3', 'spanish4', 'english4', 'result']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0n9Wyq-bCyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sp_sp = pd.read_csv('cikm_test_a_20180516.txt', sep='\t', header=None, error_bad_lines=False)\n",
        "test_sp_sp.columns = ['spanish5', 'spanish6']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERcXRM7_cIUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a752a74b-2923-4f91-bf6a-31b7317b642e"
      },
      "source": [
        "all_en = pd.DataFrame(pd.concat([train_en_sp['english1'], train_en_sp['english2'], \n",
        "                                   train_sp_en['english3'], train_sp_en['english4']], axis=0))\n",
        "all_en.columns = ['english']\n",
        "all_en = all_en.reset_index()\n",
        "all_en = all_en.drop(columns='index')\n",
        "print(all_en.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42800, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96vcmA5bcIcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe0f7028-f189-4d4b-fd1e-096b2664eb14"
      },
      "source": [
        "all_sp = pd.DataFrame(pd.concat([train_en_sp['spanish1'], train_en_sp['spanish2'], train_sp_en['spanish3'], \n",
        "                                   train_sp_en['spanish4'],test_sp_sp['spanish5'], test_sp_sp['spanish6']], axis=0))\n",
        "all_sp.columns = ['spanish']\n",
        "all_sp = all_sp.reset_index()\n",
        "all_sp = all_sp.drop(columns='index')\n",
        "print(all_sp.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(52800, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMN7XNdxdlG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us go ahead and save these files so that they can be used later by the Tokennization process\n",
        "import csv\n",
        "all_en.to_csv('all_en.txt',header=None, index=None, sep=' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTVFwYk_cIZ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1faf2e0c-ebcb-42ea-ee43-de44877bc62c"
      },
      "source": [
        "# Many of the LASER components are already in the path so we can directly use them then\n",
        "from text_processing import Token\n",
        "Token(os.path.join('all_en.txt'),\n",
        "          'all_en.tok',\n",
        "          lang='en',\n",
        "          romanize= False,\n",
        "          lower_case=True,\n",
        "          verbose=True, over_write=False)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Tokenizer: all_en.txt in language en  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHVv21bCcIXf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6e470b9-de62-42c1-d95c-673c6a34fc56"
      },
      "source": [
        "from text_processing import BPEfastApply\n",
        "\n",
        "BPEfastApply('all_en.tok',     \n",
        "            'all_en.bpe',\n",
        "             './LASER/models/93langs.fcodes',\n",
        "              verbose=True, over_write=False)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - fast BPE: processing all_en.tok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ZY3F5ZbCwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_PATH = '/content/LASER/models/'\n",
        "sentence_encoder = SentenceEncoder(\n",
        "    str(MODEL_PATH + \"bilstm.93langs.2018-12-26.pt\"),\n",
        "    max_sentences=None,\n",
        "    max_tokens=10000,\n",
        "    cpu=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP9GvdEQEhSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "class Siamese_nlp(nn.Module):\n",
        "    def __init__(self,  encoder, fc_dim, num_classes=1): # binary classification so 0/1 shall work\n",
        "        super(Siamese_nlp, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.fc_dim = fc_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.encoder.hidden_size = 1024 # This is fixed for the LASER encoder by construction\n",
        "        \n",
        "        \n",
        "        self.input_dim = 2  * self.encoder.hidden_size # since we concatinate the two sentences together\n",
        "  \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, int(self.input_dim/2)),\n",
        "            nn.Linear(int(self.input_dim/2), self.num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, s1, s2):\n",
        "        \n",
        "        encoded_sentence_1 = self.encoder.encode_sentences(sentences = s1)\n",
        "        encoded_sentence_2 = self.encoder.encode_sentences(sentences = s2)\n",
        "        \n",
        "        # utilize these two encoded vectors\n",
        "        features = torch.cat((torch.from_numpy(encoded_sentence_1),torch.from_numpy(encoded_sentence_2)), dim=0)\n",
        "        # features = v1-v2\n",
        "        features = features.cuda()\n",
        "        #print(\"size of the feature is {}\".format(features.shape))\n",
        "        features = features.reshape(-1)\n",
        "        output = self.classifier(features)\n",
        "        #print(\"One successful run\")\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhUfdh4YPift",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "78b663fd-baa8-4c52-f1b9-a775b06db05f"
      },
      "source": [
        "# model\n",
        "siamese = Siamese_nlp(encoder=sentence_encoder, fc_dim=100) #fc_dim value is arbitary here\n",
        "siamese.cuda()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Siamese_nlp(\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "    (1): Linear(in_features=1024, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqaw0k_sMuV-",
        "colab_type": "text"
      },
      "source": [
        "## Generate the Spanish sentences on which we shall train and evaluate our system\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ6dFNYhMttK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a76f1bb-85c5-416b-b542-04d8332f4f8b"
      },
      "source": [
        "import numpy as np\n",
        "df_train_en_sp = pd.read_csv('./cikm_english_train_20180516.txt', sep='\t', header=None,\n",
        "                                 error_bad_lines=False)\n",
        "df_train_sp_en = pd.read_csv('./cikm_spanish_train_20180516.txt', sep='\t', header=None,\n",
        "                             error_bad_lines=False)\n",
        "df_train_en_sp.columns = ['english1', 'spanish1', 'english2', 'spanish2', 'result']\n",
        "df_train_sp_en.columns = ['spanish1', 'english1', 'spanish2', 'english2', 'result']\n",
        "train1 = pd.DataFrame(pd.concat([df_train_en_sp['spanish1'], df_train_sp_en['spanish1']], axis=0))\n",
        "train2 = pd.DataFrame(pd.concat([df_train_en_sp['spanish2'], df_train_sp_en['spanish2']], axis=0))\n",
        "train_data = pd.concat([train1, train2], axis=1).reset_index()\n",
        "train_data = train_data.drop(['index'], axis=1)\n",
        "result = pd.DataFrame(pd.concat([df_train_en_sp['result'], df_train_sp_en['result']], axis=0)).reset_index()\n",
        "result = result.drop(['index'], axis=1)\n",
        "# pd.get_dummies(result['result']).head()\n",
        "train_data['result'] = result\n",
        "\n",
        "# Evaluation data\n",
        "test_data = pd.read_csv('./cikm_test_a_20180516.txt', sep='\t', header=None, error_bad_lines=False)\n",
        "test_data.columns = ['spanish1', 'spanish2']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data.replace('', np.nan, inplace=True)\n",
        "dirty_data = train_data[train_data.isnull().any(axis=1)]\n",
        "\n",
        "train_data = train_data.dropna()\n",
        "test_data.replace('', np.nan, inplace=True)\n",
        "test_data = test_data.dropna()\n",
        "print ('Train sample count:', train_data.shape[0], 'Test sample count:', test_data.shape[0])\n",
        "\n",
        "train_data.columns = ['s1', 's2', 'label']\n",
        "test_data.columns = ['s1', 's2']\n",
        "\n",
        "train_data.to_csv(\"spanish_train.csv\", index=False)\n",
        "test_data.to_csv(\"spanish_test.csv\", index=False)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train sample count: 21400 Test sample count: 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkqGHlSkMtot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BhJMYBGEhO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class myDS_train(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        # Assign vocabularies.\n",
        "        self.s1 = df['s1']\n",
        "        self.s2 = df['s2']\n",
        "        self.label = df['label'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence_1 = self.s1.iloc[idx]\n",
        "        sentence_2 = self.s2.iloc[idx]\n",
        "        label = float(self.label[idx])\n",
        "\n",
        "        return sentence_1, sentence_2, label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inqgccItDK62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myDS_test(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        # Assign vocabularies.\n",
        "        self.s1 = df['s1']\n",
        "        self.s2 = df['s2']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.s1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Split sentence into words.\n",
        "        sentence_1 = self.s1.iloc[idx]\n",
        "        sentence_2 = self.s2.iloc[idx]\n",
        "        \n",
        "        return sentence_1, sentence_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG03Ab3dMsj7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d826058-a33c-49f3-e2ca-b288e57e5e09"
      },
      "source": [
        "import pandas as pd\n",
        "\"\"\" Read Data \"\"\"\n",
        "\n",
        "train_data = pd.read_csv('spanish_train.csv', sep=',')\n",
        "test_data = pd.read_csv('spanish_test.csv', sep=',')\n",
        "\n",
        "# split dataset\n",
        "msk = np.random.rand(len(train_data)) < 0.8\n",
        "train = train_data[msk]\n",
        "valid = train_data[~msk]\n",
        "\n",
        "# dataset\n",
        "trainDS = myDS_train(train)\n",
        "validDS = myDS_train(valid) # Since this is still labeled data that we have\n",
        "\n",
        "print ('Data size:',train_data.shape[0], test_data.shape[0])"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size: 21400 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV7MIs173AJG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c200fc0d-9ca8-462a-d1da-2810aa07ba6a"
      },
      "source": [
        "# Quickly check if csv file has everything in place\n",
        "train_data.head()"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hola, hago clic en el producto recibido</td>\n",
              "      <td>Compré un producto y no he recibido un correo ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>¡Hola! Cerré la disputa el 21 de mayo de 2017 ...</td>\n",
              "      <td>No obtuve el reembolso de mi dinero. Han pasad...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ordené de España a España ahora que mandan el ...</td>\n",
              "      <td>Mi pedido llegó pero el color es diferente al ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>¿Debo pagar impuestos personalizados?</td>\n",
              "      <td>Cómo pagar los derechos de aduana</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>No recibí mi pedido?</td>\n",
              "      <td>Mi pedido muestra que no he pagado, pero lo hice</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  s1  ... label\n",
              "0            hola, hago clic en el producto recibido  ...     0\n",
              "1  ¡Hola! Cerré la disputa el 21 de mayo de 2017 ...  ...     0\n",
              "2  Ordené de España a España ahora que mandan el ...  ...     0\n",
              "3              ¿Debo pagar impuestos personalizados?  ...     1\n",
              "4                               No recibí mi pedido?  ...     0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yA5D8Ki3ANY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxB0qkjO3AG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5l-FZdh3ABw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA3D3dwoO0gM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer\n",
        "import torch\n",
        "def create_optimizer(optimizer_type, network=siamese, learning_rate = 1e-3):\n",
        "    optimizer = None\n",
        "    if optimizer_type == 'sgd':\n",
        "        optimizer = torch.optim.SGD(filter(lambda x: x.requires_grad, network.parameters()), lr=learning_rate)\n",
        "    elif optimizer_type == 'adam':\n",
        "        optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, network.parameters()), lr=learning_rate)\n",
        "    elif optimizer_type == 'adadelta':\n",
        "        optimizer = torch.optim.Adadelta(filter(lambda x: x.requires_grad, network.parameters()), lr=learning_rate)\n",
        "    elif optimizer_type == 'rmsprop':\n",
        "        optimizer = torch.optim.RMSprop(filter(lambda x: x.requires_grad, network.parameters()), lr=learning_rate)\n",
        "    return optimizer    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g01ex4oXSyFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = create_optimizer(optimizer_type='adam', network=siamese, learning_rate=1e-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4oKrtovR3NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss func\n",
        "from torch.autograd import Variable\n",
        "criterion = torch.nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUvD9wlbR3K8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# log info\n",
        "train_log_string = '%s :: Epoch %i :: Iter %i / %i :: train loss: %0.4f'\n",
        "valid_log_string = '%s :: Epoch %i :: valid loss: %0.4f\\n'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRhz4FqYTjBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 5\n",
        "ckpt_path = '/content/sample.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH6PTZ2yR3Hb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "2e01f4bd-8db2-42d6-f21e-e21c98490d17"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "# save every epoch for visualization\n",
        "train_loss_record = []\n",
        "valid_loss_record = []\n",
        "best_record = 10.0\n",
        "\n",
        "# training\n",
        "print ('Experiment: Siamese_lstm take 01 \\n')\n",
        "\n",
        "    \n",
        "for epoch in range(num_epochs):\n",
        "    print ('Start Epoch{} Training...'.format(epoch))\n",
        "\n",
        "    # loss\n",
        "    train_loss = []\n",
        "    train_loss_sum = []\n",
        "    # dataloader\n",
        "    train_dataloader = DataLoader(dataset=trainDS, shuffle=True, num_workers=2, batch_size=1)\n",
        "\n",
        "    siamese.train() #set it in the training mode\n",
        "    \n",
        "    for idx, data in enumerate(train_dataloader, 0):\n",
        "\n",
        "        # get data\n",
        "        s1, s2, label = data\n",
        "        # putting the data into cuda\n",
        "        #s1 = torch.from_numpy(np.array(s1))\n",
        "        #s2 = torch.from_numpy(np.array(s2))\n",
        "        label = torch.tensor(label.data, dtype=torch.float)\n",
        "#         s1 = s1.cuda()\n",
        "#         s2 = s2.cuda()\n",
        "        label = label.cuda()\n",
        "        \n",
        "        # clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        \n",
        "        # input\n",
        "        output = siamese(s1, s2)\n",
        "        #print(output)\n",
        "        #output = output.squeeze(0)\n",
        "        # Obtain the maximum index now\n",
        "        #logit = torch.max(output) # First index is the value and the second is the location of occurance of maximum\n",
        "        #samp = logit.cpu().numpy()\n",
        "        #output_list.append(samp)\n",
        "        #output = torch.from_numpy(np.asarray(output_list))\n",
        "        \n",
        "        #output = output.cuda()\n",
        "        # Get the shape of output and label just to check\n",
        "        \n",
        "\n",
        "        # loss backward\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss.data.cpu())\n",
        "        train_loss_sum.append(loss.data.cpu())\n",
        "\n",
        "        # Every once and a while check on the loss\n",
        "        if ((idx + 1) % 5000) == 0:\n",
        "            print(train_log_string % (datetime.now(), epoch, idx + 1, len(train), np.mean(train_loss)))\n",
        "            train_loss = []\n",
        "\n",
        "    # Record at every epoch\n",
        "    print ('Train Loss at epoch{}: {}\\n'.format(epoch, np.mean(train_loss_sum)))\n",
        "    train_loss_record.append(np.mean(train_loss_sum))\n",
        "\n",
        "    # Valid\n",
        "    print ('Epoch{} Validating...'.format(epoch))\n",
        "\n",
        "    # loss\n",
        "\n",
        "    \n",
        "    valid_loss = []\n",
        "    # dataloader\n",
        "    valid_dataloader = DataLoader(dataset=validDS, shuffle=True, num_workers=2, batch_size=1)\n",
        "    \n",
        "    siamese.eval() # putting the model in evaluation mode\n",
        "\n",
        "    for idx, data in enumerate(valid_dataloader, 0):\n",
        "        # get data\n",
        "        s1, s2, label = data\n",
        "\n",
        "        # putting the data into cuda\n",
        "        #s1 = torch.from_numpy(np.array(s1))\n",
        "        #s2 = torch.from_numpy(np.array(s2))\n",
        "        label = torch.tensor(label.data, dtype=torch.float)\n",
        "        #print(\"something something meri jaan, tell me something meri jaan\")\n",
        "#         s1 = s1.cuda()\n",
        "#         s2 = s2.cuda()\n",
        "        label = label.cuda()\n",
        "        \n",
        "        # input\n",
        "        output = siamese(s1, s2)\n",
        "        #output = output.squeeze(0)\n",
        "        # To get the maximum index from this output as the prediction of the network\n",
        "        #output = torch.max(output)[1] # First index is the value and the second is the location of occurance of maximum\n",
        "        \n",
        "        # loss\n",
        "        loss = criterion(output, label)\n",
        "        valid_loss.append(loss.data.cpu())\n",
        "        \n",
        "\n",
        "    print(valid_log_string % (datetime.now(), epoch, np.mean(valid_loss)))\n",
        "    # Record\n",
        "    valid_loss_record.append(np.mean(valid_loss))\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "    # Keep track of best record\n",
        "    if np.mean(valid_loss) < best_record:\n",
        "        best_record = np.mean(valid_loss)\n",
        "        # save the best model\n",
        "        state_dict = {\n",
        "            'epoch': epoch,\n",
        "            'siamese': siamese.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }\n",
        "        torch.save(state_dict, ckpt_path)\n",
        "        print ('Model saved!\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment: Siamese_lstm take 01 \n",
            "\n",
            "Start Epoch0 Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-30 09:34:23.457535 :: Epoch 0 :: Iter 5000 / 17188 :: train loss: 1.3278\n",
            "2019-05-30 09:36:29.344466 :: Epoch 0 :: Iter 10000 / 17188 :: train loss: 1.5500\n",
            "2019-05-30 09:38:35.260146 :: Epoch 0 :: Iter 15000 / 17188 :: train loss: 1.6664\n",
            "Train Loss at epoch0: 1.5230345726013184\n",
            "\n",
            "Epoch0 Validating...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-30 09:41:08.846733 :: Epoch 0 :: valid loss: 2.7550\n",
            "\n",
            "Model saved!\n",
            "\n",
            "Start Epoch1 Training...\n",
            "2019-05-30 09:43:15.083231 :: Epoch 1 :: Iter 5000 / 17188 :: train loss: 1.7801\n",
            "2019-05-30 09:45:20.564717 :: Epoch 1 :: Iter 10000 / 17188 :: train loss: 1.8376\n",
            "2019-05-30 09:47:26.326007 :: Epoch 1 :: Iter 15000 / 17188 :: train loss: 1.9514\n",
            "Train Loss at epoch1: 1.8739523887634277\n",
            "\n",
            "Epoch1 Validating...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLr0qFou-6tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruvk-1IkDK2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN_y2Bf5bCqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3PdGfctQ4ug",
        "colab_type": "code",
        "outputId": "6e138f8b-0bac-4360-88c1-c0d25071be72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "tensor_max_example = torch.tensor(\n",
        "[1,2,3,4\n",
        "])\n",
        "\n",
        "# Try the maximum operation in here\n",
        "result, index = torch.max(tensor_max_example,dim=0)\n",
        "print(type(torch.max(tensor_max_example,dim=0)[1]))\n",
        "print(result, index.cpu())"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "tensor(4) tensor(3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfgPt9oqYMIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "3d7b31c3-b63b-488e-c3d6-2e14b2476b10"
      },
      "source": [
        "embed.is_cuda()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-ba6fdc56a0e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'is_cuda'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG19Q85QYMGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbbKc9g4YMEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1g8DqAbYL_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5s572DPJmIR",
        "colab_type": "code",
        "outputId": "e9a4dfb5-0a62-46b9-e72f-eb8275bb6060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "DATA_PATH = LASER + '/data/tatoeba/v1/'\n",
        "#os.mkdir(LASER+'/embed')\n",
        "CACHE_PATH = LASER+ '/embed/'\n",
        "Token(\n",
        "    str(DATA_PATH + \"tatoeba.deu-eng.eng\"),\n",
        "    str(CACHE_PATH + \"tatoeba.deu-eng.eng\"),\n",
        "    lang=\"zh\",\n",
        "    romanize=False,\n",
        "    lower_case=True, gzip=False,\n",
        "    verbose=False, over_write=False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-c2ce9e5b62ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mromanize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     verbose=False, over_write=False)\n\u001b[0m",
            "\u001b[0;32m/content/LASER/source/lib/text_processing.py\u001b[0m in \u001b[0;36mToken\u001b[0;34m(inp_fname, out_fname, lang, lower_case, romanize, descape, verbose, over_write, gzip)\u001b[0m\n\u001b[1;32m    100\u001b[0m                    \u001b[0;34m+\u001b[0m \u001b[0;34m'>'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_fname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                    \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLD_LIBRARY_PATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMECAB\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/lib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                    shell=True, check=True)\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mover_write\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         print(' - Tokenizer: {} exists already'\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 418\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'cat /content/LASER/data/tatoeba/v1/tatoeba.deu-eng.eng|/content/LASER/tools-external/moses-tokenizer/tokenizer/remove-non-printing-char.perl|/content/LASER/tools-external/moses-tokenizer/tokenizer/normalize-punctuation.perl -l zh|/content/LASER/tools-external/moses-tokenizer/tokenizer/tokenizer.perl -q -no-escape -threads 20 -l zh| python3 -m jieba -d |python3 /content/LASER/source/lib/romanize_lc.py -l none>/content/LASER/embed/tatoeba.deu-eng.eng' returned non-zero exit status 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzi20yUrJmF2",
        "colab_type": "code",
        "outputId": "1a729b8b-14a2-4b31-988b-c59aaea0f241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# Now the byte pair-encoding part\n",
        "MODEL_PATH = LASER+ '/models/'\n",
        "bpe_codes = str(MODEL_PATH + \"93langs.fcodes\")\n",
        "BPEfastApply(\n",
        "    str(CACHE_PATH + \"tatoeba.deu-eng.eng\"),\n",
        "    str(CACHE_PATH + \"tatoeba.deu-eng.eng.bpe\"),\n",
        "    bpe_codes,\n",
        "    verbose=True, over_write=False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - fast BPE: processing tatoeba.deu-eng.eng\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-14cebebeacc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCACHE_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"tatoeba.deu-eng.eng.bpe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbpe_codes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     verbose=True, over_write=False)\n\u001b[0m",
            "\u001b[0;32m/content/LASER/source/lib/text_processing.py\u001b[0m in \u001b[0;36mBPEfastApply\u001b[0;34m(inp_fname, out_fname, bpe_codes, verbose, over_write)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;34m+\u001b[0m \u001b[0mout_fname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minp_fname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbpe_codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             + ' ' + bpe_vocab, shell=True, stderr=DEVNULL, check=True)\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mover_write\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         print(' - fast BPE: {} exists already'\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 418\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '/content/LASER/tools-external/fastBPE/fast applybpe /content/LASER/embed/tatoeba.deu-eng.eng.bpe /content/LASER/embed/tatoeba.deu-eng.eng /content/LASER/models/93langs.fcodes /content/LASER/models/93langs.fvocab' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcJKEx1FJmDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = SentenceEncoder(\n",
        "    str(MODEL_PATH + \"bilstm.93langs.2018-12-26.pt\"),\n",
        "    max_sentences=None,\n",
        "    max_tokens=10000,\n",
        "    cpu=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgx5WAVOJmA5",
        "colab_type": "code",
        "outputId": "d738ffe1-18d9-454f-d422-791752c650c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(encoder.encoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder(\n",
            "  (embed_tokens): Embedding(73640, 320, padding_idx=1)\n",
            "  (lstm): LSTM(320, 512, num_layers=5, bidirectional=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ4cojPhR6rL",
        "colab_type": "code",
        "outputId": "c463ee7d-4ccf-47c4-d306-53d61147ec40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "sentences = ['the moon drinks alcohol but not many do.','the dog drinks and moves in the sun.','the man is dancing like an idiot']\n",
        "encodings = (encoder.encode_sentences(sentences))\n",
        "encodings.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYAK8nlASw8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jilN2Fg0Sxt3",
        "colab_type": "code",
        "outputId": "dcea6efb-03c0-4e5f-9435-b352713ee3f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encod1 = cosine(encodings[0], encodings[1])\n",
        "encod2 = cosine(encodings[1], encodings[2])\n",
        "encod3 = cosine(encodings[0], encodings[2])\n",
        "print(np.max(np.asarray(encod1, encod2, encod3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6521177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g4ApxATVzfO",
        "colab_type": "code",
        "outputId": "741a8cec-ac7c-434c-81e9-cfd3e488fa0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "encodings_1 = encoder.encode_sentences(sentences[0])\n",
        "encodings_2 = encoder.encode_sentences(sentences[1])\n",
        "similarity = metrics.pairwise.cosine_similarity(encodings_1,encodings_2 )\n",
        "similarity"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9999999 , 0.84819144, 0.7724452 , 0.64278316, 0.86915743,\n",
              "        0.73745847, 0.8357326 , 0.64278316, 0.86915743, 0.8623911 ,\n",
              "        0.7655606 , 0.86331487, 0.86527085, 0.86991817, 0.71053386],\n",
              "       [0.84819144, 0.99999994, 0.8039628 , 0.60429525, 0.8414177 ,\n",
              "        0.79396594, 0.86199105, 0.60429525, 0.8414177 , 0.83985245,\n",
              "        0.7920738 , 0.8434861 , 0.8635477 , 0.8714973 , 0.7140627 ],\n",
              "       [0.7724452 , 0.8039628 , 1.0000005 , 0.705362  , 0.84215397,\n",
              "        0.9026381 , 0.7958801 , 0.705362  , 0.84215397, 0.7930572 ,\n",
              "        0.9259295 , 0.8504628 , 0.80700976, 0.82892334, 0.8174349 ],\n",
              "       [0.64278316, 0.60429525, 0.705362  , 0.99999994, 0.71757567,\n",
              "        0.65865666, 0.63482505, 0.99999994, 0.71757567, 0.6238993 ,\n",
              "        0.69545513, 0.68329984, 0.6148747 , 0.64136684, 0.8383063 ],\n",
              "       [0.8326666 , 0.8805575 , 0.80844045, 0.62919205, 0.85974383,\n",
              "        0.8033582 , 0.8779266 , 0.62919205, 0.85974383, 0.81746775,\n",
              "        0.8145156 , 0.83861154, 0.8884791 , 0.8683918 , 0.73718935],\n",
              "       [0.80762947, 0.8353993 , 0.9134916 , 0.69286805, 0.8833497 ,\n",
              "        0.9158255 , 0.8355381 , 0.69286805, 0.8833497 , 0.8397759 ,\n",
              "        0.9068426 , 0.88212943, 0.8435041 , 0.8603674 , 0.7769672 ],\n",
              "       [0.9999999 , 0.84819144, 0.7724452 , 0.64278316, 0.86915743,\n",
              "        0.73745847, 0.8357326 , 0.64278316, 0.86915743, 0.8623911 ,\n",
              "        0.7655606 , 0.86331487, 0.86527085, 0.86991817, 0.71053386],\n",
              "       [0.64278316, 0.60429525, 0.705362  , 0.99999994, 0.71757567,\n",
              "        0.65865666, 0.63482505, 0.99999994, 0.71757567, 0.6238993 ,\n",
              "        0.69545513, 0.68329984, 0.6148747 , 0.64136684, 0.8383063 ],\n",
              "       [0.86915743, 0.8414177 , 0.84215397, 0.71757567, 0.9999994 ,\n",
              "        0.81088996, 0.87158674, 0.71757567, 0.9999994 , 0.87534595,\n",
              "        0.84689015, 0.87747264, 0.84832877, 0.87317836, 0.7989505 ],\n",
              "       [0.8623911 , 0.83985245, 0.7930572 , 0.6238993 , 0.87534595,\n",
              "        0.77563167, 0.85636055, 0.6238993 , 0.87534595, 1.0000001 ,\n",
              "        0.8060592 , 0.8770285 , 0.86984795, 0.8735258 , 0.6990633 ],\n",
              "       [0.7655606 , 0.7920738 , 0.9259295 , 0.69545513, 0.84689015,\n",
              "        0.90215826, 0.8101361 , 0.69545513, 0.84689015, 0.8060592 ,\n",
              "        1.0000002 , 0.88002867, 0.83033514, 0.8535912 , 0.79597163],\n",
              "       [0.86331487, 0.8434861 , 0.8504628 , 0.68329984, 0.87747264,\n",
              "        0.83666927, 0.8575312 , 0.68329984, 0.87747264, 0.8770285 ,\n",
              "        0.88002867, 1.        , 0.8730652 , 0.8939367 , 0.78223205],\n",
              "       [0.86527085, 0.8635477 , 0.80700976, 0.6148747 , 0.84832877,\n",
              "        0.81158483, 0.8922446 , 0.6148747 , 0.84832877, 0.86984795,\n",
              "        0.83033514, 0.8730652 , 1.        , 0.88293505, 0.7073318 ],\n",
              "       [0.86991817, 0.8714973 , 0.82892334, 0.64136684, 0.87317836,\n",
              "        0.8147763 , 0.8478929 , 0.64136684, 0.87317836, 0.8735258 ,\n",
              "        0.8535912 , 0.8939367 , 0.88293505, 0.9999998 , 0.7148116 ],\n",
              "       [0.71053386, 0.7140627 , 0.8174349 , 0.8383063 , 0.7989505 ,\n",
              "        0.77588   , 0.72443604, 0.8383063 , 0.7989505 , 0.6990633 ,\n",
              "        0.79597163, 0.78223205, 0.7073318 , 0.7148116 , 1.0000002 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xa1l-h2TVwm",
        "colab_type": "code",
        "outputId": "f6db08fe-b291-462e-df5d-2b2d86eb4b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encodings[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.01505004, -0.00041162,  0.00388943, ...,  0.01399686,\n",
              "       -0.00064748,  0.00807147], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f9wr96NTYhX",
        "colab_type": "code",
        "outputId": "03f94301-8650-41ee-dca0-692d9f9e329e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encodings[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01195887, 0.00011934, 0.00153836, ..., 0.01574299, 0.00082941,\n",
              "       0.00269687], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkEZU9jXJl-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = \"A quick brown fox jumps over the lazy dog\"\n",
        "encoded_sentence_1 = encoder.encode_sentences(sentence)\n",
        "encoded_sentence_2 = encoder.encode_sentences(sentence+\"haula haula baula baula laula laula\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OaaEDwvY-Qt",
        "colab_type": "code",
        "outputId": "16db412c-142c-4af4-a53a-b8615bfbc3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "EncodeFile(\n",
        "    encoder,\n",
        "    str(CACHE_PATH + \"tatoeba.deu-eng.eng.bpe\"),\n",
        "    str(CACHE_PATH + \"tatoeba.deu-eng.eng.enc\"),\n",
        "    verbose=True, over_write=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Encoder: tatoeba.deu-eng.eng.bpe to tatoeba.deu-eng.eng.enc\n",
            " - Encoder: 1000 sentences in 0s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocvE3l_OUTtY",
        "colab_type": "code",
        "outputId": "4a964477-df49-47c4-af39-49f01193560a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "similarity = metrics.pairwise.cosine_similarity(encoded_sentence_1, encoded_sentence_2)\n",
        "print(encoded_sentence_1.shape)\n",
        "print(encoded_sentence_2.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(41, 1024)\n",
            "(76, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv2_ix8NWCCp",
        "colab_type": "code",
        "outputId": "bff798ca-2312-4e07-c34f-84a0552d385a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data_en, index_en = IndexCreate(\n",
        "   CACHE_PATH + \"tatoeba.deu-eng.eng.enc\" , 'FlatL2', verbose=True, save_index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - embedding: /content/LASER/embed/tatoeba.deu-eng.eng.enc 1000 examples of dim 1024\n",
            " - creating FAISS index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxfIH7nkg-PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = []\n",
        "all_index = []\n",
        "all_data.append(data_en)\n",
        "all_index.append(index_en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZhlLGmlg-aL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUUUWk5UXVy2",
        "colab_type": "code",
        "outputId": "ccad30f8-b4b3-42d0-901f-135e81927a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(index_en.ntotal)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-BzWBl1Zuc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us try to get an embedding of german text first and then get relevant terms from the other sentence\n",
        "deu_sentence = \"Der Junge rennt\"\n",
        "german_encoded = encoder.encode_sentences(deu_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfsNkabUeK9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_texts = []\n",
        "all_texts.append(deu_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kFjlSVXnoEB",
        "colab_type": "code",
        "outputId": "4def61cc-ae4c-4665-c78f-a5f8051a4d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "git: 'lfs' is not a git command. See 'git --help'.\n",
            "\n",
            "The most similar command is\n",
            "\tlog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvCpXCWpl-1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from indexing import IndexSearchMultiple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVKGbdVGmXT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def IndexSearchMultiple(data, idx, verbose=False, texts=None, print_errors=False):\n",
        "    nl = len(data)\n",
        "    nbex = data[0].shape[0]\n",
        "    err = np.zeros((nl, nl)).astype(float)\n",
        "    ref = np.linspace(0, nbex-1, nbex).astype(int)  # [0, nbex)\n",
        "    if verbose:\n",
        "        if texts is None: \n",
        "            print('Calculating similarity error (indices):')\n",
        "        else:\n",
        "            print('Calculating similarity error (textual):')\n",
        "    for i1 in range(nl):\n",
        "        for i2 in range(nl):\n",
        "            if i1 != i2:\n",
        "                D, I = idx[i2].search(data[i1], 1)\n",
        "                if texts: # do textual comparison\n",
        "                    e1 = 0\n",
        "                    for p in range(I.shape[0]):\n",
        "                        if texts[i2][p] != texts[i2][I[p,0]]:\n",
        "                            e1 += 1\n",
        "                            if print_errors:\n",
        "                                print('Error {:s}\\n      {:s}'\n",
        "                                      .format(texts[i2][p].strip(), texts[i2][I[p,0]].strip()))\n",
        "                    err[i1, i2] = e1 / nbex\n",
        "                else:  # do index based comparision\n",
        "                    err[i1, i2] \\\n",
        "                        = (nbex - np.equal(I.reshape(nbex), ref)\n",
        "                           .astype(int).sum()) / nbex\n",
        "                if verbose:\n",
        "                    print(' - similarity error {:s}/{:s}: {:5d}={:5.2f}%'\n",
        "                          .format(args.langs[i1], args.langs[i2],\n",
        "                                  err[i1, i2], 100.0 * err[i1, i2]))\n",
        "    return err\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o97NTjVwemIG",
        "colab_type": "code",
        "outputId": "ced631d4-47cf-4d13-e7b5-d8734ca0b3f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "IndexSearchMultiple(all_data, all_index, texts=all_texts,\n",
        "                          verbose=True, print_errors=True)\n",
        "\n",
        "#IndexSearchMultiple(data=all_data, idx=all_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating similarity error (textual):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svzimjpSksRo",
        "colab_type": "code",
        "outputId": "335229b1-e7ce-48da-ae93-f43124050476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4403
        }
      },
      "source": [
        "! cat ./lib/indexing.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#!/usr/bin/python3\n",
            "# Copyright (c) Facebook, Inc. and its affiliates.\n",
            "# All rights reserved.\n",
            "#\n",
            "# This source code is licensed under the BSD-style license found in the\n",
            "# LICENSE file in the root directory of this source tree.\n",
            "#\n",
            "# LASER  Language-Agnostic SEntence Representations\n",
            "# is a toolkit to calculate multilingual sentence embeddings\n",
            "# and to use them for document classification, bitext filtering\n",
            "# and mining\n",
            "#\n",
            "# --------------------------------------------------------\n",
            "#\n",
            "# tools for indexing and search with FAISS\n",
            "\n",
            "import faiss\n",
            "import os.path\n",
            "import sys\n",
            "import numpy as np\n",
            "\n",
            "#-------------------------------------------------------------\n",
            "# Get list of fnames:\n",
            "#  - we loop over the list of given languages\n",
            "#  - for each language, we also check if there are splitted files .%03d\n",
            "\n",
            "def SplitFnames(par_fname, langs):\n",
            "    fnames = []\n",
            "    for l in langs:\n",
            "        fname = par_fname + '.' + l\n",
            "        if os.path.isfile(fname):\n",
            "            fnames.append(fname)\n",
            "        for i in range(1000):\n",
            "            fname = par_fname + '.' + l + '.{:03d}'.format(i)\n",
            "            if os.path.isfile(fname):\n",
            "                fnames.append(fname)\n",
            "    if len(fnames) == 0:\n",
            "        print(\"ERROR: no embeddings found in {:s}*\".format(par_fname))\n",
            "        sys.exit(1)\n",
            "    return fnames\n",
            "\n",
            "def SplitOpen(par_fname, langs, dim, dtype, verbose=False):\n",
            "    M = []\n",
            "    nf = 0\n",
            "    nc = 0\n",
            "    print('Reading sentence embeddings')\n",
            "    print(' - memory mapped files {:s}'.format(par_fname))\n",
            "    for fname in SplitFnames(par_fname, langs):\n",
            "        n = int(os.path.getsize(fname) / dim / np.dtype(dtype).itemsize)\n",
            "        if verbose:\n",
            "            print(' - {:s}: {:d} x {:d}'.format(fname, n, dim))\n",
            "        Mi = np.memmap(fname, mode='r', dtype=dtype, shape=(n, dim))\n",
            "        nc += n\n",
            "        nf += 1\n",
            "        M.append(Mi)\n",
            "    print(' - total of {:d} files: {:d} x {:d}'.format(nf, nc, dim))\n",
            "    return M\n",
            "\n",
            "def SplitAccess(M, idx):\n",
            "    i = idx\n",
            "    for Mi in M:\n",
            "        n = Mi.shape[0]\n",
            "        if i < n:\n",
            "            return Mi[i,:]\n",
            "        i -= n\n",
            "    print('ERROR: index {:d} is too large form memory mapped files'.format(idx))\n",
            "    sys.exit(1)\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# create an FAISS index on the given data\n",
            "\n",
            "def IndexCreate(dname, idx_type,\n",
            "                verbose=False, normalize=True, save_index=False, dim=1024):\n",
            "\n",
            "    assert idx_type == 'FlatL2', 'only FlatL2 index is currently supported'\n",
            "    x = np.fromfile(dname, dtype=np.float32, count=-1)\n",
            "    nbex = x.shape[0] // dim\n",
            "    print(' - embedding: {:s} {:d} examples of dim {:d}'\n",
            "          .format(dname, nbex, dim))\n",
            "    x.resize(nbex, dim)\n",
            "    print(' - creating FAISS index')\n",
            "    idx = faiss.IndexFlatL2(dim)\n",
            "    if normalize:\n",
            "        faiss.normalize_L2(x)\n",
            "    idx.add(x)\n",
            "    if save_index:\n",
            "        iname = 'TODO'\n",
            "        print(' - saving index into ' + iname)\n",
            "        faiss.write_index(idx, iname)\n",
            "    return x, idx\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# search closest vector for all languages pairs and calculate error rate\n",
            "\n",
            "def IndexSearchMultiple(data, idx, verbose=False, texts=None, print_errors=False):\n",
            "    nl = len(data)\n",
            "    nbex = data[0].shape[0]\n",
            "    err = np.zeros((nl, nl)).astype(float)\n",
            "    ref = np.linspace(0, nbex-1, nbex).astype(int)  # [0, nbex)\n",
            "    if verbose:\n",
            "        if texts is None: \n",
            "            print('Calculating similarity error (indices):')\n",
            "        else:\n",
            "            print('Calculating similarity error (textual):')\n",
            "    for i1 in range(nl):\n",
            "        for i2 in range(nl):\n",
            "            if i1 != i2:\n",
            "                D, I = idx[i2].search(data[i1], 1)\n",
            "                if texts: # do textual comparison\n",
            "                    e1 = 0\n",
            "                    for p in range(I.shape[0]):\n",
            "                        if texts[i2][p] != texts[i2][I[p,0]]:\n",
            "                            e1 += 1\n",
            "                            if print_errors:\n",
            "                                print('Error {:s}\\n      {:s}'\n",
            "                                      .format(texts[i2][p].strip(), texts[i2][I[p,0]].strip()))\n",
            "                    err[i1, i2] = e1 / nbex\n",
            "                else:  # do index based comparision\n",
            "                    err[i1, i2] \\\n",
            "                        = (nbex - np.equal(I.reshape(nbex), ref)\n",
            "                           .astype(int).sum()) / nbex\n",
            "                if verbose:\n",
            "                    print(' - similarity error {:s}/{:s}: {:5d}={:5.2f}%'\n",
            "                          .format(args.langs[i1], args.langs[i2],\n",
            "                                  err[i1, i2], 100.0 * err[i1, i2]))\n",
            "    return err\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# print confusion matrix\n",
            "\n",
            "def IndexPrintConfusionMatrix(err, langs):\n",
            "    nl = len(langs)\n",
            "    assert nl == err.shape[0], 'size of errror matrix doesn not match'\n",
            "    print('Confusion matrix:')\n",
            "    print('{:8s}'.format('langs'), end='')\n",
            "    for i2 in range(nl):\n",
            "        print('{:8s} '.format(langs[i2]), end='')\n",
            "    print('{:8s}'.format('avg'))\n",
            "    for i1 in range(nl):\n",
            "        print('{:3s}'.format(langs[i1]), end='')\n",
            "        for i2 in range(nl):\n",
            "            print('{:8.2f}%'.format(100 * err[i1, i2]), end='')\n",
            "        print('{:8.2f}%'.format(100 * err[i1, :].sum() / (nl-1)))\n",
            "\n",
            "    print('avg', end='')\n",
            "    for i2 in range(nl):\n",
            "        print('{:8.2f}%'.format(100 * err[:, i2].sum() / (nl-1)), end='')\n",
            "\n",
            "    # global average\n",
            "    print('{:8.2f}%'.format(100 * err.sum() / (nl-1) / nl))\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# Load an FAISS index\n",
            "\n",
            "def IndexLoad(idx_name, nprobe, gpu=False):\n",
            "    print('Reading FAISS index')\n",
            "    print(' - index: {:s}'.format(idx_name))\n",
            "    index = faiss.read_index(idx_name)\n",
            "    print(' - found {:d} sentences of dim {:d}'.format(index.ntotal, index.d))\n",
            "    print(' - setting nbprobe to {:d}'.format(nprobe))\n",
            "    if gpu:\n",
            "        print(' - transfer index to %d GPUs ' % faiss.get_num_gpus())\n",
            "        #co = faiss.GpuMultipleClonerOptions()\n",
            "        #co.shard = True\n",
            "        index = faiss.index_cpu_to_all_gpus(index) # co=co\n",
            "        faiss.GpuParameterSpace().set_index_parameter(index, 'nprobe', nprobe)\n",
            "    return index\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# Opens a text file with the sentences corresponding to the indices used\n",
            "# by an FAISS index\n",
            "# We also need the reference files with the byte offsets to the beginning\n",
            "# of each sentence\n",
            "# optionnally:  array with number of words per sentence\n",
            "# All arrays are memory mapped\n",
            "\n",
            "def IndexTextOpen(txt_fname):\n",
            "    print('Reading text corpus')\n",
            "    print(' - texts: {:s}'.format(txt_fname))\n",
            "    txt_mmap = np.memmap(txt_fname, mode='r', dtype=np.uint8)\n",
            "    fname = txt_fname.replace('.txt', '.ref.bin32')\n",
            "    if os.path.isfile(fname):\n",
            "        print(' - sentence start offsets (32 bit): {}'.format(fname))\n",
            "        ref_mmap = np.memmap(fname, mode='r', dtype=np.uint32)\n",
            "    else:\n",
            "        fname = txt_fname.replace('.txt', '.ref.bin64')\n",
            "        if os.path.isfile(fname):\n",
            "            print(' - sentence start offsets (64 bit): {}'.format(fname))\n",
            "            ref_mmap = np.memmap(fname, mode='r', dtype=np.uint64)\n",
            "        else:\n",
            "            print('ERROR: no file with sentence start offsets found')\n",
            "            sys.exit(1)\n",
            "    print(' - found {:d} sentences'.format(ref_mmap.shape[0]))\n",
            "\n",
            "    nbw_mmap = None\n",
            "    fname = txt_fname.replace('.txt', '.nw.bin8')\n",
            "    if os.path.isfile(fname):\n",
            "        print(' - word counts: {:s}'.format(fname))\n",
            "        nbw_mmap = np.memmap(fname, mode='r', dtype=np.uint8)\n",
            "\n",
            "    M = None\n",
            "    fname = txt_fname.replace('.txt', '.meta')\n",
            "    if os.path.isfile(fname):\n",
            "        M = []\n",
            "        n = 0\n",
            "        print(' - metafile: {:s}'.format(fname))\n",
            "        with open(fname, 'r') as fp:\n",
            "            for line in fp:\n",
            "                fields = line.strip().split()\n",
            "                if len(fields) != 2:\n",
            "                    print('ERROR: format error in meta file')\n",
            "                    sys.exit(1)\n",
            "                n += int(fields[1])\n",
            "                M.append({'lang': fields[0], 'n': n})\n",
            "        print(' - found {:d} languages:'.format(len(M)), end='')\n",
            "        for L in M:\n",
            "            print(' {:s}'.format(L['lang']), end='')\n",
            "        print('')\n",
            "\n",
            "    return txt_mmap, ref_mmap, nbw_mmap, M\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# Return the text for the given index\n",
            "\n",
            "def IndexTextQuery(txt_mmap, ref_mmap, idx):\n",
            "    p = int(ref_mmap[idx])  # get starting byte position\n",
            "    i = 0\n",
            "    dim = 10000  # max sentence length in bytes\n",
            "    b = bytearray(dim)\n",
            "    #  find EOL\n",
            "    while txt_mmap[p+i] != 10 and i < dim:\n",
            "        b[i] = txt_mmap[p+i]\n",
            "        i += 1\n",
            "\n",
            "    return b[0:i].decode('utf-8')\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# Search the [k] nearest vectors of [x] in the given index\n",
            "# and return the text lines\n",
            "\n",
            "def IndexSearchKNN(index, x, T, R, kmax=1, Dmax=1.0, dedup=True):\n",
            "    D, I = index.search(x, kmax)\n",
            "    prev = {}  # for depuplication\n",
            "    res = []\n",
            "    for n in range(x.shape[0]):\n",
            "        for i in range(kmax):\n",
            "            txt = IndexTextQuery(T, R, I[n, i])\n",
            "            if (dedup and txt not in prev) and D[n, i] <= Dmax:\n",
            "                prev[txt] = 1\n",
            "                res.append([txt, D[n, i]])\n",
            "    return res\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}