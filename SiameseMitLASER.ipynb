{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SiameseMitLASER",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinmay5/NLP-Praktikum/blob/master/SiameseMitLASER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sScywYm8EJhe",
        "colab_type": "code",
        "outputId": "90429e47-7bae-41dc-f3f2-df0ed4dc93cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqSjJNruoa8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUcKCcufznUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')\n",
        "# %cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bENmrfgZLusD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install torch==1.0.1 -f https://download.pytorch.org/whl/cu100/stable # CUDA 10.0 build"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFppkR1CAEg4",
        "colab_type": "code",
        "outputId": "a5db8c1e-b1ab-4019-da44-ee3a1aa510db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip3 install torchvision\n",
        "!pip install nltk\n",
        "!pip install tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.1.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtDA2b2VYMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#coding=utf-8\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "stops1 = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "def clean_sent(sent):\n",
        "    sent = sent.lower()\n",
        "    sent = re.sub(u'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]',' ',sent)\n",
        "    sent = re.sub('¡',' ',sent)\n",
        "    sent = re.sub('¿',' ',sent)\n",
        "    sent = re.sub('Á','á',sent)\n",
        "    sent = re.sub('Ó','ó',sent)\n",
        "    sent = re.sub('Ú','ú',sent)\n",
        "    sent = re.sub('É','é',sent)\n",
        "    sent = re.sub('Í','í',sent)\n",
        "    return sent\n",
        "  \n",
        "def cleanSpanish(df):\n",
        "    df['spanish1'] = df.spanish1.map(lambda x: ' '.join([ word for word in\n",
        "                                                         nltk.word_tokenize(clean_sent(x))]))\n",
        "    df['spanish2'] = df.spanish2.map(lambda x: ' '.join([ word for word in\n",
        "                                                         nltk.word_tokenize(clean_sent(x))]))\n",
        "    \n",
        "def removeSpanishStopWords(df, stop):\n",
        "\tdf['spanish1'] = df.spanish1.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x)\n",
        "                                                         if word not in stop]))\n",
        "\tdf['spanish2'] = df.spanish2.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x)\n",
        "                                                         if word not in stop]))\n",
        "\n",
        "\n",
        "def data_preprocessing():\n",
        "\n",
        "    # Training data\n",
        "    import os\n",
        "    os.chdir(\"/content/\")\n",
        "    !ls\n",
        "\n",
        "    df_train_en_sp = pd.read_csv('./cikm_english_train_20180516.txt', sep='\t', header=None,\n",
        "                                 error_bad_lines=False)\n",
        "    df_train_sp_en = pd.read_csv('./cikm_spanish_train_20180516.txt', sep='\t', header=None,\n",
        "                                 error_bad_lines=False)\n",
        "    df_train_en_sp.columns = ['english1', 'spanish1', 'english2', 'spanish2', 'result']\n",
        "    df_train_sp_en.columns = ['spanish1', 'english1', 'spanish2', 'english2', 'result']\n",
        "    train1 = pd.DataFrame(pd.concat([df_train_en_sp['spanish1'], df_train_sp_en['spanish1']], axis=0))\n",
        "    train2 = pd.DataFrame(pd.concat([df_train_en_sp['spanish2'], df_train_sp_en['spanish2']], axis=0))\n",
        "    train_data = pd.concat([train1, train2], axis=1).reset_index()\n",
        "    train_data = train_data.drop(['index'], axis=1)\n",
        "    result = pd.DataFrame(pd.concat([df_train_en_sp['result'], df_train_sp_en['result']], axis=0)).reset_index()\n",
        "    result = result.drop(['index'], axis=1)\n",
        "    # pd.get_dummies(result['result']).head()\n",
        "    train_data['result'] = result\n",
        "\n",
        "    # Evaluation data\n",
        "    test_data = pd.read_csv('./cikm_test_a_20180516.txt', sep='\t', header=None, error_bad_lines=False)\n",
        "    test_data.columns = ['spanish1', 'spanish2']\n",
        "\n",
        "\n",
        "    cleanSpanish(train_data)\n",
        "    removeSpanishStopWords(train_data, stops1)\n",
        "    cleanSpanish(test_data)\n",
        "    removeSpanishStopWords(test_data, stops1)\n",
        "\n",
        "    train_data.replace('', np.nan, inplace=True)\n",
        "    dirty_data = train_data[train_data.isnull().any(axis=1)]\n",
        "    print ('dirty sample count:', dirty_data.shape[0])\n",
        "    print ('positive dirty training sample:', len(dirty_data[dirty_data['result'] == 1]))\n",
        "    print ('negative dirty training sample:', len(dirty_data[dirty_data['result'] == 0]))\n",
        "\n",
        "    train_data = train_data.dropna()\n",
        "    test_data.replace('', np.nan, inplace=True)\n",
        "    test_data = test_data.dropna()\n",
        "    print ('Train sample count:', train_data.shape[0], 'Test sample count:', test_data.shape[0])\n",
        "\n",
        "    train_data.columns = ['s1', 's2', 'label']\n",
        "    test_data.columns = ['s1', 's2']\n",
        "\n",
        "    train_data.to_csv(\"cleaned_train.csv\", index=False)\n",
        "    test_data.to_csv(\"cleaned_test.csv\", index=False)\n",
        "\n",
        "def get_embedding(word_dict, embedding_path, embedding_dim=300):\n",
        "    # find existing word embeddings\n",
        "    word_vec = {}\n",
        "    with open(embedding_path) as f:\n",
        "        for line in f:\n",
        "            word, vec = line.split(' ', 1)\n",
        "            if word in word_dict:\n",
        "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
        "    print('Found {0}/{1} words with embedding vectors'.format(\n",
        "        len(word_vec), len(word_dict)))\n",
        "    missing_word_num = len(word_dict) - len(word_vec)\n",
        "    missing_ratio = round(float(missing_word_num) / len(word_dict), 4) * 100\n",
        "    print('Missing Ratio: {}%'.format(missing_ratio))\n",
        "\n",
        "    # handling unknown embeddings\n",
        "    for word in word_dict:\n",
        "        if word not in word_vec:\n",
        "            # If word not in word_vec, create a random embedding for it\n",
        "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "            word_vec[word] = new_embedding\n",
        "    print (\"Filled missing words' embeddings.\")\n",
        "    print (\"Embedding Matrix Size: \", len(word_vec))\n",
        "\n",
        "    return word_vec\n",
        "\n",
        "def save_embed(obj, path):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "    print ('Embedding saved')\n",
        "\n",
        "def load_embed(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkVYLPcL3iTA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWYKXO5gJPf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Some of the variables that are going to be treated as constants and used throughout the code\n",
        "\"\"\"\n",
        "\n",
        "data_preprocess = True # Would preprocess the data and generate the embeddings\n",
        "make_dict = True #  Would generate the embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZEEUAKwX2Uh",
        "colab_type": "code",
        "outputId": "6cdca1fd-0182-444a-f377-77c661c05eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "\"\"\" Data Preprocessing \"\"\"\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "os.chdir(\"/content/\")\n",
        "!ls\n",
        "\n",
        "with open('siamese-config.yaml') as f:\n",
        "  config = yaml.load(f)\n",
        "\n",
        "  \n",
        "if data_preprocess:\n",
        "    print ('Pre-processing Original Data ...')\n",
        "    data_preprocessing()\n",
        "    print ('Data Pre-processing Done!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cikm_english_train_20180516.txt  cleaned_train.csv    wiki.en.vec\n",
            "cikm_spanish_train_20180516.txt  embedding.pkl\t      wiki.es.vec\n",
            "cikm_test_a_20180516.txt\t sample_data\n",
            "cleaned_test.csv\t\t siamese-config.yaml\n",
            "Pre-processing Original Data ...\n",
            "cikm_english_train_20180516.txt  cleaned_train.csv    wiki.en.vec\n",
            "cikm_spanish_train_20180516.txt  embedding.pkl\t      wiki.es.vec\n",
            "cikm_test_a_20180516.txt\t sample_data\n",
            "cleaned_test.csv\t\t siamese-config.yaml\n",
            "dirty sample count: 73\n",
            "positive dirty training sample: 5\n",
            "negative dirty training sample: 68\n",
            "Train sample count: 21327 Test sample count: 4998\n",
            "Data Pre-processing Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C05Cgh29Lx-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.embed_size = config['model']['embed_size']\n",
        "        self.batch_size = config['model']['batch_size']\n",
        "        self.hidden_size = config['model']['encoder']['hidden_size']\n",
        "        self.num_layers = config['model']['encoder']['num_layers']\n",
        "        self.bidir = config['model']['encoder']['bidirectional']\n",
        "        if self.bidir:\n",
        "            self.direction = 2\n",
        "        else: self.direction = 1\n",
        "        self.dropout = config['model']['encoder']['dropout']\n",
        "\n",
        "        self.embedding = config['embedding_matrix']\n",
        "        self.lstm = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size, dropout=self.dropout,\n",
        "                            num_layers=self.num_layers, bidirectional=self.bidir)\n",
        "        self.lstm = self.lstm.cuda()\n",
        "\n",
        "    def initHiddenCell(self):\n",
        "        rand_hidden = Variable(torch.randn(self.direction * self.num_layers, self.batch_size, self.hidden_size))\n",
        "        rand_cell = Variable(torch.randn(self.direction * self.num_layers, self.batch_size, self.hidden_size))\n",
        "        rand_hidden = rand_hidden.cuda()\n",
        "        rand_cell = rand_cell.cuda()\n",
        "        return rand_hidden, rand_cell\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = self.embedding(input).view(1, 1, -1)\n",
        "        output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "        return output, hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egyATa-gLyBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Siamese_lstm(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Siamese_lstm, self).__init__()\n",
        "\n",
        "        self.encoder = LSTMEncoder(config)\n",
        "        self.fc_dim = config['model']['fc_dim']\n",
        "\n",
        "        self.input_dim = 5 * self.encoder.direction * self.encoder.hidden_size\n",
        "        # self.classifier = nn.Sequential(\n",
        "        #     nn.Linear(self.input_dim, self.fc_dim),\n",
        "        #     nn.Linear(self.fc_dim, 2)\n",
        "        # )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, int(self.input_dim/2)),\n",
        "            nn.Linear(int(self.input_dim/2), 2)\n",
        "        ).cuda()\n",
        "\n",
        "    def forward(self, s1, s2):\n",
        "\n",
        "        # init hidden, cell\n",
        "        h1, c1 = self.encoder.initHiddenCell()\n",
        "        h2, c2 = self.encoder.initHiddenCell()\n",
        "\n",
        "        # input one by one\n",
        "\n",
        "        for i in range(len(s1)):\n",
        "\n",
        "            v1, h1, c1 = self.encoder(s1[i], h1, c1)\n",
        "            \n",
        "        for j in range(len(s2)):\n",
        "            v2, h2, c2 = self.encoder(s2[j], h2, c2)\n",
        "            \n",
        "        # utilize these two encoded vectors\n",
        "        features = torch.cat((v1,torch.abs(v1 - v2),v2,v1*v2, (v1+v2)/2), 2)\n",
        "        # features = v1-v2\n",
        "        output = self.classifier(features)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6detYnDOBU9",
        "colab_type": "code",
        "outputId": "9f3f2137-105d-4b32-f8c6-3592111857c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "!cat siamese-config.yaml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "experiment_name: 'siamese-baseline'\n",
            "\n",
            "task: 'train'\n",
            "make_dict: False\n",
            "data_preprocessing: False\n",
            "\n",
            "ckpt_dir: 'ckpt/'\n",
            "\n",
            "training:\n",
            "    num_epochs: 20\n",
            "    learning_rate: 0.01\n",
            "    # options = ['adam', 'adadelta', 'rmsprop']\n",
            "    optimizer: 'sgd'\n",
            "\n",
            "\n",
            "embedding:\n",
            "    full_embedding_path: 'input/wiki.es.vec'\n",
            "    cur_embedding_path: 'input/embedding.pkl'\n",
            "\n",
            "model:\n",
            "    fc_dim: 100\n",
            "    name: 'siamese'\n",
            "    embed_size: 300\n",
            "    batch_size: 1\n",
            "    embedding_freeze: False\n",
            "    encoder:\n",
            "        hidden_size: 150\n",
            "        num_layers: 1\n",
            "        bidirectional: False\n",
            "        dropout: 0.0\n",
            "\n",
            "result:\n",
            "    filename: 'result.txt'\n",
            "    filepath: 'res/'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfZBaI7FO4lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class myDS(Dataset):\n",
        "\n",
        "    def __init__(self, df, all_sents):\n",
        "        # Assign vocabularies.\n",
        "        self.s1 = df['s1'].tolist()\n",
        "        self.s2 = df['s2'].tolist()\n",
        "        self.label = df['label'].tolist()\n",
        "        self.vocab = Vocab(all_sents, sos_token='<sos>', eos_token='<eos>', unk_token='<unk>')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Split sentence into words.\n",
        "        s1_words = self.s1[idx].split()\n",
        "        s2_words = self.s2[idx].split()\n",
        "\n",
        "        # Add <SOS> and <EOS> tokens.\n",
        "        s1_words = [self.vocab.sos_token] + s1_words + [self.vocab.eos_token]\n",
        "        s2_words = [self.vocab.sos_token] + s2_words + [self.vocab.eos_token]\n",
        "\n",
        "        # Lookup word ids in vocabularies.\n",
        "        s1_ids = [self.vocab.word2id(word) for word in s1_words]\n",
        "        s2_ids = [self.vocab.word2id(word) for word in s2_words]\n",
        "        label = self.label[idx]\n",
        "\n",
        "        return s1_ids, s2_ids, label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_dkKjxNO4q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class mytestDS(Dataset):\n",
        "\n",
        "    def __init__(self, df, all_sents):\n",
        "        # Assign vocabularies.\n",
        "        self.s1 = df['s1'].tolist()\n",
        "        self.s2 = df['s2'].tolist()\n",
        "        self.vocab = Vocab(all_sents, sos_token='<sos>', eos_token='<eos>', unk_token='<unk>')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.s1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Split sentence into words.\n",
        "        s1_words = self.s1[idx].split()\n",
        "        s2_words = self.s2[idx].split()\n",
        "\n",
        "        # Add <SOS> and <EOS> tokens.\n",
        "        s1_words = [self.vocab.sos_token] + s1_words + [self.vocab.eos_token]\n",
        "        s2_words = [self.vocab.sos_token] + s2_words + [self.vocab.eos_token]\n",
        "\n",
        "        # Lookup word ids in vocabularies.\n",
        "        s1_ids = [self.vocab.word2id(word) for word in s1_words]\n",
        "        s2_ids = [self.vocab.word2id(word) for word in s2_words]\n",
        "        \n",
        "        return s1_ids, s2_ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBD2uMOtO4od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab(object):\n",
        "    def __init__(self, all_sents, max_size=None, sos_token=None, eos_token=None, unk_token=None):\n",
        "        \"\"\"Initialize the vocabulary.\n",
        "        Args:\n",
        "            iter: An iterable which produces sequences of tokens used to update\n",
        "                the vocabulary.\n",
        "            max_size: (Optional) Maximum number of tokens in the vocabulary.\n",
        "            sos_token: (Optional) Token denoting the start of a sequence.\n",
        "            eos_token: (Optional) Token denoting the end of a sequence.\n",
        "            unk_token: (Optional) Token denoting an unknown element in a\n",
        "                sequence.\n",
        "        \"\"\"\n",
        "        self.max_size = max_size\n",
        "        self.pad_token = '<pad>'\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.unk_token = unk_token\n",
        "\n",
        "        # Add special tokens.\n",
        "        id2word = [self.pad_token]\n",
        "        if sos_token is not None:\n",
        "            id2word.append(self.sos_token)\n",
        "        if eos_token is not None:\n",
        "            id2word.append(self.eos_token)\n",
        "        if unk_token is not None:\n",
        "            id2word.append(self.unk_token)\n",
        "\n",
        "        # Update counter with token counts.\n",
        "        counter = Counter()\n",
        "        for x in all_sents:\n",
        "            counter.update(x.split())\n",
        "\n",
        "        # Extract lookup tables.\n",
        "        if max_size is not None:\n",
        "            counts = counter.most_common(max_size)\n",
        "        else:\n",
        "            counts = counter.items()\n",
        "            counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
        "        words = [x[0] for x in counts]\n",
        "        id2word.extend(words)\n",
        "        word2id = {x: i for i, x in enumerate(id2word)}\n",
        "\n",
        "        self._id2word = id2word\n",
        "        self._word2id = word2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._id2word)\n",
        "\n",
        "    def word2id(self, word):\n",
        "        \"\"\"Map a word in the vocabulary to its unique integer id.\n",
        "        Args:\n",
        "            word: Word to lookup.\n",
        "        Returns:\n",
        "            id: The integer id of the word being looked up.\n",
        "        \"\"\"\n",
        "        if word in self._word2id:\n",
        "            return self._word2id[word]\n",
        "        elif self.unk_token is not None:\n",
        "            return self._word2id[self.unk_token]\n",
        "        else:\n",
        "            raise KeyError('Word \"%s\" not in vocabulary.' % word)\n",
        "\n",
        "    def id2word(self, id):\n",
        "        \"\"\"Map an integer id to its corresponding word in the vocabulary.\n",
        "        Args:\n",
        "            id: Integer id of the word being looked up.\n",
        "        Returns:\n",
        "            word: The corresponding word.\n",
        "        \"\"\"\n",
        "        return self._id2word[id]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgowMsbBJPWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wiv2kjMRENYH",
        "colab_type": "code",
        "outputId": "4ea4a437-cb67-4afe-a131-5bcaec79b461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q09HF_6GX2b_",
        "colab_type": "code",
        "outputId": "eede4ac8-bce4-4c02-8b54-23d80dfc1c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\"\"\" Read Data \"\"\"\n",
        "\n",
        "train_data = pd.read_csv('cleaned_train.csv')\n",
        "test_data = pd.read_csv('cleaned_test.csv')\n",
        "\n",
        "# split dataset\n",
        "msk = np.random.rand(len(train_data)) < 0.8\n",
        "train = train_data[msk]\n",
        "valid = train_data[~msk]\n",
        "all_sents = train_data['s1'].tolist() + train_data['s2'].tolist() + test_data['s1'].tolist() + test_data['s2'].tolist()\n",
        "\n",
        "# dataset\n",
        "trainDS = myDS(train, all_sents)\n",
        "validDS = myDS(valid, all_sents)\n",
        "\n",
        "print ('Data size:',train_data.shape[0], test_data.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size: 21327 4998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8NPOcxrGdBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Get Embedding \"\"\"\n",
        "\n",
        "#full_embed_path = config['embedding']['full_embedding_path']\n",
        "#cur_embed_path = config['embedding']['cur_embedding_path']\n",
        "\n",
        "# TODO: Make sure we read it from the config file in the future\n",
        "full_embed_path = \"wiki.es.vec\"\n",
        "cur_embed_path = \"embedding.pkl\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxSsU6PmVVWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this line if the embeddings need to be loaded\n",
        "\n",
        "\n",
        "#!curl https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec > ./wiki.en.vec\n",
        "#!curl https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.es.vec > ./wiki.es.vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1kwrEk8YBqR",
        "colab_type": "code",
        "outputId": "ab414571-b0ad-4864-a3cd-867d5d843062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists(cur_embed_path) and not make_dict:\n",
        "    embed_dict = load_embed(cur_embed_path)\n",
        "    print ('Loaded existing embedding.')\n",
        "else:\n",
        "    print ('Making embedding...')\n",
        "    embed_dict = get_embedding(trainDS.vocab._id2word, full_embed_path)\n",
        "    save_embed(embed_dict,\"embedding.pkl\")\n",
        "    print ('Saved generated embedding.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Making embedding...\n",
            "Found 5141/5773 words with embedding vectors\n",
            "Missing Ratio: 10.95%\n",
            "Filled missing words' embeddings.\n",
            "Embedding Matrix Size:  5773\n",
            "Embedding saved\n",
            "Saved generated embedding.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0hY462lGO_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(embed_dict)\n",
        "# initialize nn embedding\n",
        "embedding = nn.Embedding(vocab_size, config['model']['embed_size'])\n",
        "embed_list = []\n",
        "for word in trainDS.vocab._id2word:\n",
        "    embed_list.append(embed_dict[word])\n",
        "weight_matrix = np.array(embed_list)\n",
        "# pass weights to nn embedding\n",
        "embedding.weight = nn.Parameter(torch.from_numpy(weight_matrix).type(torch.FloatTensor), requires_grad = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knAMkjneRumZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding\n",
        "config['embedding_matrix'] = embedding\n",
        "config['vocab_size'] = len(embed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoaBdfULRur3",
        "colab_type": "code",
        "outputId": "73ecdd4c-7b65-4e04-e5bf-11e3a1441889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# model\n",
        "siamese = Siamese_lstm(config)\n",
        "siamese.cuda()\n",
        "siamese.encoder.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMEncoder(\n",
              "  (embedding): Embedding(5773, 300)\n",
              "  (lstm): LSTM(300, 150)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQHsY8anYMs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(embed_dict)\n",
        "# initialize nn embedding\n",
        "embedding = nn.Embedding(vocab_size, config['model']['embed_size'])\n",
        "embed_list = []\n",
        "for word in trainDS.vocab._id2word:\n",
        "    embed_list.append(embed_dict[word])\n",
        "weight_matrix = np.array(embed_list)\n",
        "# pass weights to nn embedding\n",
        "embedding.weight = nn.Parameter(torch.from_numpy(weight_matrix).type(torch.FloatTensor), requires_grad = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SphmC-tGZeLP",
        "colab_type": "text"
      },
      "source": [
        "Code for preprocessing the text and other cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRmMu1-GYB1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTXhvxZHXRIL",
        "colab_type": "text"
      },
      "source": [
        "Setting up of the Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y_obpC3XPyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer\n",
        "learning_rate = config['training']['learning_rate']\n",
        "if config['training']['optimizer'] == 'sgd':\n",
        "    optimizer = torch.optim.SGD(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)\n",
        "elif config['training']['optimizer'] == 'adam':\n",
        "    optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)\n",
        "elif config['training']['optimizer'] == 'adadelta':\n",
        "    optimizer = torch.optim.Adadelta(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)\n",
        "elif config['training']['optimizer'] == 'rmsprop':\n",
        "    optimizer = torch.optim.RMSprop(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plIkyXb4XP4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss func\n",
        "loss_weights = Variable(torch.FloatTensor([1, 3]))\n",
        "if torch.cuda.is_available():\n",
        "    loss_weights = loss_weights.cuda()\n",
        "criterion = torch.nn.CrossEntropyLoss(loss_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFOSUSNwXPtH",
        "colab_type": "code",
        "outputId": "f1ac3007-4793-4f52-cb63-671c7889b035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Restore saved model (if one exists).\n",
        "ckpt_path = os.path.join('./', config['experiment_name']+'.pt')\n",
        "\n",
        "if os.path.exists(ckpt_path):\n",
        "    print('Loading checkpoint: %s' % ckpt_path)\n",
        "    ckpt = torch.load(ckpt_path)\n",
        "    epoch = ckpt['epoch']\n",
        "    siamese.load_state_dict(ckpt['siamese'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "else:\n",
        "    epoch = 0\n",
        "    print ('Fresh start!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fresh start!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqeYI35WFi74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# log info\n",
        "train_log_string = '%s :: Epoch %i :: Iter %i / %i :: train loss: %0.4f'\n",
        "valid_log_string = '%s :: Epoch %i :: valid loss: %0.4f\\n'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9R6hGfqVJh_",
        "colab_type": "text"
      },
      "source": [
        "## Training Procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Ua1mQOVI0t",
        "colab_type": "code",
        "outputId": "9e5a4d91-25a5-4a4b-cd6d-b19e98ef8ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2465
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "# save every epoch for visualization\n",
        "train_loss_record = []\n",
        "valid_loss_record = []\n",
        "best_record = 10.0\n",
        "\n",
        "# training\n",
        "print ('Experiment:{}\\n'.format(config['experiment_name']))\n",
        "\n",
        "    \n",
        "while (epoch < config['training']['num_epochs']):\n",
        "\n",
        "    print ('Start Epoch{} Training...'.format(epoch))\n",
        "\n",
        "    # loss\n",
        "    train_loss = []\n",
        "    train_loss_sum = []\n",
        "    # dataloader\n",
        "    train_dataloader = DataLoader(dataset=trainDS, shuffle=True, num_workers=2, batch_size=1)\n",
        "\n",
        "    for idx, data in enumerate(train_dataloader, 0):\n",
        "\n",
        "        # get data\n",
        "        s1, s2, label = data\n",
        "        \n",
        "        # putting the data into cuda\n",
        "        s1 = torch.from_numpy(np.array(s1))\n",
        "        s2 = torch.from_numpy(np.array(s2))\n",
        "        label = torch.from_numpy(np.array(label))\n",
        "        \n",
        "        s1 = s1.cuda()\n",
        "        s2 = s2.cuda()\n",
        "        label = label.cuda()\n",
        "        \n",
        "        # clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        \n",
        "        # input\n",
        "        output = siamese(s1, s2)\n",
        "        output = output.squeeze(0)\n",
        "\n",
        "        # loss backward\n",
        "        loss = criterion(output, Variable(label))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss.data.cpu())\n",
        "        train_loss_sum.append(loss.data.cpu())\n",
        "\n",
        "        # Every once and a while check on the loss\n",
        "        if ((idx + 1) % 5000) == 0:\n",
        "            print(train_log_string % (datetime.now(), epoch, idx + 1, len(train), np.mean(train_loss)))\n",
        "            train_loss = []\n",
        "\n",
        "    # Record at every epoch\n",
        "    print ('Train Loss at epoch{}: {}\\n'.format(epoch, np.mean(train_loss_sum)))\n",
        "    train_loss_record.append(np.mean(train_loss_sum))\n",
        "\n",
        "    # Valid\n",
        "    print ('Epoch{} Validating...'.format(epoch))\n",
        "\n",
        "    # loss\n",
        "    valid_loss = []\n",
        "    # dataloader\n",
        "    valid_dataloader = DataLoader(dataset=validDS, shuffle=True, num_workers=2, batch_size=1)\n",
        "\n",
        "    for idx, data in enumerate(valid_dataloader, 0):\n",
        "        # get data\n",
        "        s1, s2, label = data\n",
        "\n",
        "        # putting the data into cuda\n",
        "        s1 = torch.from_numpy(np.array(s1))\n",
        "        s2 = torch.from_numpy(np.array(s2))\n",
        "        label = torch.from_numpy(np.array(label))\n",
        "        \n",
        "        s1 = s1.cuda()\n",
        "        s2 = s2.cuda()\n",
        "        label = label.cuda()\n",
        "        \n",
        "        # input\n",
        "        output = siamese(s1, s2)\n",
        "        output = output.squeeze(0)\n",
        "\n",
        "        # loss\n",
        "        loss = criterion(output, Variable(label))\n",
        "        valid_loss.append(loss.data.cpu())\n",
        "\n",
        "    print(valid_log_string % (datetime.now(), epoch, np.mean(valid_loss)))\n",
        "    # Record\n",
        "    valid_loss_record.append(np.mean(valid_loss))\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "    # Keep track of best record\n",
        "    if np.mean(valid_loss) < best_record:\n",
        "        best_record = np.mean(valid_loss)\n",
        "        # save the best model\n",
        "        state_dict = {\n",
        "            'epoch': epoch,\n",
        "            'siamese': siamese.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }\n",
        "        torch.save(state_dict, ckpt_path)\n",
        "        print ('Model saved!\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment:siamese-baseline\n",
            "\n",
            "Start Epoch3 Training...\n",
            "2019-05-21 22:27:26.995351 :: Epoch 3 :: Iter 5000 / 17013 :: train loss: 0.3915\n",
            "2019-05-21 22:28:47.501385 :: Epoch 3 :: Iter 10000 / 17013 :: train loss: 0.4048\n",
            "2019-05-21 22:30:11.273321 :: Epoch 3 :: Iter 15000 / 17013 :: train loss: 0.3719\n",
            "Train Loss at epoch3: 0.3892628252506256\n",
            "\n",
            "Epoch3 Validating...\n",
            "2019-05-21 22:31:30.875104 :: Epoch 3 :: valid loss: 0.3873\n",
            "\n",
            "Model saved!\n",
            "\n",
            "Start Epoch4 Training...\n",
            "2019-05-21 22:32:52.124570 :: Epoch 4 :: Iter 5000 / 17013 :: train loss: 0.3404\n",
            "2019-05-21 22:34:13.403940 :: Epoch 4 :: Iter 10000 / 17013 :: train loss: 0.3437\n",
            "2019-05-21 22:35:35.186432 :: Epoch 4 :: Iter 15000 / 17013 :: train loss: 0.3472\n",
            "Train Loss at epoch4: 0.34338438510894775\n",
            "\n",
            "Epoch4 Validating...\n",
            "2019-05-21 22:36:54.765014 :: Epoch 4 :: valid loss: 0.3658\n",
            "\n",
            "Model saved!\n",
            "\n",
            "Start Epoch5 Training...\n",
            "2019-05-21 22:38:15.622470 :: Epoch 5 :: Iter 5000 / 17013 :: train loss: 0.3026\n",
            "2019-05-21 22:39:37.149971 :: Epoch 5 :: Iter 10000 / 17013 :: train loss: 0.3044\n",
            "2019-05-21 22:40:59.543065 :: Epoch 5 :: Iter 15000 / 17013 :: train loss: 0.2995\n",
            "Train Loss at epoch5: 0.30207836627960205\n",
            "\n",
            "Epoch5 Validating...\n",
            "2019-05-21 22:42:19.070706 :: Epoch 5 :: valid loss: 0.3708\n",
            "\n",
            "Start Epoch6 Training...\n",
            "2019-05-21 22:43:39.453905 :: Epoch 6 :: Iter 5000 / 17013 :: train loss: 0.2656\n",
            "2019-05-21 22:45:00.235498 :: Epoch 6 :: Iter 10000 / 17013 :: train loss: 0.2559\n",
            "2019-05-21 22:46:21.893381 :: Epoch 6 :: Iter 15000 / 17013 :: train loss: 0.2617\n",
            "Train Loss at epoch6: 0.26249587535858154\n",
            "\n",
            "Epoch6 Validating...\n",
            "2019-05-21 22:47:40.839431 :: Epoch 6 :: valid loss: 0.3710\n",
            "\n",
            "Start Epoch7 Training...\n",
            "2019-05-21 22:49:01.295399 :: Epoch 7 :: Iter 5000 / 17013 :: train loss: 0.2071\n",
            "2019-05-21 22:50:21.693219 :: Epoch 7 :: Iter 10000 / 17013 :: train loss: 0.2336\n",
            "2019-05-21 22:51:42.306404 :: Epoch 7 :: Iter 15000 / 17013 :: train loss: 0.2409\n",
            "Train Loss at epoch7: 0.22568874061107635\n",
            "\n",
            "Epoch7 Validating...\n",
            "2019-05-21 22:53:00.329146 :: Epoch 7 :: valid loss: 0.4011\n",
            "\n",
            "Start Epoch8 Training...\n",
            "2019-05-21 22:54:19.119699 :: Epoch 8 :: Iter 5000 / 17013 :: train loss: 0.1892\n",
            "2019-05-21 22:55:38.455321 :: Epoch 8 :: Iter 10000 / 17013 :: train loss: 0.1910\n",
            "2019-05-21 22:56:58.760145 :: Epoch 8 :: Iter 15000 / 17013 :: train loss: 0.1998\n",
            "Train Loss at epoch8: 0.1960529237985611\n",
            "\n",
            "Epoch8 Validating...\n",
            "2019-05-21 22:58:15.878047 :: Epoch 8 :: valid loss: 0.3801\n",
            "\n",
            "Start Epoch9 Training...\n",
            "2019-05-21 22:59:34.371517 :: Epoch 9 :: Iter 5000 / 17013 :: train loss: 0.1583\n",
            "2019-05-21 23:00:54.094949 :: Epoch 9 :: Iter 10000 / 17013 :: train loss: 0.1601\n",
            "2019-05-21 23:02:12.508282 :: Epoch 9 :: Iter 15000 / 17013 :: train loss: 0.1763\n",
            "Train Loss at epoch9: 0.16643072664737701\n",
            "\n",
            "Epoch9 Validating...\n",
            "2019-05-21 23:03:28.998171 :: Epoch 9 :: valid loss: 0.4001\n",
            "\n",
            "Start Epoch10 Training...\n",
            "2019-05-21 23:04:48.428865 :: Epoch 10 :: Iter 5000 / 17013 :: train loss: 0.1125\n",
            "2019-05-21 23:06:06.187788 :: Epoch 10 :: Iter 10000 / 17013 :: train loss: 0.1490\n",
            "2019-05-21 23:07:24.878376 :: Epoch 10 :: Iter 15000 / 17013 :: train loss: 0.1442\n",
            "Train Loss at epoch10: 0.13759568333625793\n",
            "\n",
            "Epoch10 Validating...\n",
            "2019-05-21 23:08:41.578418 :: Epoch 10 :: valid loss: 0.4468\n",
            "\n",
            "Start Epoch11 Training...\n",
            "2019-05-21 23:09:59.687554 :: Epoch 11 :: Iter 5000 / 17013 :: train loss: 0.1071\n",
            "2019-05-21 23:11:16.624119 :: Epoch 11 :: Iter 10000 / 17013 :: train loss: 0.1124\n",
            "2019-05-21 23:12:33.356165 :: Epoch 11 :: Iter 15000 / 17013 :: train loss: 0.1321\n",
            "Train Loss at epoch11: 0.1182505190372467\n",
            "\n",
            "Epoch11 Validating...\n",
            "2019-05-21 23:13:49.492473 :: Epoch 11 :: valid loss: 0.4469\n",
            "\n",
            "Start Epoch12 Training...\n",
            "2019-05-21 23:15:08.352013 :: Epoch 12 :: Iter 5000 / 17013 :: train loss: 0.0857\n",
            "2019-05-21 23:16:26.771588 :: Epoch 12 :: Iter 10000 / 17013 :: train loss: 0.1056\n",
            "2019-05-21 23:17:44.872561 :: Epoch 12 :: Iter 15000 / 17013 :: train loss: 0.1006\n",
            "Train Loss at epoch12: 0.10035441815853119\n",
            "\n",
            "Epoch12 Validating...\n",
            "2019-05-21 23:19:01.312102 :: Epoch 12 :: valid loss: 0.5057\n",
            "\n",
            "Start Epoch13 Training...\n",
            "2019-05-21 23:20:19.715872 :: Epoch 13 :: Iter 5000 / 17013 :: train loss: 0.0704\n",
            "2019-05-21 23:21:38.338429 :: Epoch 13 :: Iter 10000 / 17013 :: train loss: 0.0894\n",
            "2019-05-21 23:22:56.419657 :: Epoch 13 :: Iter 15000 / 17013 :: train loss: 0.0763\n",
            "Train Loss at epoch13: 0.083287812769413\n",
            "\n",
            "Epoch13 Validating...\n",
            "2019-05-21 23:24:12.591097 :: Epoch 13 :: valid loss: 0.4954\n",
            "\n",
            "Start Epoch14 Training...\n",
            "2019-05-21 23:25:30.594853 :: Epoch 14 :: Iter 5000 / 17013 :: train loss: 0.0645\n",
            "2019-05-21 23:26:49.055352 :: Epoch 14 :: Iter 10000 / 17013 :: train loss: 0.0737\n",
            "2019-05-21 23:28:06.590857 :: Epoch 14 :: Iter 15000 / 17013 :: train loss: 0.0766\n",
            "Train Loss at epoch14: 0.0723523274064064\n",
            "\n",
            "Epoch14 Validating...\n",
            "2019-05-21 23:29:22.522570 :: Epoch 14 :: valid loss: 0.5376\n",
            "\n",
            "Start Epoch15 Training...\n",
            "2019-05-21 23:30:40.224660 :: Epoch 15 :: Iter 5000 / 17013 :: train loss: 0.0572\n",
            "2019-05-21 23:31:58.923758 :: Epoch 15 :: Iter 10000 / 17013 :: train loss: 0.0604\n",
            "2019-05-21 23:33:17.094919 :: Epoch 15 :: Iter 15000 / 17013 :: train loss: 0.0752\n",
            "Train Loss at epoch15: 0.06663064658641815\n",
            "\n",
            "Epoch15 Validating...\n",
            "2019-05-21 23:34:33.251234 :: Epoch 15 :: valid loss: 0.5427\n",
            "\n",
            "Start Epoch16 Training...\n",
            "2019-05-21 23:35:51.166357 :: Epoch 16 :: Iter 5000 / 17013 :: train loss: 0.0520\n",
            "2019-05-21 23:37:09.817456 :: Epoch 16 :: Iter 10000 / 17013 :: train loss: 0.0598\n",
            "2019-05-21 23:38:27.546974 :: Epoch 16 :: Iter 15000 / 17013 :: train loss: 0.0596\n",
            "Train Loss at epoch16: 0.05654129013419151\n",
            "\n",
            "Epoch16 Validating...\n",
            "2019-05-21 23:39:43.913475 :: Epoch 16 :: valid loss: 0.5851\n",
            "\n",
            "Start Epoch17 Training...\n",
            "2019-05-21 23:41:02.353307 :: Epoch 17 :: Iter 5000 / 17013 :: train loss: 0.0418\n",
            "2019-05-21 23:42:21.366902 :: Epoch 17 :: Iter 10000 / 17013 :: train loss: 0.0466\n",
            "2019-05-21 23:43:38.549071 :: Epoch 17 :: Iter 15000 / 17013 :: train loss: 0.0590\n",
            "Train Loss at epoch17: 0.048996467143297195\n",
            "\n",
            "Epoch17 Validating...\n",
            "2019-05-21 23:44:54.937262 :: Epoch 17 :: valid loss: 0.6124\n",
            "\n",
            "Start Epoch18 Training...\n",
            "2019-05-21 23:46:13.781334 :: Epoch 18 :: Iter 5000 / 17013 :: train loss: 0.0302\n",
            "2019-05-21 23:47:30.893147 :: Epoch 18 :: Iter 10000 / 17013 :: train loss: 0.0475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jACju1PFOBXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1s2hwjaWi_g",
        "colab_type": "text"
      },
      "source": [
        "Include Torch vision in here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0-2kOgMOBdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "\n",
        "plt.plot(train_loss_record)\n",
        "plt.plot(valid_loss_record)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltprn13EOBaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\" Inference \"\"\"\n",
        "# if config['taks'] == 'inference':\n",
        "testDS = mytestDS(test_data, all_sents)\n",
        "# Do not shuffle here\n",
        "test_dataloader = DataLoader(dataset=testDS, num_workers=2, batch_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXq8_NtIWcqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = []\n",
        "for idx, data in enumerate(test_dataloader, 0):\n",
        "\n",
        "    # get data\n",
        "    s1, s2 = data\n",
        "\n",
        "    # input\n",
        "    output = siamese(s1,s2)\n",
        "    output = output.squeeze(0)\n",
        "\n",
        "    # feed output into softmax to get prob prediction\n",
        "    sm = nn.Softmax(dim=1)\n",
        "    res = sm(output.data)[:,1]\n",
        "    result += res.data.tolist()\n",
        "\n",
        "result = pd.DataFrame(result)\n",
        "print 'Inference Done.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcfTeWUVpiK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6BpPi7aWctf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res_path = os.path.join(config['result']['filepath'], config['result']['filename'])\n",
        "result.to_csv(res_path,header=False,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3FwhbOyhwHA",
        "colab_type": "text"
      },
      "source": [
        "##Extra Section that will come in handy later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gphZG0tjZLhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "   \n",
        "    def __init__(self, margin):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.eps = 1e-9\n",
        "\n",
        "    def forward(self, output1, output2, target, size_average=True):\n",
        "        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n",
        "        losses = 0.5 * (target.float() * distances +\n",
        "                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2)) #ReLU function does the same task as selecting max\n",
        "        if size_average:\n",
        "          return losses.mean() \n",
        "        return losses.sum()\n",
        "\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplet loss\n",
        "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative, size_average=True):\n",
        "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
        "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
        "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
        "        if size_average:\n",
        "          return losses.mean()\n",
        "        return losses.sum()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApJFvPn_h8P6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OnlineContrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The pair selector and embeddings are sent\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin, pair_selector):\n",
        "        super(OnlineContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.pair_selector = pair_selector\n",
        "\n",
        "    def forward(self, embeddings, target):\n",
        "        positive_pairs, negative_pairs = self.pair_selector.get_pairs(embeddings, target)\n",
        "        if embeddings.is_cuda:\n",
        "            positive_pairs = positive_pairs.cuda()\n",
        "            negative_pairs = negative_pairs.cuda()\n",
        "        positive_loss = (embeddings[positive_pairs[:, 0]] - embeddings[positive_pairs[:, 1]]).pow(2).sum(1)\n",
        "        negative_loss = F.relu(\n",
        "            self.margin - (embeddings[negative_pairs[:, 0]] - embeddings[negative_pairs[:, 1]]).pow(2).sum(\n",
        "                1).sqrt()).pow(2)\n",
        "        loss = torch.cat([positive_loss, negative_loss], dim=0) #dim 0 is the rows\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class OnlineTripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplet selector and the embeddings are sent\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin, triplet_selector):\n",
        "        super(OnlineTripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.triplet_selector = triplet_selector\n",
        "\n",
        "    def forward(self, embeddings, target):\n",
        "\n",
        "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
        "\n",
        "        if embeddings.is_cuda:\n",
        "            triplets = triplets.cuda()\n",
        "\n",
        "        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n",
        "        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n",
        "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
        "\n",
        "        return losses.mean(), len(triplets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3AupsAslLAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TripletDataSet(Dataset):\n",
        "    \"\"\"\n",
        "    Train: For each sample (anchor) randomly chooses a positive and negative samples\n",
        "    Test: Creates fixed triplets for testing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_df):\n",
        "        self.train_df = train_df\n",
        "        self.arg1 = train_df['s1']\n",
        "        self.arg2 = train_df['s2']\n",
        "\n",
        "        \n",
        "        random_state = np.random.RandomState(29)\n",
        "\n",
        "        triplets = [[i,\n",
        "                     random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
        "                     random_state.choice(self.label_to_indices[\n",
        "                                             np.random.choice(\n",
        "                                                 list(self.labels_set - set([self.test_labels[i].item()]))\n",
        "                                             )\n",
        "                                         ])\n",
        "                     ]\n",
        "                    for i in range(len(self.train_df))]\n",
        "        self.test_triplets = triplets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Split sentence into words.\n",
        "        s1_words = self.arg1[idx].split()\n",
        "        s2_words = self.arg2[idx].split()\n",
        "\n",
        "        # Add <SOS> and <EOS> tokens.\n",
        "        s1_words = [self.vocab.sos_token] + s1_words + [self.vocab.eos_token]\n",
        "        s2_words = [self.vocab.sos_token] + s2_words + [self.vocab.eos_token]\n",
        "\n",
        "        # Lookup word ids in vocabularies.\n",
        "        s1_ids = [self.vocab.word2id(word) for word in s1_words]\n",
        "        s2_ids = [self.vocab.word2id(word) for word in s2_words]\n",
        "        label = self.label[idx]\n",
        "\n",
        "        return s1_ids, s2_ids, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wUDuuzC3knQ",
        "colab_type": "text"
      },
      "source": [
        "# This is an attempt to include the LASER multi lingual embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mngRKDEg3x2h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2d8b32fa-d86f-44d1-f93e-ba6dabef6942"
      },
      "source": [
        "!git clone https://github.com/ceshine/LASER.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'LASER'...\n",
            "remote: Enumerating objects: 504, done.\u001b[K\n",
            "remote: Total 504 (delta 0), reused 0 (delta 0), pack-reused 504\u001b[K\n",
            "Receiving objects: 100% (504/504), 2.56 MiB | 13.72 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_8LAr8c6Wyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf \\\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Akm1QeP4lea",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c425f61e-268a-439a-f141-cfffd2e2f95e"
      },
      "source": [
        "%env LASER=/content/LASER\n",
        "!echo $LASER\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: LASER=/content/LASER\n",
            "/content/LASER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo67QGwc7m7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2057ea17-1d95-40a9-ffa8-80892f5e66dc"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LASER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KxeDCC74lhZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "560dd8dc-ae68-4ac2-eae7-37674aaa3fe8"
      },
      "source": [
        "%cd LASER"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'LASER'\n",
            "/content/LASER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjFh5jFf4lmg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a8cf7e0-bc50-48dd-c86e-22bec79cdff9"
      },
      "source": [
        "! pwd"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LASER/tasks/similarity\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7ADRwFZ4lp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d0e7a029-5d12-466a-cc25-9eceb86d8947"
      },
      "source": [
        "% cd ../../\n",
        "!bash install_models.sh"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LASER\n",
            "Downloading networks\n",
            " - creating directory /content/LASER/models\n",
            " - bilstm.eparl21.2018-11-19.pt\n",
            " - eparl21.fcodes\n",
            " - eparl21.fvocab\n",
            " - bilstm.93langs.2018-12-26.pt\n",
            " - 93langs.fcodes\n",
            " - 93langs.fvocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5F_sNpe4lkm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        },
        "outputId": "32e69c89-6d5c-4f8c-c9fd-025e4e8b3f31"
      },
      "source": [
        "!bash install_external_tools.sh"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing external tools\n",
            " - creating directory /content/LASER/tools-external/moses-tokenizer/tokenizer\n",
            " - download tokenizer/tokenizer.perl\n",
            " - download tokenizer/detokenizer.perl\n",
            " - download tokenizer/normalize-punctuation.perl\n",
            " - download tokenizer/remove-non-printing-char.perl\n",
            " - download tokenizer/deescape-special-chars.perl\n",
            " - download tokenizer/lowercase.perl\n",
            " - download tokenizer/basic-protected-patterns\n",
            " - creating directory /content/LASER/tools-external/moses-tokenizer/share/nonbreaking_prefixes\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ca\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.cs\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.de\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.el\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.en\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.es\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.fi\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.fr\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ga\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.hu\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.is\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.it\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.lt\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.lv\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.nl\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.pl\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.pt\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ro\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ru\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.sk\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.sl\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.sv\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.ta\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.yue\n",
            " - download share/nonbreaking_prefixes/nonbreaking_prefix.zh\n",
            " - download fastBPE software from github\n",
            "--2019-05-24 13:01:45--  https://github.com/glample/fastBPE/archive/master.zip\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/glample/fastBPE/zip/master [following]\n",
            "--2019-05-24 13:01:46--  https://codeload.github.com/glample/fastBPE/zip/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 192.30.253.121\n",
            "Connecting to codeload.github.com (codeload.github.com)|192.30.253.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [ <=>                ]   8.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-05-24 13:01:46 (97.2 MB/s) - ‘master.zip’ saved [9091]\n",
            "\n",
            "Archive:  master.zip\n",
            "87eeb4dc5dd1542c2346d18b35aad6942bd04c6f\n",
            "   creating: fastBPE-master/\n",
            "  inflating: fastBPE-master/LICENSE  \n",
            "  inflating: fastBPE-master/README.md  \n",
            "   creating: fastBPE-master/fastBPE/\n",
            "  inflating: fastBPE-master/fastBPE/fastBPE.hpp  \n",
            "  inflating: fastBPE-master/fastBPE/fastBPE.pyx  \n",
            "  inflating: fastBPE-master/fastBPE/main.cc  \n",
            "  inflating: fastBPE-master/setup.py  \n",
            " - compiling\n",
            "\u001b[01m\u001b[Kg++:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kfast.cc: No such file or directory\n",
            "\u001b[01m\u001b[Kg++:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kno input files\n",
            "compilation terminated.\n",
            "ERROR: compilation failed, please install manually\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq9RVwqi8pyM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25957a8a-40c8-4651-a5c3-963550a3480f"
      },
      "source": [
        "# %cd tools-external/fastBPE/\n",
        "# ! g++ -std=c++11 -pthread -O3 fastBPE/main.cc -IfastBPE -o fast"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LASER/tools-external/fastBPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpraUBs38jZO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ea499975-9f36-4972-d75f-94a23f3375e3"
      },
      "source": [
        "%cd ../../tasks/similarity/\n",
        "! ls"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LASER/tasks/similarity\n",
            "README.md  wmt.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVjP99krE02v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "a801623a-c3b8-47f4-c785-2a7ea5dbd28e"
      },
      "source": [
        "! apt install libopenblas-base libomp-dev"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopenblas-base is already the newest version (0.2.20+ds-4).\n",
            "libopenblas-base set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libomp-doc\n",
            "The following NEW packages will be installed:\n",
            "  libomp-dev libomp5\n",
            "0 upgraded, 2 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 239 kB of archives.\n",
            "After this operation, 804 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp-dev amd64 5.0.1-1 [5,088 B]\n",
            "Fetched 239 kB in 1s (310 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 130911 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Selecting previously unselected package libomp-dev.\n",
            "Preparing to unpack .../libomp-dev_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp-dev (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libomp-dev (5.0.1-1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdgEtRK5AN9g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9f6f773f-8ed6-46dd-d79e-81c43152c015"
      },
      "source": [
        "! pip install faiss"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from faiss) (1.16.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBPIo9wj9GSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "d10005ab-2657-4aff-ee2a-86ce40a160a4"
      },
      "source": [
        "% cd ./tasks/similarity\n",
        "! bash wmt.sh"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LASER/tasks/similarity\n",
            "LASER: similarity search\n",
            "\n",
            "Processing:\n",
            " - loading encoder /content/LASER/models/bilstm.93langs.2018-12-26.pt\n",
            " - creating directory embed\n",
            " - Tokenizer: newstest2012.cs in language cs  \n",
            " - fast BPE: processing newstest2012.tok.cs\n",
            " - Encoder: newstest2012.bpe.cs to newstest2012.enc.cs\n",
            " - Encoder: 3003 sentences in 3s\n",
            " - embedding: ./embed/newstest2012.enc.cs 3003 examples of dim 1024\n",
            " - creating FAISS index\n",
            " - Tokenizer: newstest2012.de in language de  \n",
            " - fast BPE: processing newstest2012.tok.de\n",
            " - Encoder: newstest2012.bpe.de to newstest2012.enc.de\n",
            " - Encoder: 3003 sentences in 3s\n",
            " - embedding: ./embed/newstest2012.enc.de 3003 examples of dim 1024\n",
            " - creating FAISS index\n",
            " - Tokenizer: newstest2012.en in language en  \n",
            " - fast BPE: processing newstest2012.tok.en\n",
            " - Encoder: newstest2012.bpe.en to newstest2012.enc.en\n",
            " - Encoder: 3003 sentences in 2s\n",
            " - embedding: ./embed/newstest2012.enc.en 3003 examples of dim 1024\n",
            " - creating FAISS index\n",
            " - Tokenizer: newstest2012.es in language es  \n",
            " - fast BPE: processing newstest2012.tok.es\n",
            " - Encoder: newstest2012.bpe.es to newstest2012.enc.es\n",
            " - Encoder: 3003 sentences in 3s\n",
            " - embedding: ./embed/newstest2012.enc.es 3003 examples of dim 1024\n",
            " - creating FAISS index\n",
            " - Tokenizer: newstest2012.fr in language fr  \n",
            " - fast BPE: processing newstest2012.tok.fr\n",
            " - Encoder: newstest2012.bpe.fr to newstest2012.enc.fr\n",
            " - Encoder: 3003 sentences in 3s\n",
            " - embedding: ./embed/newstest2012.enc.fr 3003 examples of dim 1024\n",
            " - creating FAISS index\n",
            "Confusion matrix:\n",
            "langs   cs       de       en       es       fr       avg     \n",
            "cs     0.00%    0.70%    0.90%    0.67%    0.77%    0.76%\n",
            "de     0.83%    0.00%    1.17%    0.93%    1.03%    0.99%\n",
            "en     0.93%    1.27%    0.00%    0.83%    1.07%    1.02%\n",
            "es     0.53%    0.77%    0.97%    0.00%    0.57%    0.71%\n",
            "fr     0.50%    0.90%    1.13%    0.60%    0.00%    0.78%\n",
            "avg    0.70%    0.91%    1.04%    0.76%    0.86%    1.07%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sS0FP2kG-cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample feasibility check\n",
        "# % cd ../../source/\n",
        "\n",
        "import os\n",
        "import sys\n",
        "LASER = os.environ['LASER']\n",
        "# now include the extra files in the source\n",
        "sys.path.append(LASER + '/source')\n",
        "sys.path.append(LASER + '/source/lib')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39VFH-l3G-l4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from embed import SentenceEncoder, EncodeLoad, EncodeFile\n",
        "from text_processing import Token, BPEfastApply\n",
        "from indexing import IndexCreate, IndexSearchMultiple, IndexPrintConfusionMatrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5s572DPJmIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a3777f7-ca76-499f-a0d6-0f0186c7f898"
      },
      "source": [
        "DATA_PATH = LASER + '/data/tatoeba/v1/'\n",
        "#os.mkdir(LASER+'/embed')\n",
        "CACHE_PATH = LASER+ '/embed/'\n",
        "Token(\n",
        "    str(DATA_PATH + \"tatoeba.deu-eng.eng\"),\n",
        "    str(CACHE_PATH + \"tatoeba.deu-eng.eng\"),\n",
        "    lang=\"zh\",\n",
        "    romanize=False,\n",
        "    lower_case=True, gzip=False,\n",
        "    verbose=True, over_write=False)\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Tokenizer: tatoeba.deu-eng.eng in language zh  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzi20yUrJmF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8723289-e1d7-45ca-872f-2f7f60a008f4"
      },
      "source": [
        "# Now the byte pair-encoding part\n",
        "MODEL_PATH = LASER+ '/models/'\n",
        "bpe_codes = str(MODEL_PATH + \"93langs.fcodes\")\n",
        "BPEfastApply(\n",
        "    str(CACHE_PATH + \"tatoeba.deu-eng.eng\"),\n",
        "    str(CACHE_PATH + \"tatoeba.deu-eng.eng.bpe\"),\n",
        "    bpe_codes,\n",
        "    verbose=True, over_write=False)\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - fast BPE: processing tatoeba.deu-eng.eng\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcJKEx1FJmDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = SentenceEncoder(\n",
        "    str(MODEL_PATH + \"bilstm.93langs.2018-12-26.pt\"),\n",
        "    max_sentences=None,\n",
        "    max_tokens=10000,\n",
        "    cpu=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgx5WAVOJmA5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "be18c3d8-27f7-493c-f63f-db465ddd653b"
      },
      "source": [
        "print(encoder.encoder)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder(\n",
            "  (embed_tokens): Embedding(73640, 320, padding_idx=1)\n",
            "  (lstm): LSTM(320, 512, num_layers=5, bidirectional=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkEZU9jXJl-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = \"A quick brown fox jumps over the lazy dog\"\n",
        "encoded_sentence_1 = encoder.encode_sentences(sentence)\n",
        "encoded_sentence_2 = encoder.encode_sentences(sentence+\"haula haula baula baula laula laula\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OaaEDwvY-Qt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "16db412c-142c-4af4-a53a-b8615bfbc3e6"
      },
      "source": [
        "EncodeFile(\n",
        "    encoder,\n",
        "    str(CACHE_PATH + \"tatoeba.deu-eng.eng.bpe\"),\n",
        "    str(CACHE_PATH + \"tatoeba.deu-eng.eng.enc\"),\n",
        "    verbose=True, over_write=False)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Encoder: tatoeba.deu-eng.eng.bpe to tatoeba.deu-eng.eng.enc\n",
            " - Encoder: 1000 sentences in 0s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocvE3l_OUTtY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4a964477-df49-47c4-af39-49f01193560a"
      },
      "source": [
        "from sklearn import metrics\n",
        "similarity = metrics.pairwise.cosine_similarity(encoded_sentence_1, encoded_sentence_2)\n",
        "print(encoded_sentence_1.shape)\n",
        "print(encoded_sentence_2.shape)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(41, 1024)\n",
            "(76, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv2_ix8NWCCp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bff798ca-2312-4e07-c34f-84a0552d385a"
      },
      "source": [
        "data_en, index_en = IndexCreate(\n",
        "   CACHE_PATH + \"tatoeba.deu-eng.eng.enc\" , 'FlatL2', verbose=True, save_index=False)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - embedding: /content/LASER/embed/tatoeba.deu-eng.eng.enc 1000 examples of dim 1024\n",
            " - creating FAISS index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxfIH7nkg-PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = []\n",
        "all_index = []\n",
        "all_data.append(data_en)\n",
        "all_index.append(index_en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZhlLGmlg-aL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUUUWk5UXVy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccad30f8-b4b3-42d0-901f-135e81927a79"
      },
      "source": [
        "print(index_en.ntotal)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-BzWBl1Zuc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us try to get an embedding of german text first and then get relevant terms from the other sentence\n",
        "deu_sentence = \"Der Junge rennt\"\n",
        "german_encoded = encoder.encode_sentences(deu_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfsNkabUeK9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_texts = []\n",
        "all_texts.append(deu_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvCpXCWpl-1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from indexing import IndexSearchMultiple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVKGbdVGmXT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def IndexSearchMultiple(data, idx, verbose=False, texts=None, print_errors=False):\n",
        "    nl = len(data)\n",
        "    nbex = data[0].shape[0]\n",
        "    err = np.zeros((nl, nl)).astype(float)\n",
        "    ref = np.linspace(0, nbex-1, nbex).astype(int)  # [0, nbex)\n",
        "    if verbose:\n",
        "        if texts is None: \n",
        "            print('Calculating similarity error (indices):')\n",
        "        else:\n",
        "            print('Calculating similarity error (textual):')\n",
        "    for i1 in range(nl):\n",
        "        for i2 in range(nl):\n",
        "            if i1 != i2:\n",
        "                D, I = idx[i2].search(data[i1], 1)\n",
        "                if texts: # do textual comparison\n",
        "                    e1 = 0\n",
        "                    for p in range(I.shape[0]):\n",
        "                        if texts[i2][p] != texts[i2][I[p,0]]:\n",
        "                            e1 += 1\n",
        "                            if print_errors:\n",
        "                                print('Error {:s}\\n      {:s}'\n",
        "                                      .format(texts[i2][p].strip(), texts[i2][I[p,0]].strip()))\n",
        "                    err[i1, i2] = e1 / nbex\n",
        "                else:  # do index based comparision\n",
        "                    err[i1, i2] \\\n",
        "                        = (nbex - np.equal(I.reshape(nbex), ref)\n",
        "                           .astype(int).sum()) / nbex\n",
        "                if verbose:\n",
        "                    print(' - similarity error {:s}/{:s}: {:5d}={:5.2f}%'\n",
        "                          .format(args.langs[i1], args.langs[i2],\n",
        "                                  err[i1, i2], 100.0 * err[i1, i2]))\n",
        "    return err\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o97NTjVwemIG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ced631d4-47cf-4d13-e7b5-d8734ca0b3f0"
      },
      "source": [
        "\n",
        "IndexSearchMultiple(all_data, all_index, texts=all_texts,\n",
        "                          verbose=True, print_errors=True)\n",
        "\n",
        "#IndexSearchMultiple(data=all_data, idx=all_index)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating similarity error (textual):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svzimjpSksRo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4403
        },
        "outputId": "335229b1-e7ce-48da-ae93-f43124050476"
      },
      "source": [
        "! cat ./lib/indexing.py"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#!/usr/bin/python3\n",
            "# Copyright (c) Facebook, Inc. and its affiliates.\n",
            "# All rights reserved.\n",
            "#\n",
            "# This source code is licensed under the BSD-style license found in the\n",
            "# LICENSE file in the root directory of this source tree.\n",
            "#\n",
            "# LASER  Language-Agnostic SEntence Representations\n",
            "# is a toolkit to calculate multilingual sentence embeddings\n",
            "# and to use them for document classification, bitext filtering\n",
            "# and mining\n",
            "#\n",
            "# --------------------------------------------------------\n",
            "#\n",
            "# tools for indexing and search with FAISS\n",
            "\n",
            "import faiss\n",
            "import os.path\n",
            "import sys\n",
            "import numpy as np\n",
            "\n",
            "#-------------------------------------------------------------\n",
            "# Get list of fnames:\n",
            "#  - we loop over the list of given languages\n",
            "#  - for each language, we also check if there are splitted files .%03d\n",
            "\n",
            "def SplitFnames(par_fname, langs):\n",
            "    fnames = []\n",
            "    for l in langs:\n",
            "        fname = par_fname + '.' + l\n",
            "        if os.path.isfile(fname):\n",
            "            fnames.append(fname)\n",
            "        for i in range(1000):\n",
            "            fname = par_fname + '.' + l + '.{:03d}'.format(i)\n",
            "            if os.path.isfile(fname):\n",
            "                fnames.append(fname)\n",
            "    if len(fnames) == 0:\n",
            "        print(\"ERROR: no embeddings found in {:s}*\".format(par_fname))\n",
            "        sys.exit(1)\n",
            "    return fnames\n",
            "\n",
            "def SplitOpen(par_fname, langs, dim, dtype, verbose=False):\n",
            "    M = []\n",
            "    nf = 0\n",
            "    nc = 0\n",
            "    print('Reading sentence embeddings')\n",
            "    print(' - memory mapped files {:s}'.format(par_fname))\n",
            "    for fname in SplitFnames(par_fname, langs):\n",
            "        n = int(os.path.getsize(fname) / dim / np.dtype(dtype).itemsize)\n",
            "        if verbose:\n",
            "            print(' - {:s}: {:d} x {:d}'.format(fname, n, dim))\n",
            "        Mi = np.memmap(fname, mode='r', dtype=dtype, shape=(n, dim))\n",
            "        nc += n\n",
            "        nf += 1\n",
            "        M.append(Mi)\n",
            "    print(' - total of {:d} files: {:d} x {:d}'.format(nf, nc, dim))\n",
            "    return M\n",
            "\n",
            "def SplitAccess(M, idx):\n",
            "    i = idx\n",
            "    for Mi in M:\n",
            "        n = Mi.shape[0]\n",
            "        if i < n:\n",
            "            return Mi[i,:]\n",
            "        i -= n\n",
            "    print('ERROR: index {:d} is too large form memory mapped files'.format(idx))\n",
            "    sys.exit(1)\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# create an FAISS index on the given data\n",
            "\n",
            "def IndexCreate(dname, idx_type,\n",
            "                verbose=False, normalize=True, save_index=False, dim=1024):\n",
            "\n",
            "    assert idx_type == 'FlatL2', 'only FlatL2 index is currently supported'\n",
            "    x = np.fromfile(dname, dtype=np.float32, count=-1)\n",
            "    nbex = x.shape[0] // dim\n",
            "    print(' - embedding: {:s} {:d} examples of dim {:d}'\n",
            "          .format(dname, nbex, dim))\n",
            "    x.resize(nbex, dim)\n",
            "    print(' - creating FAISS index')\n",
            "    idx = faiss.IndexFlatL2(dim)\n",
            "    if normalize:\n",
            "        faiss.normalize_L2(x)\n",
            "    idx.add(x)\n",
            "    if save_index:\n",
            "        iname = 'TODO'\n",
            "        print(' - saving index into ' + iname)\n",
            "        faiss.write_index(idx, iname)\n",
            "    return x, idx\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# search closest vector for all languages pairs and calculate error rate\n",
            "\n",
            "def IndexSearchMultiple(data, idx, verbose=False, texts=None, print_errors=False):\n",
            "    nl = len(data)\n",
            "    nbex = data[0].shape[0]\n",
            "    err = np.zeros((nl, nl)).astype(float)\n",
            "    ref = np.linspace(0, nbex-1, nbex).astype(int)  # [0, nbex)\n",
            "    if verbose:\n",
            "        if texts is None: \n",
            "            print('Calculating similarity error (indices):')\n",
            "        else:\n",
            "            print('Calculating similarity error (textual):')\n",
            "    for i1 in range(nl):\n",
            "        for i2 in range(nl):\n",
            "            if i1 != i2:\n",
            "                D, I = idx[i2].search(data[i1], 1)\n",
            "                if texts: # do textual comparison\n",
            "                    e1 = 0\n",
            "                    for p in range(I.shape[0]):\n",
            "                        if texts[i2][p] != texts[i2][I[p,0]]:\n",
            "                            e1 += 1\n",
            "                            if print_errors:\n",
            "                                print('Error {:s}\\n      {:s}'\n",
            "                                      .format(texts[i2][p].strip(), texts[i2][I[p,0]].strip()))\n",
            "                    err[i1, i2] = e1 / nbex\n",
            "                else:  # do index based comparision\n",
            "                    err[i1, i2] \\\n",
            "                        = (nbex - np.equal(I.reshape(nbex), ref)\n",
            "                           .astype(int).sum()) / nbex\n",
            "                if verbose:\n",
            "                    print(' - similarity error {:s}/{:s}: {:5d}={:5.2f}%'\n",
            "                          .format(args.langs[i1], args.langs[i2],\n",
            "                                  err[i1, i2], 100.0 * err[i1, i2]))\n",
            "    return err\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# print confusion matrix\n",
            "\n",
            "def IndexPrintConfusionMatrix(err, langs):\n",
            "    nl = len(langs)\n",
            "    assert nl == err.shape[0], 'size of errror matrix doesn not match'\n",
            "    print('Confusion matrix:')\n",
            "    print('{:8s}'.format('langs'), end='')\n",
            "    for i2 in range(nl):\n",
            "        print('{:8s} '.format(langs[i2]), end='')\n",
            "    print('{:8s}'.format('avg'))\n",
            "    for i1 in range(nl):\n",
            "        print('{:3s}'.format(langs[i1]), end='')\n",
            "        for i2 in range(nl):\n",
            "            print('{:8.2f}%'.format(100 * err[i1, i2]), end='')\n",
            "        print('{:8.2f}%'.format(100 * err[i1, :].sum() / (nl-1)))\n",
            "\n",
            "    print('avg', end='')\n",
            "    for i2 in range(nl):\n",
            "        print('{:8.2f}%'.format(100 * err[:, i2].sum() / (nl-1)), end='')\n",
            "\n",
            "    # global average\n",
            "    print('{:8.2f}%'.format(100 * err.sum() / (nl-1) / nl))\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# Load an FAISS index\n",
            "\n",
            "def IndexLoad(idx_name, nprobe, gpu=False):\n",
            "    print('Reading FAISS index')\n",
            "    print(' - index: {:s}'.format(idx_name))\n",
            "    index = faiss.read_index(idx_name)\n",
            "    print(' - found {:d} sentences of dim {:d}'.format(index.ntotal, index.d))\n",
            "    print(' - setting nbprobe to {:d}'.format(nprobe))\n",
            "    if gpu:\n",
            "        print(' - transfer index to %d GPUs ' % faiss.get_num_gpus())\n",
            "        #co = faiss.GpuMultipleClonerOptions()\n",
            "        #co.shard = True\n",
            "        index = faiss.index_cpu_to_all_gpus(index) # co=co\n",
            "        faiss.GpuParameterSpace().set_index_parameter(index, 'nprobe', nprobe)\n",
            "    return index\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# Opens a text file with the sentences corresponding to the indices used\n",
            "# by an FAISS index\n",
            "# We also need the reference files with the byte offsets to the beginning\n",
            "# of each sentence\n",
            "# optionnally:  array with number of words per sentence\n",
            "# All arrays are memory mapped\n",
            "\n",
            "def IndexTextOpen(txt_fname):\n",
            "    print('Reading text corpus')\n",
            "    print(' - texts: {:s}'.format(txt_fname))\n",
            "    txt_mmap = np.memmap(txt_fname, mode='r', dtype=np.uint8)\n",
            "    fname = txt_fname.replace('.txt', '.ref.bin32')\n",
            "    if os.path.isfile(fname):\n",
            "        print(' - sentence start offsets (32 bit): {}'.format(fname))\n",
            "        ref_mmap = np.memmap(fname, mode='r', dtype=np.uint32)\n",
            "    else:\n",
            "        fname = txt_fname.replace('.txt', '.ref.bin64')\n",
            "        if os.path.isfile(fname):\n",
            "            print(' - sentence start offsets (64 bit): {}'.format(fname))\n",
            "            ref_mmap = np.memmap(fname, mode='r', dtype=np.uint64)\n",
            "        else:\n",
            "            print('ERROR: no file with sentence start offsets found')\n",
            "            sys.exit(1)\n",
            "    print(' - found {:d} sentences'.format(ref_mmap.shape[0]))\n",
            "\n",
            "    nbw_mmap = None\n",
            "    fname = txt_fname.replace('.txt', '.nw.bin8')\n",
            "    if os.path.isfile(fname):\n",
            "        print(' - word counts: {:s}'.format(fname))\n",
            "        nbw_mmap = np.memmap(fname, mode='r', dtype=np.uint8)\n",
            "\n",
            "    M = None\n",
            "    fname = txt_fname.replace('.txt', '.meta')\n",
            "    if os.path.isfile(fname):\n",
            "        M = []\n",
            "        n = 0\n",
            "        print(' - metafile: {:s}'.format(fname))\n",
            "        with open(fname, 'r') as fp:\n",
            "            for line in fp:\n",
            "                fields = line.strip().split()\n",
            "                if len(fields) != 2:\n",
            "                    print('ERROR: format error in meta file')\n",
            "                    sys.exit(1)\n",
            "                n += int(fields[1])\n",
            "                M.append({'lang': fields[0], 'n': n})\n",
            "        print(' - found {:d} languages:'.format(len(M)), end='')\n",
            "        for L in M:\n",
            "            print(' {:s}'.format(L['lang']), end='')\n",
            "        print('')\n",
            "\n",
            "    return txt_mmap, ref_mmap, nbw_mmap, M\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# Return the text for the given index\n",
            "\n",
            "def IndexTextQuery(txt_mmap, ref_mmap, idx):\n",
            "    p = int(ref_mmap[idx])  # get starting byte position\n",
            "    i = 0\n",
            "    dim = 10000  # max sentence length in bytes\n",
            "    b = bytearray(dim)\n",
            "    #  find EOL\n",
            "    while txt_mmap[p+i] != 10 and i < dim:\n",
            "        b[i] = txt_mmap[p+i]\n",
            "        i += 1\n",
            "\n",
            "    return b[0:i].decode('utf-8')\n",
            "\n",
            "\n",
            "###############################################################################\n",
            "# Search the [k] nearest vectors of [x] in the given index\n",
            "# and return the text lines\n",
            "\n",
            "def IndexSearchKNN(index, x, T, R, kmax=1, Dmax=1.0, dedup=True):\n",
            "    D, I = index.search(x, kmax)\n",
            "    prev = {}  # for depuplication\n",
            "    res = []\n",
            "    for n in range(x.shape[0]):\n",
            "        for i in range(kmax):\n",
            "            txt = IndexTextQuery(T, R, I[n, i])\n",
            "            if (dedup and txt not in prev) and D[n, i] <= Dmax:\n",
            "                prev[txt] = 1\n",
            "                res.append([txt, D[n, i]])\n",
            "    return res\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}