{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese-LSTM",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinmay5/NLP-Praktikum/blob/master/Siamese_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sScywYm8EJhe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4a946591-c83a-4ea3-ade7-4389b4ba6daa"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqSjJNruoa8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUcKCcufznUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')\n",
        "# %cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bENmrfgZLusD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install torch==1.0.1 -f https://download.pytorch.org/whl/cu100/stable # CUDA 10.0 build"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFppkR1CAEg4",
        "colab_type": "code",
        "outputId": "a2bc31d4-4c57-40b8-84a0-4009bc1ddfc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip3 install torchvision\n",
        "!pip install nltk\n",
        "!pip install tqdm"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtDA2b2VYMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#coding=utf-8\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "stops1 = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "def clean_sent(sent):\n",
        "    sent = sent.lower()\n",
        "    sent = re.sub(u'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]',' ',sent)\n",
        "    sent = re.sub('¡',' ',sent)\n",
        "    sent = re.sub('¿',' ',sent)\n",
        "    sent = re.sub('Á','á',sent)\n",
        "    sent = re.sub('Ó','ó',sent)\n",
        "    sent = re.sub('Ú','ú',sent)\n",
        "    sent = re.sub('É','é',sent)\n",
        "    sent = re.sub('Í','í',sent)\n",
        "    return sent\n",
        "  \n",
        "def cleanSpanish(df):\n",
        "    df['spanish1'] = df.spanish1.map(lambda x: ' '.join([ word for word in\n",
        "                                                         nltk.word_tokenize(clean_sent(x))]))\n",
        "    df['spanish2'] = df.spanish2.map(lambda x: ' '.join([ word for word in\n",
        "                                                         nltk.word_tokenize(clean_sent(x))]))\n",
        "    \n",
        "def removeSpanishStopWords(df, stop):\n",
        "\tdf['spanish1'] = df.spanish1.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x)\n",
        "                                                         if word not in stop]))\n",
        "\tdf['spanish2'] = df.spanish2.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x)\n",
        "                                                         if word not in stop]))\n",
        "\n",
        "\n",
        "def data_preprocessing():\n",
        "\n",
        "    # Training data\n",
        "    import os\n",
        "    os.chdir(\"/content/\")\n",
        "    !ls\n",
        "\n",
        "    df_train_en_sp = pd.read_csv('./cikm_english_train_20180516.txt', sep='\t', header=None,\n",
        "                                 error_bad_lines=False)\n",
        "    df_train_sp_en = pd.read_csv('./cikm_spanish_train_20180516.txt', sep='\t', header=None,\n",
        "                                 error_bad_lines=False)\n",
        "    df_train_en_sp.columns = ['english1', 'spanish1', 'english2', 'spanish2', 'result']\n",
        "    df_train_sp_en.columns = ['spanish1', 'english1', 'spanish2', 'english2', 'result']\n",
        "    train1 = pd.DataFrame(pd.concat([df_train_en_sp['spanish1'], df_train_sp_en['spanish1']], axis=0))\n",
        "    train2 = pd.DataFrame(pd.concat([df_train_en_sp['spanish2'], df_train_sp_en['spanish2']], axis=0))\n",
        "    train_data = pd.concat([train1, train2], axis=1).reset_index()\n",
        "    train_data = train_data.drop(['index'], axis=1)\n",
        "    result = pd.DataFrame(pd.concat([df_train_en_sp['result'], df_train_sp_en['result']], axis=0)).reset_index()\n",
        "    result = result.drop(['index'], axis=1)\n",
        "    # pd.get_dummies(result['result']).head()\n",
        "    train_data['result'] = result\n",
        "\n",
        "    # Evaluation data\n",
        "    test_data = pd.read_csv('./cikm_test_a_20180516.txt', sep='\t', header=None, error_bad_lines=False)\n",
        "    test_data.columns = ['spanish1', 'spanish2']\n",
        "\n",
        "\n",
        "    cleanSpanish(train_data)\n",
        "    removeSpanishStopWords(train_data, stops1)\n",
        "    cleanSpanish(test_data)\n",
        "    removeSpanishStopWords(test_data, stops1)\n",
        "\n",
        "    train_data.replace('', np.nan, inplace=True)\n",
        "    dirty_data = train_data[train_data.isnull().any(axis=1)]\n",
        "    print ('dirty sample count:', dirty_data.shape[0])\n",
        "    print ('positive dirty training sample:', len(dirty_data[dirty_data['result'] == 1]))\n",
        "    print ('negative dirty training sample:', len(dirty_data[dirty_data['result'] == 0]))\n",
        "\n",
        "    train_data = train_data.dropna()\n",
        "    test_data.replace('', np.nan, inplace=True)\n",
        "    test_data = test_data.dropna()\n",
        "    print ('Train sample count:', train_data.shape[0], 'Test sample count:', test_data.shape[0])\n",
        "\n",
        "    train_data.columns = ['s1', 's2', 'label']\n",
        "    test_data.columns = ['s1', 's2']\n",
        "\n",
        "    train_data.to_csv(\"cleaned_train.csv\", index=False)\n",
        "    test_data.to_csv(\"cleaned_test.csv\", index=False)\n",
        "\n",
        "def get_embedding(word_dict, embedding_path, embedding_dim=300):\n",
        "    # find existing word embeddings\n",
        "    word_vec = {}\n",
        "    with open(embedding_path) as f:\n",
        "        for line in f:\n",
        "            word, vec = line.split(' ', 1)\n",
        "            if word in word_dict:\n",
        "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
        "    print('Found {0}/{1} words with embedding vectors'.format(\n",
        "        len(word_vec), len(word_dict)))\n",
        "    missing_word_num = len(word_dict) - len(word_vec)\n",
        "    missing_ratio = round(float(missing_word_num) / len(word_dict), 4) * 100\n",
        "    print('Missing Ratio: {}%'.format(missing_ratio))\n",
        "\n",
        "    # handling unknown embeddings\n",
        "    for word in word_dict:\n",
        "        if word not in word_vec:\n",
        "            # If word not in word_vec, create a random embedding for it\n",
        "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "            word_vec[word] = new_embedding\n",
        "    print (\"Filled missing words' embeddings.\")\n",
        "    print (\"Embedding Matrix Size: \", len(word_vec))\n",
        "\n",
        "    return word_vec\n",
        "\n",
        "def save_embed(obj, path):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "    print ('Embedding saved')\n",
        "\n",
        "def load_embed(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWYKXO5gJPf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Some of the variables that are going to be treated as constants and used throughout the code\n",
        "\"\"\"\n",
        "\n",
        "data_preprocess = True # Would preprocess the data and generate the embeddings\n",
        "make_dict = True #  Would generate the embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZEEUAKwX2Uh",
        "colab_type": "code",
        "outputId": "6cdca1fd-0182-444a-f377-77c661c05eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "\"\"\" Data Preprocessing \"\"\"\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "os.chdir(\"/content/\")\n",
        "!ls\n",
        "\n",
        "with open('siamese-config.yaml') as f:\n",
        "  config = yaml.load(f)\n",
        "\n",
        "  \n",
        "if data_preprocess:\n",
        "    print ('Pre-processing Original Data ...')\n",
        "    data_preprocessing()\n",
        "    print ('Data Pre-processing Done!')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cikm_english_train_20180516.txt  cleaned_train.csv    wiki.en.vec\n",
            "cikm_spanish_train_20180516.txt  embedding.pkl\t      wiki.es.vec\n",
            "cikm_test_a_20180516.txt\t sample_data\n",
            "cleaned_test.csv\t\t siamese-config.yaml\n",
            "Pre-processing Original Data ...\n",
            "cikm_english_train_20180516.txt  cleaned_train.csv    wiki.en.vec\n",
            "cikm_spanish_train_20180516.txt  embedding.pkl\t      wiki.es.vec\n",
            "cikm_test_a_20180516.txt\t sample_data\n",
            "cleaned_test.csv\t\t siamese-config.yaml\n",
            "dirty sample count: 73\n",
            "positive dirty training sample: 5\n",
            "negative dirty training sample: 68\n",
            "Train sample count: 21327 Test sample count: 4998\n",
            "Data Pre-processing Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C05Cgh29Lx-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.embed_size = config['model']['embed_size']\n",
        "        self.batch_size = config['model']['batch_size']\n",
        "        self.hidden_size = config['model']['encoder']['hidden_size']\n",
        "        self.num_layers = config['model']['encoder']['num_layers']\n",
        "        self.bidir = config['model']['encoder']['bidirectional']\n",
        "        if self.bidir:\n",
        "            self.direction = 2\n",
        "        else: self.direction = 1\n",
        "        self.dropout = config['model']['encoder']['dropout']\n",
        "\n",
        "        self.embedding = config['embedding_matrix']\n",
        "        self.lstm = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size, dropout=self.dropout,\n",
        "                            num_layers=self.num_layers, bidirectional=self.bidir)\n",
        "        self.lstm = self.lstm.cuda()\n",
        "\n",
        "    def initHiddenCell(self):\n",
        "        rand_hidden = Variable(torch.randn(self.direction * self.num_layers, self.batch_size, self.hidden_size))\n",
        "        rand_cell = Variable(torch.randn(self.direction * self.num_layers, self.batch_size, self.hidden_size))\n",
        "        rand_hidden = rand_hidden.cuda()\n",
        "        rand_cell = rand_cell.cuda()\n",
        "        return rand_hidden, rand_cell\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = self.embedding(input).view(1, 1, -1)\n",
        "        output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "        return output, hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egyATa-gLyBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Siamese_lstm(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Siamese_lstm, self).__init__()\n",
        "\n",
        "        self.encoder = LSTMEncoder(config)\n",
        "        self.fc_dim = config['model']['fc_dim']\n",
        "\n",
        "        self.input_dim = 5 * self.encoder.direction * self.encoder.hidden_size\n",
        "        # self.classifier = nn.Sequential(\n",
        "        #     nn.Linear(self.input_dim, self.fc_dim),\n",
        "        #     nn.Linear(self.fc_dim, 2)\n",
        "        # )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, int(self.input_dim/2)),\n",
        "            nn.Linear(int(self.input_dim/2), 2)\n",
        "        ).cuda()\n",
        "\n",
        "    def forward(self, s1, s2):\n",
        "\n",
        "        # init hidden, cell\n",
        "        h1, c1 = self.encoder.initHiddenCell()\n",
        "        h2, c2 = self.encoder.initHiddenCell()\n",
        "\n",
        "        # input one by one\n",
        "\n",
        "        for i in range(len(s1)):\n",
        "\n",
        "            v1, h1, c1 = self.encoder(s1[i], h1, c1)\n",
        "            \n",
        "        for j in range(len(s2)):\n",
        "            v2, h2, c2 = self.encoder(s2[j], h2, c2)\n",
        "            \n",
        "        # utilize these two encoded vectors\n",
        "        features = torch.cat((v1,torch.abs(v1 - v2),v2,v1*v2, (v1+v2)/2), 2)\n",
        "        # features = v1-v2\n",
        "        output = self.classifier(features)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6detYnDOBU9",
        "colab_type": "code",
        "outputId": "9f3f2137-105d-4b32-f8c6-3592111857c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "!cat siamese-config.yaml"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "experiment_name: 'siamese-baseline'\n",
            "\n",
            "task: 'train'\n",
            "make_dict: False\n",
            "data_preprocessing: False\n",
            "\n",
            "ckpt_dir: 'ckpt/'\n",
            "\n",
            "training:\n",
            "    num_epochs: 20\n",
            "    learning_rate: 0.01\n",
            "    # options = ['adam', 'adadelta', 'rmsprop']\n",
            "    optimizer: 'sgd'\n",
            "\n",
            "\n",
            "embedding:\n",
            "    full_embedding_path: 'input/wiki.es.vec'\n",
            "    cur_embedding_path: 'input/embedding.pkl'\n",
            "\n",
            "model:\n",
            "    fc_dim: 100\n",
            "    name: 'siamese'\n",
            "    embed_size: 300\n",
            "    batch_size: 1\n",
            "    embedding_freeze: False\n",
            "    encoder:\n",
            "        hidden_size: 150\n",
            "        num_layers: 1\n",
            "        bidirectional: False\n",
            "        dropout: 0.0\n",
            "\n",
            "result:\n",
            "    filename: 'result.txt'\n",
            "    filepath: 'res/'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfZBaI7FO4lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class myDS(Dataset):\n",
        "\n",
        "    def __init__(self, df, all_sents):\n",
        "        # Assign vocabularies.\n",
        "        self.s1 = df['s1'].tolist()\n",
        "        self.s2 = df['s2'].tolist()\n",
        "        self.label = df['label'].tolist()\n",
        "        self.vocab = Vocab(all_sents, sos_token='<sos>', eos_token='<eos>', unk_token='<unk>')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Split sentence into words.\n",
        "        s1_words = self.s1[idx].split()\n",
        "        s2_words = self.s2[idx].split()\n",
        "\n",
        "        # Add <SOS> and <EOS> tokens.\n",
        "        s1_words = [self.vocab.sos_token] + s1_words + [self.vocab.eos_token]\n",
        "        s2_words = [self.vocab.sos_token] + s2_words + [self.vocab.eos_token]\n",
        "\n",
        "        # Lookup word ids in vocabularies.\n",
        "        s1_ids = [self.vocab.word2id(word) for word in s1_words]\n",
        "        s2_ids = [self.vocab.word2id(word) for word in s2_words]\n",
        "        label = self.label[idx]\n",
        "\n",
        "        return s1_ids, s2_ids, label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_dkKjxNO4q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class mytestDS(Dataset):\n",
        "\n",
        "    def __init__(self, df, all_sents):\n",
        "        # Assign vocabularies.\n",
        "        self.s1 = df['s1'].tolist()\n",
        "        self.s2 = df['s2'].tolist()\n",
        "        self.vocab = Vocab(all_sents, sos_token='<sos>', eos_token='<eos>', unk_token='<unk>')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.s1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Split sentence into words.\n",
        "        s1_words = self.s1[idx].split()\n",
        "        s2_words = self.s2[idx].split()\n",
        "\n",
        "        # Add <SOS> and <EOS> tokens.\n",
        "        s1_words = [self.vocab.sos_token] + s1_words + [self.vocab.eos_token]\n",
        "        s2_words = [self.vocab.sos_token] + s2_words + [self.vocab.eos_token]\n",
        "\n",
        "        # Lookup word ids in vocabularies.\n",
        "        s1_ids = [self.vocab.word2id(word) for word in s1_words]\n",
        "        s2_ids = [self.vocab.word2id(word) for word in s2_words]\n",
        "        \n",
        "        return s1_ids, s2_ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBD2uMOtO4od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab(object):\n",
        "    def __init__(self, all_sents, max_size=None, sos_token=None, eos_token=None, unk_token=None):\n",
        "        \"\"\"Initialize the vocabulary.\n",
        "        Args:\n",
        "            iter: An iterable which produces sequences of tokens used to update\n",
        "                the vocabulary.\n",
        "            max_size: (Optional) Maximum number of tokens in the vocabulary.\n",
        "            sos_token: (Optional) Token denoting the start of a sequence.\n",
        "            eos_token: (Optional) Token denoting the end of a sequence.\n",
        "            unk_token: (Optional) Token denoting an unknown element in a\n",
        "                sequence.\n",
        "        \"\"\"\n",
        "        self.max_size = max_size\n",
        "        self.pad_token = '<pad>'\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.unk_token = unk_token\n",
        "\n",
        "        # Add special tokens.\n",
        "        id2word = [self.pad_token]\n",
        "        if sos_token is not None:\n",
        "            id2word.append(self.sos_token)\n",
        "        if eos_token is not None:\n",
        "            id2word.append(self.eos_token)\n",
        "        if unk_token is not None:\n",
        "            id2word.append(self.unk_token)\n",
        "\n",
        "        # Update counter with token counts.\n",
        "        counter = Counter()\n",
        "        for x in all_sents:\n",
        "            counter.update(x.split())\n",
        "\n",
        "        # Extract lookup tables.\n",
        "        if max_size is not None:\n",
        "            counts = counter.most_common(max_size)\n",
        "        else:\n",
        "            counts = counter.items()\n",
        "            counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
        "        words = [x[0] for x in counts]\n",
        "        id2word.extend(words)\n",
        "        word2id = {x: i for i, x in enumerate(id2word)}\n",
        "\n",
        "        self._id2word = id2word\n",
        "        self._word2id = word2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._id2word)\n",
        "\n",
        "    def word2id(self, word):\n",
        "        \"\"\"Map a word in the vocabulary to its unique integer id.\n",
        "        Args:\n",
        "            word: Word to lookup.\n",
        "        Returns:\n",
        "            id: The integer id of the word being looked up.\n",
        "        \"\"\"\n",
        "        if word in self._word2id:\n",
        "            return self._word2id[word]\n",
        "        elif self.unk_token is not None:\n",
        "            return self._word2id[self.unk_token]\n",
        "        else:\n",
        "            raise KeyError('Word \"%s\" not in vocabulary.' % word)\n",
        "\n",
        "    def id2word(self, id):\n",
        "        \"\"\"Map an integer id to its corresponding word in the vocabulary.\n",
        "        Args:\n",
        "            id: Integer id of the word being looked up.\n",
        "        Returns:\n",
        "            word: The corresponding word.\n",
        "        \"\"\"\n",
        "        return self._id2word[id]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgowMsbBJPWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wiv2kjMRENYH",
        "colab_type": "code",
        "outputId": "4ea4a437-cb67-4afe-a131-5bcaec79b461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q09HF_6GX2b_",
        "colab_type": "code",
        "outputId": "eede4ac8-bce4-4c02-8b54-23d80dfc1c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\"\"\" Read Data \"\"\"\n",
        "\n",
        "train_data = pd.read_csv('cleaned_train.csv')\n",
        "test_data = pd.read_csv('cleaned_test.csv')\n",
        "\n",
        "# split dataset\n",
        "msk = np.random.rand(len(train_data)) < 0.8\n",
        "train = train_data[msk]\n",
        "valid = train_data[~msk]\n",
        "all_sents = train_data['s1'].tolist() + train_data['s2'].tolist() + test_data['s1'].tolist() + test_data['s2'].tolist()\n",
        "\n",
        "# dataset\n",
        "trainDS = myDS(train, all_sents)\n",
        "validDS = myDS(valid, all_sents)\n",
        "\n",
        "print ('Data size:',train_data.shape[0], test_data.shape[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size: 21327 4998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8NPOcxrGdBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Get Embedding \"\"\"\n",
        "\n",
        "#full_embed_path = config['embedding']['full_embedding_path']\n",
        "#cur_embed_path = config['embedding']['cur_embedding_path']\n",
        "\n",
        "# TODO: Make sure we read it from the config file in the future\n",
        "full_embed_path = \"wiki.es.vec\"\n",
        "cur_embed_path = \"embedding.pkl\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxSsU6PmVVWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this line if the embeddings need to be loaded\n",
        "\n",
        "\n",
        "#!curl https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec > ./wiki.en.vec\n",
        "#!curl https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.es.vec > ./wiki.es.vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1kwrEk8YBqR",
        "colab_type": "code",
        "outputId": "ab414571-b0ad-4864-a3cd-867d5d843062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists(cur_embed_path) and not make_dict:\n",
        "    embed_dict = load_embed(cur_embed_path)\n",
        "    print ('Loaded existing embedding.')\n",
        "else:\n",
        "    print ('Making embedding...')\n",
        "    embed_dict = get_embedding(trainDS.vocab._id2word, full_embed_path)\n",
        "    save_embed(embed_dict,\"embedding.pkl\")\n",
        "    print ('Saved generated embedding.')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Making embedding...\n",
            "Found 5141/5773 words with embedding vectors\n",
            "Missing Ratio: 10.95%\n",
            "Filled missing words' embeddings.\n",
            "Embedding Matrix Size:  5773\n",
            "Embedding saved\n",
            "Saved generated embedding.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0hY462lGO_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(embed_dict)\n",
        "# initialize nn embedding\n",
        "embedding = nn.Embedding(vocab_size, config['model']['embed_size'])\n",
        "embed_list = []\n",
        "for word in trainDS.vocab._id2word:\n",
        "    embed_list.append(embed_dict[word])\n",
        "weight_matrix = np.array(embed_list)\n",
        "# pass weights to nn embedding\n",
        "embedding.weight = nn.Parameter(torch.from_numpy(weight_matrix).type(torch.FloatTensor), requires_grad = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knAMkjneRumZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding\n",
        "config['embedding_matrix'] = embedding\n",
        "config['vocab_size'] = len(embed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoaBdfULRur3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "73ecdd4c-7b65-4e04-e5bf-11e3a1441889"
      },
      "source": [
        "# model\n",
        "siamese = Siamese_lstm(config)\n",
        "siamese.cuda()\n",
        "siamese.encoder.cuda()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMEncoder(\n",
              "  (embedding): Embedding(5773, 300)\n",
              "  (lstm): LSTM(300, 150)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQHsY8anYMs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(embed_dict)\n",
        "# initialize nn embedding\n",
        "embedding = nn.Embedding(vocab_size, config['model']['embed_size'])\n",
        "embed_list = []\n",
        "for word in trainDS.vocab._id2word:\n",
        "    embed_list.append(embed_dict[word])\n",
        "weight_matrix = np.array(embed_list)\n",
        "# pass weights to nn embedding\n",
        "embedding.weight = nn.Parameter(torch.from_numpy(weight_matrix).type(torch.FloatTensor), requires_grad = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SphmC-tGZeLP",
        "colab_type": "text"
      },
      "source": [
        "Code for preprocessing the text and other cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRmMu1-GYB1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTXhvxZHXRIL",
        "colab_type": "text"
      },
      "source": [
        "Setting up of the Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y_obpC3XPyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer\n",
        "learning_rate = config['training']['learning_rate']\n",
        "if config['training']['optimizer'] == 'sgd':\n",
        "    optimizer = torch.optim.SGD(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)\n",
        "elif config['training']['optimizer'] == 'adam':\n",
        "    optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)\n",
        "elif config['training']['optimizer'] == 'adadelta':\n",
        "    optimizer = torch.optim.Adadelta(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)\n",
        "elif config['training']['optimizer'] == 'rmsprop':\n",
        "    optimizer = torch.optim.RMSprop(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plIkyXb4XP4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss func\n",
        "loss_weights = Variable(torch.FloatTensor([1, 3]))\n",
        "if torch.cuda.is_available():\n",
        "    loss_weights = loss_weights.cuda()\n",
        "criterion = torch.nn.CrossEntropyLoss(loss_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFOSUSNwXPtH",
        "colab_type": "code",
        "outputId": "f1ac3007-4793-4f52-cb63-671c7889b035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Restore saved model (if one exists).\n",
        "ckpt_path = os.path.join('./', config['experiment_name']+'.pt')\n",
        "\n",
        "if os.path.exists(ckpt_path):\n",
        "    print('Loading checkpoint: %s' % ckpt_path)\n",
        "    ckpt = torch.load(ckpt_path)\n",
        "    epoch = ckpt['epoch']\n",
        "    siamese.load_state_dict(ckpt['siamese'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "else:\n",
        "    epoch = 0\n",
        "    print ('Fresh start!')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fresh start!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqeYI35WFi74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# log info\n",
        "train_log_string = '%s :: Epoch %i :: Iter %i / %i :: train loss: %0.4f'\n",
        "valid_log_string = '%s :: Epoch %i :: valid loss: %0.4f\\n'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9R6hGfqVJh_",
        "colab_type": "text"
      },
      "source": [
        "## Training Procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Ua1mQOVI0t",
        "colab_type": "code",
        "outputId": "9e5a4d91-25a5-4a4b-cd6d-b19e98ef8ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2465
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "# save every epoch for visualization\n",
        "train_loss_record = []\n",
        "valid_loss_record = []\n",
        "best_record = 10.0\n",
        "\n",
        "# training\n",
        "print ('Experiment:{}\\n'.format(config['experiment_name']))\n",
        "\n",
        "    \n",
        "while (epoch < config['training']['num_epochs']):\n",
        "\n",
        "    print ('Start Epoch{} Training...'.format(epoch))\n",
        "\n",
        "    # loss\n",
        "    train_loss = []\n",
        "    train_loss_sum = []\n",
        "    # dataloader\n",
        "    train_dataloader = DataLoader(dataset=trainDS, shuffle=True, num_workers=2, batch_size=1)\n",
        "\n",
        "    for idx, data in enumerate(train_dataloader, 0):\n",
        "\n",
        "        # get data\n",
        "        s1, s2, label = data\n",
        "        \n",
        "        # putting the data into cuda\n",
        "        s1 = torch.from_numpy(np.array(s1))\n",
        "        s2 = torch.from_numpy(np.array(s2))\n",
        "        label = torch.from_numpy(np.array(label))\n",
        "        \n",
        "        s1 = s1.cuda()\n",
        "        s2 = s2.cuda()\n",
        "        label = label.cuda()\n",
        "        \n",
        "        # clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        \n",
        "        # input\n",
        "        output = siamese(s1, s2)\n",
        "        output = output.squeeze(0)\n",
        "\n",
        "        # loss backward\n",
        "        loss = criterion(output, Variable(label))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss.data.cpu())\n",
        "        train_loss_sum.append(loss.data.cpu())\n",
        "\n",
        "        # Every once and a while check on the loss\n",
        "        if ((idx + 1) % 5000) == 0:\n",
        "            print(train_log_string % (datetime.now(), epoch, idx + 1, len(train), np.mean(train_loss)))\n",
        "            train_loss = []\n",
        "\n",
        "    # Record at every epoch\n",
        "    print ('Train Loss at epoch{}: {}\\n'.format(epoch, np.mean(train_loss_sum)))\n",
        "    train_loss_record.append(np.mean(train_loss_sum))\n",
        "\n",
        "    # Valid\n",
        "    print ('Epoch{} Validating...'.format(epoch))\n",
        "\n",
        "    # loss\n",
        "    valid_loss = []\n",
        "    # dataloader\n",
        "    valid_dataloader = DataLoader(dataset=validDS, shuffle=True, num_workers=2, batch_size=1)\n",
        "\n",
        "    for idx, data in enumerate(valid_dataloader, 0):\n",
        "        # get data\n",
        "        s1, s2, label = data\n",
        "\n",
        "        # putting the data into cuda\n",
        "        s1 = torch.from_numpy(np.array(s1))\n",
        "        s2 = torch.from_numpy(np.array(s2))\n",
        "        label = torch.from_numpy(np.array(label))\n",
        "        \n",
        "        s1 = s1.cuda()\n",
        "        s2 = s2.cuda()\n",
        "        label = label.cuda()\n",
        "        \n",
        "        # input\n",
        "        output = siamese(s1, s2)\n",
        "        output = output.squeeze(0)\n",
        "\n",
        "        # loss\n",
        "        loss = criterion(output, Variable(label))\n",
        "        valid_loss.append(loss.data.cpu())\n",
        "\n",
        "    print(valid_log_string % (datetime.now(), epoch, np.mean(valid_loss)))\n",
        "    # Record\n",
        "    valid_loss_record.append(np.mean(valid_loss))\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "    # Keep track of best record\n",
        "    if np.mean(valid_loss) < best_record:\n",
        "        best_record = np.mean(valid_loss)\n",
        "        # save the best model\n",
        "        state_dict = {\n",
        "            'epoch': epoch,\n",
        "            'siamese': siamese.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }\n",
        "        torch.save(state_dict, ckpt_path)\n",
        "        print ('Model saved!\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment:siamese-baseline\n",
            "\n",
            "Start Epoch3 Training...\n",
            "2019-05-21 22:27:26.995351 :: Epoch 3 :: Iter 5000 / 17013 :: train loss: 0.3915\n",
            "2019-05-21 22:28:47.501385 :: Epoch 3 :: Iter 10000 / 17013 :: train loss: 0.4048\n",
            "2019-05-21 22:30:11.273321 :: Epoch 3 :: Iter 15000 / 17013 :: train loss: 0.3719\n",
            "Train Loss at epoch3: 0.3892628252506256\n",
            "\n",
            "Epoch3 Validating...\n",
            "2019-05-21 22:31:30.875104 :: Epoch 3 :: valid loss: 0.3873\n",
            "\n",
            "Model saved!\n",
            "\n",
            "Start Epoch4 Training...\n",
            "2019-05-21 22:32:52.124570 :: Epoch 4 :: Iter 5000 / 17013 :: train loss: 0.3404\n",
            "2019-05-21 22:34:13.403940 :: Epoch 4 :: Iter 10000 / 17013 :: train loss: 0.3437\n",
            "2019-05-21 22:35:35.186432 :: Epoch 4 :: Iter 15000 / 17013 :: train loss: 0.3472\n",
            "Train Loss at epoch4: 0.34338438510894775\n",
            "\n",
            "Epoch4 Validating...\n",
            "2019-05-21 22:36:54.765014 :: Epoch 4 :: valid loss: 0.3658\n",
            "\n",
            "Model saved!\n",
            "\n",
            "Start Epoch5 Training...\n",
            "2019-05-21 22:38:15.622470 :: Epoch 5 :: Iter 5000 / 17013 :: train loss: 0.3026\n",
            "2019-05-21 22:39:37.149971 :: Epoch 5 :: Iter 10000 / 17013 :: train loss: 0.3044\n",
            "2019-05-21 22:40:59.543065 :: Epoch 5 :: Iter 15000 / 17013 :: train loss: 0.2995\n",
            "Train Loss at epoch5: 0.30207836627960205\n",
            "\n",
            "Epoch5 Validating...\n",
            "2019-05-21 22:42:19.070706 :: Epoch 5 :: valid loss: 0.3708\n",
            "\n",
            "Start Epoch6 Training...\n",
            "2019-05-21 22:43:39.453905 :: Epoch 6 :: Iter 5000 / 17013 :: train loss: 0.2656\n",
            "2019-05-21 22:45:00.235498 :: Epoch 6 :: Iter 10000 / 17013 :: train loss: 0.2559\n",
            "2019-05-21 22:46:21.893381 :: Epoch 6 :: Iter 15000 / 17013 :: train loss: 0.2617\n",
            "Train Loss at epoch6: 0.26249587535858154\n",
            "\n",
            "Epoch6 Validating...\n",
            "2019-05-21 22:47:40.839431 :: Epoch 6 :: valid loss: 0.3710\n",
            "\n",
            "Start Epoch7 Training...\n",
            "2019-05-21 22:49:01.295399 :: Epoch 7 :: Iter 5000 / 17013 :: train loss: 0.2071\n",
            "2019-05-21 22:50:21.693219 :: Epoch 7 :: Iter 10000 / 17013 :: train loss: 0.2336\n",
            "2019-05-21 22:51:42.306404 :: Epoch 7 :: Iter 15000 / 17013 :: train loss: 0.2409\n",
            "Train Loss at epoch7: 0.22568874061107635\n",
            "\n",
            "Epoch7 Validating...\n",
            "2019-05-21 22:53:00.329146 :: Epoch 7 :: valid loss: 0.4011\n",
            "\n",
            "Start Epoch8 Training...\n",
            "2019-05-21 22:54:19.119699 :: Epoch 8 :: Iter 5000 / 17013 :: train loss: 0.1892\n",
            "2019-05-21 22:55:38.455321 :: Epoch 8 :: Iter 10000 / 17013 :: train loss: 0.1910\n",
            "2019-05-21 22:56:58.760145 :: Epoch 8 :: Iter 15000 / 17013 :: train loss: 0.1998\n",
            "Train Loss at epoch8: 0.1960529237985611\n",
            "\n",
            "Epoch8 Validating...\n",
            "2019-05-21 22:58:15.878047 :: Epoch 8 :: valid loss: 0.3801\n",
            "\n",
            "Start Epoch9 Training...\n",
            "2019-05-21 22:59:34.371517 :: Epoch 9 :: Iter 5000 / 17013 :: train loss: 0.1583\n",
            "2019-05-21 23:00:54.094949 :: Epoch 9 :: Iter 10000 / 17013 :: train loss: 0.1601\n",
            "2019-05-21 23:02:12.508282 :: Epoch 9 :: Iter 15000 / 17013 :: train loss: 0.1763\n",
            "Train Loss at epoch9: 0.16643072664737701\n",
            "\n",
            "Epoch9 Validating...\n",
            "2019-05-21 23:03:28.998171 :: Epoch 9 :: valid loss: 0.4001\n",
            "\n",
            "Start Epoch10 Training...\n",
            "2019-05-21 23:04:48.428865 :: Epoch 10 :: Iter 5000 / 17013 :: train loss: 0.1125\n",
            "2019-05-21 23:06:06.187788 :: Epoch 10 :: Iter 10000 / 17013 :: train loss: 0.1490\n",
            "2019-05-21 23:07:24.878376 :: Epoch 10 :: Iter 15000 / 17013 :: train loss: 0.1442\n",
            "Train Loss at epoch10: 0.13759568333625793\n",
            "\n",
            "Epoch10 Validating...\n",
            "2019-05-21 23:08:41.578418 :: Epoch 10 :: valid loss: 0.4468\n",
            "\n",
            "Start Epoch11 Training...\n",
            "2019-05-21 23:09:59.687554 :: Epoch 11 :: Iter 5000 / 17013 :: train loss: 0.1071\n",
            "2019-05-21 23:11:16.624119 :: Epoch 11 :: Iter 10000 / 17013 :: train loss: 0.1124\n",
            "2019-05-21 23:12:33.356165 :: Epoch 11 :: Iter 15000 / 17013 :: train loss: 0.1321\n",
            "Train Loss at epoch11: 0.1182505190372467\n",
            "\n",
            "Epoch11 Validating...\n",
            "2019-05-21 23:13:49.492473 :: Epoch 11 :: valid loss: 0.4469\n",
            "\n",
            "Start Epoch12 Training...\n",
            "2019-05-21 23:15:08.352013 :: Epoch 12 :: Iter 5000 / 17013 :: train loss: 0.0857\n",
            "2019-05-21 23:16:26.771588 :: Epoch 12 :: Iter 10000 / 17013 :: train loss: 0.1056\n",
            "2019-05-21 23:17:44.872561 :: Epoch 12 :: Iter 15000 / 17013 :: train loss: 0.1006\n",
            "Train Loss at epoch12: 0.10035441815853119\n",
            "\n",
            "Epoch12 Validating...\n",
            "2019-05-21 23:19:01.312102 :: Epoch 12 :: valid loss: 0.5057\n",
            "\n",
            "Start Epoch13 Training...\n",
            "2019-05-21 23:20:19.715872 :: Epoch 13 :: Iter 5000 / 17013 :: train loss: 0.0704\n",
            "2019-05-21 23:21:38.338429 :: Epoch 13 :: Iter 10000 / 17013 :: train loss: 0.0894\n",
            "2019-05-21 23:22:56.419657 :: Epoch 13 :: Iter 15000 / 17013 :: train loss: 0.0763\n",
            "Train Loss at epoch13: 0.083287812769413\n",
            "\n",
            "Epoch13 Validating...\n",
            "2019-05-21 23:24:12.591097 :: Epoch 13 :: valid loss: 0.4954\n",
            "\n",
            "Start Epoch14 Training...\n",
            "2019-05-21 23:25:30.594853 :: Epoch 14 :: Iter 5000 / 17013 :: train loss: 0.0645\n",
            "2019-05-21 23:26:49.055352 :: Epoch 14 :: Iter 10000 / 17013 :: train loss: 0.0737\n",
            "2019-05-21 23:28:06.590857 :: Epoch 14 :: Iter 15000 / 17013 :: train loss: 0.0766\n",
            "Train Loss at epoch14: 0.0723523274064064\n",
            "\n",
            "Epoch14 Validating...\n",
            "2019-05-21 23:29:22.522570 :: Epoch 14 :: valid loss: 0.5376\n",
            "\n",
            "Start Epoch15 Training...\n",
            "2019-05-21 23:30:40.224660 :: Epoch 15 :: Iter 5000 / 17013 :: train loss: 0.0572\n",
            "2019-05-21 23:31:58.923758 :: Epoch 15 :: Iter 10000 / 17013 :: train loss: 0.0604\n",
            "2019-05-21 23:33:17.094919 :: Epoch 15 :: Iter 15000 / 17013 :: train loss: 0.0752\n",
            "Train Loss at epoch15: 0.06663064658641815\n",
            "\n",
            "Epoch15 Validating...\n",
            "2019-05-21 23:34:33.251234 :: Epoch 15 :: valid loss: 0.5427\n",
            "\n",
            "Start Epoch16 Training...\n",
            "2019-05-21 23:35:51.166357 :: Epoch 16 :: Iter 5000 / 17013 :: train loss: 0.0520\n",
            "2019-05-21 23:37:09.817456 :: Epoch 16 :: Iter 10000 / 17013 :: train loss: 0.0598\n",
            "2019-05-21 23:38:27.546974 :: Epoch 16 :: Iter 15000 / 17013 :: train loss: 0.0596\n",
            "Train Loss at epoch16: 0.05654129013419151\n",
            "\n",
            "Epoch16 Validating...\n",
            "2019-05-21 23:39:43.913475 :: Epoch 16 :: valid loss: 0.5851\n",
            "\n",
            "Start Epoch17 Training...\n",
            "2019-05-21 23:41:02.353307 :: Epoch 17 :: Iter 5000 / 17013 :: train loss: 0.0418\n",
            "2019-05-21 23:42:21.366902 :: Epoch 17 :: Iter 10000 / 17013 :: train loss: 0.0466\n",
            "2019-05-21 23:43:38.549071 :: Epoch 17 :: Iter 15000 / 17013 :: train loss: 0.0590\n",
            "Train Loss at epoch17: 0.048996467143297195\n",
            "\n",
            "Epoch17 Validating...\n",
            "2019-05-21 23:44:54.937262 :: Epoch 17 :: valid loss: 0.6124\n",
            "\n",
            "Start Epoch18 Training...\n",
            "2019-05-21 23:46:13.781334 :: Epoch 18 :: Iter 5000 / 17013 :: train loss: 0.0302\n",
            "2019-05-21 23:47:30.893147 :: Epoch 18 :: Iter 10000 / 17013 :: train loss: 0.0475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jACju1PFOBXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1s2hwjaWi_g",
        "colab_type": "text"
      },
      "source": [
        "Include Torch vision in here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0-2kOgMOBdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "\n",
        "plt.plot(train_loss_record)\n",
        "plt.plot(valid_loss_record)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltprn13EOBaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\" Inference \"\"\"\n",
        "# if config['taks'] == 'inference':\n",
        "testDS = mytestDS(test_data, all_sents)\n",
        "# Do not shuffle here\n",
        "test_dataloader = DataLoader(dataset=testDS, num_workers=2, batch_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXq8_NtIWcqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = []\n",
        "for idx, data in enumerate(test_dataloader, 0):\n",
        "\n",
        "    # get data\n",
        "    s1, s2 = data\n",
        "\n",
        "    # input\n",
        "    output = siamese(s1,s2)\n",
        "    output = output.squeeze(0)\n",
        "\n",
        "    # feed output into softmax to get prob prediction\n",
        "    sm = nn.Softmax(dim=1)\n",
        "    res = sm(output.data)[:,1]\n",
        "    result += res.data.tolist()\n",
        "\n",
        "result = pd.DataFrame(result)\n",
        "print 'Inference Done.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcfTeWUVpiK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6BpPi7aWctf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res_path = os.path.join(config['result']['filepath'], config['result']['filename'])\n",
        "result.to_csv(res_path,header=False,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3FwhbOyhwHA",
        "colab_type": "text"
      },
      "source": [
        "##Extra Section that will come in handy later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gphZG0tjZLhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "   \n",
        "    def __init__(self, margin):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.eps = 1e-9\n",
        "\n",
        "    def forward(self, output1, output2, target, size_average=True):\n",
        "        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n",
        "        losses = 0.5 * (target.float() * distances +\n",
        "                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2)) #ReLU function does the same task as selecting max\n",
        "        if size_average:\n",
        "          return losses.mean() \n",
        "        return losses.sum()\n",
        "\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplet loss\n",
        "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative, size_average=True):\n",
        "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
        "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
        "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
        "        if size_average:\n",
        "          return losses.mean()\n",
        "        return losses.sum()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApJFvPn_h8P6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OnlineContrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The pair selector and embeddings are sent\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin, pair_selector):\n",
        "        super(OnlineContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.pair_selector = pair_selector\n",
        "\n",
        "    def forward(self, embeddings, target):\n",
        "        positive_pairs, negative_pairs = self.pair_selector.get_pairs(embeddings, target)\n",
        "        if embeddings.is_cuda:\n",
        "            positive_pairs = positive_pairs.cuda()\n",
        "            negative_pairs = negative_pairs.cuda()\n",
        "        positive_loss = (embeddings[positive_pairs[:, 0]] - embeddings[positive_pairs[:, 1]]).pow(2).sum(1)\n",
        "        negative_loss = F.relu(\n",
        "            self.margin - (embeddings[negative_pairs[:, 0]] - embeddings[negative_pairs[:, 1]]).pow(2).sum(\n",
        "                1).sqrt()).pow(2)\n",
        "        loss = torch.cat([positive_loss, negative_loss], dim=0) #dim 0 is the rows\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class OnlineTripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplet selector and the embeddings are sent\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin, triplet_selector):\n",
        "        super(OnlineTripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.triplet_selector = triplet_selector\n",
        "\n",
        "    def forward(self, embeddings, target):\n",
        "\n",
        "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
        "\n",
        "        if embeddings.is_cuda:\n",
        "            triplets = triplets.cuda()\n",
        "\n",
        "        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n",
        "        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n",
        "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
        "\n",
        "        return losses.mean(), len(triplets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3AupsAslLAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TripletDataSet(Dataset):\n",
        "    \"\"\"\n",
        "    Train: For each sample (anchor) randomly chooses a positive and negative samples\n",
        "    Test: Creates fixed triplets for testing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_df):\n",
        "        self.train_df = train_df\n",
        "        self.arg1 = train_df['s1']\n",
        "        self.arg2 = train_df['s2']\n",
        "\n",
        "        \n",
        "        random_state = np.random.RandomState(29)\n",
        "\n",
        "        triplets = [[i,\n",
        "                     random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
        "                     random_state.choice(self.label_to_indices[\n",
        "                                             np.random.choice(\n",
        "                                                 list(self.labels_set - set([self.test_labels[i].item()]))\n",
        "                                             )\n",
        "                                         ])\n",
        "                     ]\n",
        "                    for i in range(len(self.train_df))]\n",
        "        self.test_triplets = triplets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Split sentence into words.\n",
        "        s1_words = self.arg1[idx].split()\n",
        "        s2_words = self.arg2[idx].split()\n",
        "\n",
        "        # Add <SOS> and <EOS> tokens.\n",
        "        s1_words = [self.vocab.sos_token] + s1_words + [self.vocab.eos_token]\n",
        "        s2_words = [self.vocab.sos_token] + s2_words + [self.vocab.eos_token]\n",
        "\n",
        "        # Lookup word ids in vocabularies.\n",
        "        s1_ids = [self.vocab.word2id(word) for word in s1_words]\n",
        "        s2_ids = [self.vocab.word2id(word) for word in s2_words]\n",
        "        label = self.label[idx]\n",
        "\n",
        "        return s1_ids, s2_ids, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_df)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}